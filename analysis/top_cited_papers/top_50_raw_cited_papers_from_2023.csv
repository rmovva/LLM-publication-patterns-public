,id,authors,title,categories,abstract,versions,first_category,v1_date,LM_related_terms,mentions_LM_keyword,cluster,domains,industry,academic,above_pred_female_threshold,inferred_female_frac_nqg_uncertainty_threshold_0.100,citationCount,percentile_rank_in_3_month_window,percentile_rank_in_12_month_window,n_authors,percentile_rank_in_6_month_window,experienced_first_author,experienced_last_author
11254,arXiv:2302.13971,"['Hugo Touvron', 'Thibaut Lavril', 'Gautier Izacard', 'Xavier Martinet', 'Marie-Anne Lachaux', 'Timothée Lacroix', 'Baptiste Rozière', 'Naman Goyal', 'Eric Hambro', 'Faisal Azhar', 'Aurelien Rodriguez', 'Armand Joulin', 'Edouard Grave', 'Guillaume Lample']",LLaMA: Open and Efficient Foundation Language Models,['cs.CL'],"  We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.
","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 17:11:15 GMT'}]",cs.CL,2023-02-27 17:11:15,"['language model', 'GPT-3', 'PaLM', 'LLaMA']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.0714285714,1378.0,0.9985272459499264,0.9965769410300296,14,0.9978237214363439,False,True
11524,arXiv:2303.08774,['OpenAI'],GPT-4 Technical Report,"['cs.CL', 'cs.AI']","  We report the development of GPT-4, a large-scale, multimodal model which can
accept image and text inputs and produce text outputs. While less capable than
humans in many real-world scenarios, GPT-4 exhibits human-level performance on
various professional and academic benchmarks, including passing a simulated bar
exam with a score around the top 10% of test takers. GPT-4 is a
Transformer-based model pre-trained to predict the next token in a document.
The post-training alignment process results in improved performance on measures
of factuality and adherence to desired behavior. A core component of this
project was developing infrastructure and optimization methods that behave
predictably across a wide range of scales. This allowed us to accurately
predict some aspects of GPT-4's performance based on models trained with no
more than 1/1,000th the compute of GPT-4.
","[{'version': 'v1', 'created': 'Wed, 15 Mar 2023 17:15:04 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Mar 2023 04:59:24 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Mar 2023 17:46:54 GMT'}]",cs.CL,2023-03-15 17:15:04,['GPT-4'],True,Applications of LLMs/ChatGPT,[],False,False,,,1006.0,0.9977908689248896,0.9964213474404855,1,0.9976060935799782,False,False
11652,arXiv:2303.12712,"['Sébastien Bubeck', 'Varun Chandrasekaran', 'Ronen Eldan', 'Johannes Gehrke', 'Eric Horvitz', 'Ece Kamar', 'Peter Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Scott Lundberg', 'Harsha Nori', 'Hamid Palangi', 'Marco Tulio Ribeiro', 'Yi Zhang']",Sparks of Artificial General Intelligence: Early experiments with GPT-4,"['cs.CL', 'cs.AI']","  Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
","[{'version': 'v1', 'created': 'Wed, 22 Mar 2023 16:51:28 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Mar 2023 17:07:43 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Mar 2023 22:36:40 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Apr 2023 17:00:10 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Apr 2023 20:41:31 GMT'}]",cs.CL,2023-03-22 16:51:28,"['language model', 'PaLM', 'GPT-4', 'large language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,[],False,False,False,0.0909090909,546.0,0.9970544918998527,0.9962657538509413,14,0.9973884657236126,False,True
10807,arXiv:2301.12597,"['Junnan Li', 'Dongxu Li', 'Silvio Savarese', 'Steven Hoi']","BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image
  Encoders and Large Language Models",['cs.CV'],"  The cost of vision-and-language pre-training has become increasingly
prohibitive due to end-to-end training of large-scale models. This paper
proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps
vision-language pre-training from off-the-shelf frozen pre-trained image
encoders and frozen large language models. BLIP-2 bridges the modality gap with
a lightweight Querying Transformer, which is pre-trained in two stages. The
first stage bootstraps vision-language representation learning from a frozen
image encoder. The second stage bootstraps vision-to-language generative
learning from a frozen language model. BLIP-2 achieves state-of-the-art
performance on various vision-language tasks, despite having significantly
fewer trainable parameters than existing methods. For example, our model
outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable
parameters. We also demonstrate the model's emerging capabilities of zero-shot
image-to-text generation that can follow natural language instructions.
","[{'version': 'v1', 'created': 'Mon, 30 Jan 2023 00:56:51 GMT'}, {'version': 'v2', 'created': 'Mon, 1 May 2023 07:30:11 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Jun 2023 07:57:29 GMT'}]",cs.CV,2023-01-30 00:56:51,"['language model', 'large language model']",True,Vision-Language Models,[],False,False,False,0.0,449.0,0.9963181148748159,0.9961101602613972,4,0.997170837867247,True,True
11875,arXiv:2303.18223,"['Wayne Xin Zhao', 'Kun Zhou', 'Junyi Li', 'Tianyi Tang', 'Xiaolei Wang', 'Yupeng Hou', 'Yingqian Min', 'Beichen Zhang', 'Junjie Zhang', 'Zican Dong', 'Yifan Du', 'Chen Yang', 'Yushuo Chen', 'Zhipeng Chen', 'Jinhao Jiang', 'Ruiyang Ren', 'Yifan Li', 'Xinyu Tang', 'Zikang Liu', 'Peiyu Liu', 'Jian-Yun Nie', 'Ji-Rong Wen']",A Survey of Large Language Models,"['cs.CL', 'cs.AI']","  Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
","[{'version': 'v1', 'created': 'Fri, 31 Mar 2023 17:28:46 GMT'}, {'version': 'v10', 'created': 'Sun, 7 May 2023 17:59:15 GMT'}, {'version': 'v11', 'created': 'Thu, 29 Jun 2023 16:09:05 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Apr 2023 15:49:09 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Apr 2023 16:20:17 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Apr 2023 16:13:54 GMT'}, {'version': 'v5', 'created': 'Sun, 16 Apr 2023 16:42:37 GMT'}, {'version': 'v6', 'created': 'Mon, 24 Apr 2023 16:53:57 GMT'}, {'version': 'v7', 'created': 'Tue, 25 Apr 2023 14:42:36 GMT'}, {'version': 'v8', 'created': 'Thu, 27 Apr 2023 15:54:48 GMT'}, {'version': 'v9', 'created': 'Fri, 28 Apr 2023 15:39:09 GMT'}]",cs.CL,2023-03-31 17:28:46,"['language model', 'large language model', 'ChatGPT']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.0,326.0,0.9955817378497791,0.995798973082309,22,0.9969532100108814,True,True
10955,arXiv:2302.04023,"['Yejin Bang', 'Samuel Cahyawijaya', 'Nayeon Lee', 'Wenliang Dai', 'Dan Su', 'Bryan Wilie', 'Holy Lovenia', 'Ziwei Ji', 'Tiezheng Yu', 'Willy Chung', 'Quyet V. Do', 'Yan Xu', 'Pascale Fung']","A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on
  Reasoning, Hallucination, and Interactivity","['cs.CL', 'cs.AI']","  This paper proposes a framework for quantitatively evaluating interactive
LLMs such as ChatGPT using publicly available data sets. We carry out an
extensive technical evaluation of ChatGPT using 23 data sets covering 8
different common NLP application tasks. We evaluate the multitask, multilingual
and multi-modal aspects of ChatGPT based on these data sets and a newly
designed multimodal dataset. We find that ChatGPT outperforms LLMs with
zero-shot learning on most tasks and even outperforms fine-tuned models on some
tasks. We find that it is better at understanding non-Latin script languages
than generating them. It is able to generate multimodal content from textual
prompts, via an intermediate code generation step. Moreover, we find that
ChatGPT is 63.41% accurate on average in 10 different reasoning categories
under logical reasoning, non-textual reasoning, and commonsense reasoning,
hence making it an unreliable reasoner. It is, for example, better at deductive
than inductive reasoning. ChatGPT suffers from hallucination problems like
other LLMs and it generates more extrinsic hallucinations from its parametric
memory as it does not have access to an external knowledge base. Finally, the
interactive feature of ChatGPT enables human collaboration with the underlying
LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++
on machine translation, in a multi-turn ""prompt engineering"" fashion. We also
release codebase for evaluation set extraction.
","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 12:35:34 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Feb 2023 15:20:21 GMT'}]",cs.CL,2023-02-08 12:35:34,['ChatGPT'],True,Applications of LLMs/ChatGPT,['ust.hk'],False,True,False,0.33333333330000003,290.0,0.9948453608247423,0.9956433794927649,13,0.9967355821545157,True,True
10978,arXiv:2302.04761,"['Timo Schick', 'Jane Dwivedi-Yu', 'Roberto Dessì', 'Roberta Raileanu', 'Maria Lomeli', 'Luke Zettlemoyer', 'Nicola Cancedda', 'Thomas Scialom']",Toolformer: Language Models Can Teach Themselves to Use Tools,['cs.CL'],"  Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.
","[{'version': 'v1', 'created': 'Thu, 9 Feb 2023 16:49:57 GMT'}]",cs.CL,2023-02-09 16:49:57,['language model'],True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.4285714286,237.0,0.9941089837997055,0.9954877859032207,8,0.9965179542981502,True,True
11364,arXiv:2303.03378,"['Danny Driess', 'Fei Xia', 'Mehdi S. M. Sajjadi', 'Corey Lynch', 'Aakanksha Chowdhery', 'Brian Ichter', 'Ayzaan Wahid', 'Jonathan Tompson', 'Quan Vuong', 'Tianhe Yu', 'Wenlong Huang', 'Yevgen Chebotar', 'Pierre Sermanet', 'Daniel Duckworth', 'Sergey Levine', 'Vincent Vanhoucke', 'Karol Hausman', 'Marc Toussaint', 'Klaus Greff', 'Andy Zeng', 'Igor Mordatch', 'Pete Florence']",PaLM-E: An Embodied Multimodal Language Model,"['cs.LG', 'cs.AI', 'cs.RO']","  Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 18:58:06 GMT'}]",cs.LG,2023-03-06 18:58:06,"['language model', 'large language model', 'PaLM']",True,"Software, Planning, Robotics",[],False,False,False,0.058823529400000005,231.0,0.9933726067746687,0.9953321923136766,22,0.9963003264417846,False,True
11851,arXiv:2303.17580,"['Yongliang Shen', 'Kaitao Song', 'Xu Tan', 'Dongsheng Li', 'Weiming Lu', 'Yueting Zhuang']","HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging
  Face","['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']","  Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are abundant AI models
available for different domains and modalities, they cannot handle complicated
AI tasks. Considering large language models (LLMs) have exhibited exceptional
ability in language understanding, generation, interaction, and reasoning, we
advocate that LLMs could act as a controller to manage existing AI models to
solve complicated AI tasks and language could be a generic interface to empower
this. Based on this philosophy, we present HuggingGPT, a framework that
leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning
communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use
ChatGPT to conduct task planning when receiving a user request, select models
according to their function descriptions available in Hugging Face, execute
each subtask with the selected AI model, and summarize the response according
to the execution results. By leveraging the strong language capability of
ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover
numerous sophisticated AI tasks in different modalities and domains and achieve
impressive results in language, vision, speech, and other challenging tasks,
which paves a new way towards artificial general intelligence.
","[{'version': 'v1', 'created': 'Thu, 30 Mar 2023 17:48:28 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Apr 2023 17:24:47 GMT'}, {'version': 'v3', 'created': 'Thu, 25 May 2023 15:50:20 GMT'}]",cs.CL,2023-03-30 17:48:28,"['language model', 'large language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,"['microsoft.com', 'zju.edu.cn']",True,True,False,0.0,189.0,0.9926362297496318,0.9950210051345885,6,0.996082698585419,True,False
12302,arXiv:2304.10592,"['Deyao Zhu', 'Jun Chen', 'Xiaoqian Shen', 'Xiang Li', 'Mohamed Elhoseiny']","MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large
  Language Models",['cs.CV'],"  The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such
as directly generating websites from handwritten text and identifying humorous
elements within images. These features are rarely observed in previous
vision-language models. We believe the primary reason for GPT-4's advanced
multi-modal generation capabilities lies in the utilization of a more advanced
large language model (LLM). To examine this phenomenon, we present MiniGPT-4,
which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one
projection layer. Our findings reveal that MiniGPT-4 possesses many
capabilities similar to those exhibited by GPT-4 like detailed image
description generation and website creation from hand-written drafts.
Furthermore, we also observe other emerging capabilities in MiniGPT-4,
including writing stories and poems inspired by given images, providing
solutions to problems shown in images, teaching users how to cook based on food
photos, etc. In our experiment, we found that only performing the pretraining
on raw image-text pairs could produce unnatural language outputs that lack
coherency including repetition and fragmented sentences. To address this
problem, we curate a high-quality, well-aligned dataset in the second stage to
finetune our model using a conversational template. This step proved crucial
for augmenting the model's generation reliability and overall usability.
Notably, our model is highly computationally efficient, as we only train a
projection layer utilizing approximately 5 million aligned image-text pairs.
Our code, pre-trained model, and collected dataset are available at
https://minigpt-4.github.io/.
","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 18:25:35 GMT'}]",cs.CV,2023-04-20 18:25:35,"['language model', 'large language model', 'GPT-4']",True,Video & Multimodal Models,['kaust.edu.sa'],False,True,False,0.0,180.0,0.9975285758418289,0.9948654115450444,5,0.9958650707290533,False,True
11034,arXiv:2302.06476,"['Chengwei Qin', 'Aston Zhang', 'Zhuosheng Zhang', 'Jiaao Chen', 'Michihiro Yasunaga', 'Diyi Yang']",Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,"['cs.CL', 'cs.AI']","  Spurred by advancements in scale, large language models (LLMs) have
demonstrated the ability to perform a variety of natural language processing
(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,
the debut of ChatGPT has drawn a great deal of attention from the natural
language processing (NLP) community due to the fact that it can generate
high-quality responses to human input and self-correct previous mistakes based
on subsequent conversations. However, it is not yet known whether ChatGPT can
serve as a generalist model that can perform many NLP tasks zero-shot. In this
work, we empirically analyze the zero-shot learning ability of ChatGPT by
evaluating it on 20 popular NLP datasets covering 7 representative task
categories. With extensive empirical studies, we demonstrate both the
effectiveness and limitations of the current version of ChatGPT. We find that
ChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,
arithmetic reasoning) while it still faces challenges when solving specific
tasks such as sequence tagging. We additionally provide in-depth analysis
through qualitative case studies.
","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 09:44:51 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Feb 2023 17:46:20 GMT'}]",cs.CL,2023-02-08 09:44:51,"['language model', 'large language model', 'ChatGPT']",True,Human Feedback & Interaction,"['ntu.edu.sg', 'amazon.com']",True,True,False,0.0,176.0,0.991899852724595,0.9947098179555003,6,0.9956474428726877,True,True
12214,arXiv:2304.08485,"['Haotian Liu', 'Chunyuan Li', 'Qingyang Wu', 'Yong Jae Lee']",Visual Instruction Tuning,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","  Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.
","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 17:59:25 GMT'}]",cs.CV,2023-04-17 17:59:25,"['language model', 'large language model', 'GPT-4']",True,Vision-Language Models,[],False,False,False,0.0,172.0,0.9972196478220574,0.9945542243659561,4,0.9954298150163221,False,False
13076,arXiv:2305.10403,"['Rohan Anil', 'Andrew M. Dai', 'Orhan Firat', 'Melvin Johnson', 'Dmitry Lepikhin', 'Alexandre Passos', 'Siamak Shakeri', 'Emanuel Taropa', 'Paige Bailey', 'Zhifeng Chen', 'Eric Chu', 'Jonathan H. Clark', 'Laurent El Shafey', 'Yanping Huang', 'Kathy Meier-Hellstern', 'Gaurav Mishra', 'Erica Moreira', 'Mark Omernick', 'Kevin Robinson', 'Sebastian Ruder', 'Yi Tay', 'Kefan Xiao', 'Yuanzhong Xu', 'Yujing Zhang', 'Gustavo Hernandez Abrego', 'Junwhan Ahn', 'Jacob Austin', 'Paul Barham', 'Jan Botha', 'James Bradbury', 'Siddhartha Brahma', 'Kevin Brooks', 'Michele Catasta', 'Yong Cheng', 'Colin Cherry', 'Christopher A. Choquette-Choo', 'Aakanksha Chowdhery', 'Clément Crepy', 'Shachi Dave', 'Mostafa Dehghani', 'Sunipa Dev', 'Jacob Devlin', 'Mark Díaz', 'Nan Du', 'Ethan Dyer', 'Vlad Feinberg', 'Fangxiaoyu Feng', 'Vlad Fienber', 'Markus Freitag', 'Xavier Garcia', 'Sebastian Gehrmann', 'Lucas Gonzalez', 'Guy Gur-Ari', 'Steven Hand', 'Hadi Hashemi', 'Le Hou', 'Joshua Howland', 'Andrea Hu', 'Jeffrey Hui', 'Jeremy Hurwitz', 'Michael Isard', 'Abe Ittycheriah', 'Matthew Jagielski', 'Wenhao Jia', 'Kathleen Kenealy', 'Maxim Krikun', 'Sneha Kudugunta', 'Chang Lan', 'Katherine Lee', 'Benjamin Lee', 'Eric Li', 'Music Li', 'Wei Li', 'YaGuang Li', 'Jian Li', 'Hyeontaek Lim', 'Hanzhao Lin', 'Zhongtao Liu', 'Frederick Liu', 'Marcello Maggioni', 'Aroma Mahendru', 'Joshua Maynez', 'Vedant Misra', 'Maysam Moussalem', 'Zachary Nado', 'John Nham', 'Eric Ni', 'Andrew Nystrom', 'Alicia Parrish', 'Marie Pellat', 'Martin Polacek', 'Alex Polozov', 'Reiner Pope', 'Siyuan Qiao', 'Emily Reif', 'Bryan Richter', 'Parker Riley', 'Alex Castro Ros', 'Aurko Roy', 'Brennan Saeta', 'Rajkumar Samuel', 'Renee Shelby', 'Ambrose Slone', 'Daniel Smilkov', 'David R. So', 'Daniel Sohn', 'Simon Tokumine', 'Dasha Valter', 'Vijay Vasudevan', 'Kiran Vodrahalli', 'Xuezhi Wang', 'Pidong Wang', 'Zirui Wang', 'Tao Wang', 'John Wieting', 'Yuhuai Wu', 'Kelvin Xu', 'Yunhan Xu', 'Linting Xue', 'Pengcheng Yin', 'Jiahui Yu', 'Qiao Zhang', 'Steven Zheng', 'Ce Zheng', 'Weikang Zhou', 'Denny Zhou', 'Slav Petrov', 'Yonghui Wu']",PaLM 2 Technical Report,"['cs.CL', 'cs.AI']","  We introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
  When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report.
","[{'version': 'v1', 'created': 'Wed, 17 May 2023 17:46:53 GMT'}]",cs.CL,2023-05-17 17:46:53,"['language model', 'PaLM']",True,Pretrained LMs & Text Classification,['google.com'],True,False,False,0.1489361702,168.0,0.9969107198022861,0.994398630776412,128,0.9952121871599565,True,True
11397,arXiv:2303.04671,"['Chenfei Wu', 'Shengming Yin', 'Weizhen Qi', 'Xiaodong Wang', 'Zecheng Tang', 'Nan Duan']","Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation
  Models",['cs.CV'],"  ChatGPT is attracting a cross-field interest as it provides a language
interface with remarkable conversational competency and reasoning capabilities
across many domains. However, since ChatGPT is trained with languages, it is
currently not capable of processing or generating images from the visual world.
At the same time, Visual Foundation Models, such as Visual Transformers or
Stable Diffusion, although showing great visual understanding and generation
capabilities, they are only experts on specific tasks with one-round fixed
inputs and outputs. To this end, We build a system called \textbf{Visual
ChatGPT}, incorporating different Visual Foundation Models, to enable the user
to interact with ChatGPT by 1) sending and receiving not only languages but
also images 2) providing complex visual questions or visual editing
instructions that require the collaboration of multiple AI models with
multi-steps. 3) providing feedback and asking for corrected results. We design
a series of prompts to inject the visual model information into ChatGPT,
considering models of multiple inputs/outputs and models that require visual
feedback. Experiments show that Visual ChatGPT opens the door to investigating
the visual roles of ChatGPT with the help of Visual Foundation Models. Our
system is publicly available at
\url{https://github.com/microsoft/visual-chatgpt}.
","[{'version': 'v1', 'created': 'Wed, 8 Mar 2023 15:50:02 GMT'}]",cs.CV,2023-03-08 15:50:02,"['ChatGPT', 'foundation model']",True,Applications of LLMs/ChatGPT,['microsoft.com'],True,False,False,0.0,158.0,0.9911634756995582,0.9942430371868679,6,0.9949945593035908,False,True
11688,arXiv:2303.13375,"['Harsha Nori', 'Nicholas King', 'Scott Mayer McKinney', 'Dean Carignan', 'Eric Horvitz']",Capabilities of GPT-4 on Medical Challenge Problems,"['cs.CL', 'cs.AI']","  Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.
","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 16:18:38 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Apr 2023 16:48:39 GMT'}]",cs.CL,2023-03-20 16:18:38,"['language model', 'GPT-3', 'PaLM', 'GPT-4', 'large language model']",True,NLP for Healthcare,[],False,False,False,0.0,141.0,0.9904270986745214,0.9940874435973238,5,0.9947769314472252,False,True
12013,arXiv:2304.03277,"['Baolin Peng', 'Chunyuan Li', 'Pengcheng He', 'Michel Galley', 'Jianfeng Gao']",Instruction Tuning with GPT-4,"['cs.CL', 'cs.AI']","  Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.
","[{'version': 'v1', 'created': 'Thu, 6 Apr 2023 17:58:09 GMT'}]",cs.CL,2023-04-06 17:58:09,"['language model', 'large language model', 'LLaMA', 'GPT-4']",True,Prompts & In-Context Learning,['microsoft.com'],True,False,False,0.0,138.0,0.9966017917825146,0.9939318500077797,5,0.9945593035908596,True,True
10688,arXiv:2301.07597,"['Biyang Guo', 'Xin Zhang', 'Ziyuan Wang', 'Minqi Jiang', 'Jinran Nie', 'Yuxuan Ding', 'Jianwei Yue', 'Yupeng Wu']","How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,
  and Detection",['cs.CL'],"  The introduction of ChatGPT has garnered widespread attention in both
academic and industrial communities. ChatGPT is able to respond effectively to
a wide range of human questions, providing fluent and comprehensive answers
that significantly surpass previous public chatbots in terms of security and
usefulness. On one hand, people are curious about how ChatGPT is able to
achieve such strength and how far it is from human experts. On the other hand,
people are starting to worry about the potential negative impacts that large
language models (LLMs) like ChatGPT could have on society, such as fake news,
plagiarism, and social security issues. In this work, we collected tens of
thousands of comparison responses from both human experts and ChatGPT, with
questions ranging from open-domain, financial, medical, legal, and
psychological areas. We call the collected dataset the Human ChatGPT Comparison
Corpus (HC3). Based on the HC3 dataset, we study the characteristics of
ChatGPT's responses, the differences and gaps from human experts, and future
directions for LLMs. We conducted comprehensive human evaluations and
linguistic analyses of ChatGPT-generated content compared with that of humans,
where many interesting results are revealed. After that, we conduct extensive
experiments on how to effectively detect whether a certain text is generated by
ChatGPT or humans. We build three different detection systems, explore several
key factors that influence their effectiveness, and evaluate them in different
scenarios. The dataset, code, and models are all publicly available at
https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.
","[{'version': 'v1', 'created': 'Wed, 18 Jan 2023 15:23:25 GMT'}]",cs.CL,2023-01-18 15:23:25,"['language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,[],False,False,,,133.0,0.9893225331369662,0.9936984596234635,8,0.9942328618063112,True,False
10558,arXiv:2301.00704,"['Huiwen Chang', 'Han Zhang', 'Jarred Barber', 'AJ Maschinot', 'Jose Lezama', 'Lu Jiang', 'Ming-Hsuan Yang', 'Kevin Murphy', 'William T. Freeman', 'Michael Rubinstein', 'Yuanzhen Li', 'Dilip Krishnan']",Muse: Text-To-Image Generation via Masked Generative Transformers,"['cs.CV', 'cs.AI', 'cs.LG']","  We present Muse, a text-to-image Transformer model that achieves
state-of-the-art image generation performance while being significantly more
efficient than diffusion or autoregressive models. Muse is trained on a masked
modeling task in discrete token space: given the text embedding extracted from
a pre-trained large language model (LLM), Muse is trained to predict randomly
masked image tokens. Compared to pixel-space diffusion models, such as Imagen
and DALL-E 2, Muse is significantly more efficient due to the use of discrete
tokens and requiring fewer sampling iterations; compared to autoregressive
models, such as Parti, Muse is more efficient due to the use of parallel
decoding. The use of a pre-trained LLM enables fine-grained language
understanding, translating to high-fidelity image generation and the
understanding of visual concepts such as objects, their spatial relationships,
pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M,
with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88
on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also
directly enables a number of image editing applications without the need to
fine-tune or invert the model: inpainting, outpainting, and mask-free editing.
More results are available at https://muse-model.github.io
","[{'version': 'v1', 'created': 'Mon, 2 Jan 2023 14:43:38 GMT'}]",cs.CV,2023-01-02 14:43:38,"['language model', 'large language model']",True,Video & Multimodal Models,['google.com'],True,False,False,0.0,133.0,0.9893225331369662,0.9936984596234635,12,0.9942328618063112,False,False
10935,arXiv:2302.03494,['Ali Borji'],A Categorical Archive of ChatGPT Failures,"['cs.CL', 'cs.AI', 'cs.LG']","  Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
","[{'version': 'v1', 'created': 'Mon, 6 Feb 2023 04:21:59 GMT'}, {'version': 'v2', 'created': 'Fri, 10 Feb 2023 01:01:51 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Feb 2023 09:26:35 GMT'}, {'version': 'v4', 'created': 'Sun, 19 Feb 2023 03:12:05 GMT'}, {'version': 'v5', 'created': 'Tue, 21 Feb 2023 05:27:25 GMT'}, {'version': 'v6', 'created': 'Thu, 23 Feb 2023 00:05:29 GMT'}, {'version': 'v7', 'created': 'Mon, 6 Mar 2023 09:34:38 GMT'}, {'version': 'v8', 'created': 'Mon, 3 Apr 2023 20:02:26 GMT'}]",cs.CL,2023-02-06 04:21:59,"['language model', 'large language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,[],False,False,False,0.0,129.0,0.9882179675994109,0.9934650692391473,1,0.9939064200217628,False,False
11259,arXiv:2302.14045,"['Shaohan Huang', 'Li Dong', 'Wenhui Wang', 'Yaru Hao', 'Saksham Singhal', 'Shuming Ma', 'Tengchao Lv', 'Lei Cui', 'Owais Khan Mohammed', 'Barun Patra', 'Qiang Liu', 'Kriti Aggarwal', 'Zewen Chi', 'Johan Bjorck', 'Vishrav Chaudhary', 'Subhojit Som', 'Xia Song', 'Furu Wei']",Language Is Not All You Need: Aligning Perception with Language Models,"['cs.CL', 'cs.CV']","  A big convergence of language, multimodal perception, action, and world
modeling is a key step toward artificial general intelligence. In this work, we
introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive
general modalities, learn in context (i.e., few-shot), and follow instructions
(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale
multimodal corpora, including arbitrarily interleaved text and images,
image-caption pairs, and text data. We evaluate various settings, including
zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range
of tasks without any gradient updates or finetuning. Experimental results show
that Kosmos-1 achieves impressive performance on (i) language understanding,
generation, and even OCR-free NLP (directly fed with document images), (ii)
perception-language tasks, including multimodal dialogue, image captioning,
visual question answering, and (iii) vision tasks, such as image recognition
with descriptions (specifying classification via text instructions). We also
show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge
from language to multimodal, and from multimodal to language. In addition, we
introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning
capability of MLLMs.
","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 18:55:27 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 11:04:51 GMT'}]",cs.CL,2023-02-27 18:55:27,"['language model', 'large language model']",True,Vision-Language Models,[],False,False,False,0.1428571429,123.0,0.9874815905743741,0.9932316788548312,18,0.9935799782372143,True,True
12021,arXiv:2304.03442,"['Joon Sung Park', ""Joseph C. O'Brien"", 'Carrie J. Cai', 'Meredith Ringel Morris', 'Percy Liang', 'Michael S. Bernstein']",Generative Agents: Interactive Simulacra of Human Behavior,"['cs.HC', 'cs.AI', 'cs.LG']","  Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 01:55:19 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Aug 2023 00:21:19 GMT'}]",cs.HC,2023-04-07 01:55:19,"['language model', 'large language model']",True,Human Feedback & Interaction,"['google.com', 'stanford.edu']",True,True,False,0.4,123.0,0.9962928637627433,0.9932316788548312,6,0.9935799782372143,True,True
11175,arXiv:2302.11382,"['Jules White', 'Quchen Fu', 'Sam Hays', 'Michael Sandborn', 'Carlos Olea', 'Henry Gilbert', 'Ashraf Elnashar', 'Jesse Spencer-Smith', 'Douglas C. Schmidt']",A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,"['cs.SE', 'cs.AI']","  Prompt engineering is an increasingly important skill set needed to converse
effectively with large language models (LLMs), such as ChatGPT. Prompts are
instructions given to an LLM to enforce rules, automate processes, and ensure
specific qualities (and quantities) of generated output. Prompts are also a
form of programming that can customize the outputs and interactions with an
LLM. This paper describes a catalog of prompt engineering techniques presented
in pattern form that have been applied to solve common problems when conversing
with LLMs. Prompt patterns are a knowledge transfer method analogous to
software patterns since they provide reusable solutions to common problems
faced in a particular context, i.e., output generation and interaction when
working with LLMs. This paper provides the following contributions to research
on prompt engineering that apply LLMs to automate software development tasks.
First, it provides a framework for documenting patterns for structuring prompts
to solve a range of problems so that they can be adapted to different domains.
Second, it presents a catalog of patterns that have been applied successfully
to improve the outputs of LLM conversations. Third, it explains how prompts can
be built from multiple patterns and illustrates prompt patterns that benefit
from combination with other prompt patterns.
","[{'version': 'v1', 'created': 'Tue, 21 Feb 2023 12:42:44 GMT'}]",cs.SE,2023-02-21 12:42:44,"['language model', 'large language model', 'ChatGPT']",True,"Software, Planning, Robotics",['vanderbilt.edu'],False,True,False,0.0,118.0,0.9867452135493373,0.992998288470515,9,0.9932535364526659,False,False
13092,arXiv:2305.10601,"['Shunyu Yao', 'Dian Yu', 'Jeffrey Zhao', 'Izhak Shafran', 'Thomas L. Griffiths', 'Yuan Cao', 'Karthik Narasimhan']",Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"['cs.CL', 'cs.AI', 'cs.LG']","  Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/ysymyth/tree-of-thought-llm.
","[{'version': 'v1', 'created': 'Wed, 17 May 2023 23:16:17 GMT'}]",cs.CL,2023-05-17 23:16:17,"['language model', 'large language model', 'GPT-4']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.1666666667,117.0,0.9959839357429718,0.9928426948809709,7,0.9930359085963003,True,True
11928,arXiv:2304.01373,"['Stella Biderman', 'Hailey Schoelkopf', 'Quentin Anthony', 'Herbie Bradley', ""Kyle O'Brien"", 'Eric Hallahan', 'Mohammad Aflah Khan', 'Shivanshu Purohit', 'USVSN Sai Prashanth', 'Edward Raff', 'Aviya Skowron', 'Lintang Sutawika', 'Oskar van der Wal']","Pythia: A Suite for Analyzing Large Language Models Across Training and
  Scaling",['cs.CL'],"  How do large language models (LLMs) develop and evolve over the course of
training? How do these patterns change as models scale? To answer these
questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on
public data seen in the exact same order and ranging in size from 70M to 12B
parameters. We provide public access to 154 checkpoints for each one of the 16
models, alongside tools to download and reconstruct their exact training
dataloaders for further study. We intend \textit{Pythia} to facilitate research
in many areas, and we present several case studies including novel results in
memorization, term frequency effects on few-shot performance, and reducing
gender bias. We demonstrate that this highly controlled setup can be used to
yield novel insights toward LLMs and their training dynamics. Trained models,
analysis code, training code, and training data can be found at
\url{https://github.com/EleutherAI/pythia}.
","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 20:58:15 GMT'}, {'version': 'v2', 'created': 'Wed, 31 May 2023 17:54:07 GMT'}]",cs.CL,2023-04-03 20:58:15,"['language model', 'large language model']",True,Datasets & Benchmarks,['eleuther.ai'],False,False,False,0.30000000000000004,113.0,0.9955205437133148,0.9925315077018827,13,0.9926006528835691,True,True
11757,arXiv:2303.15056,"['Fabrizio Gilardi', 'Meysam Alizadeh', 'Maël Kubli']",ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks,"['cs.CL', 'cs.CY']","  Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification.
","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 09:59:48 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jul 2023 14:10:55 GMT'}]",cs.CL,2023-03-27 09:59:48,"['language model', 'large language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,[],False,False,False,0.0,113.0,0.9860088365243005,0.9925315077018827,3,0.9926006528835691,True,False
14514,arXiv:2306.05685,"['Lianmin Zheng', 'Wei-Lin Chiang', 'Ying Sheng', 'Siyuan Zhuang', 'Zhanghao Wu', 'Yonghao Zhuang', 'Zi Lin', 'Zhuohan Li', 'Dacheng Li', 'Eric. P Xing', 'Hao Zhang', 'Joseph E. Gonzalez', 'Ion Stoica']",Judging LLM-as-a-judge with MT-Bench and Chatbot Arena,"['cs.CL', 'cs.AI']","  Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, including position, verbosity, and
self-enhancement biases, as well as limited reasoning ability, and propose
solutions to mitigate some of them. We then verify the agreement between LLM
judges and human preferences by introducing two benchmarks: MT-bench, a
multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our
results reveal that strong LLM judges like GPT-4 can match both controlled and
crowdsourced human preferences well, achieving over 80\% agreement, the same
level of agreement between humans. Hence, LLM-as-a-judge is a scalable and
explainable way to approximate human preferences, which are otherwise very
expensive to obtain. Additionally, we show our benchmark and traditional
benchmarks complement each other by evaluating several variants of LLaMA and
Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K
conversations with human preferences from Chatbot Arena.
","[{'version': 'v1', 'created': 'Fri, 9 Jun 2023 05:55:52 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Jul 2023 01:42:26 GMT'}]",cs.CL,2023-06-09 05:55:52,"['language model', 'large language model', 'LLaMA', 'GPT-4']",True,Applications and Benchmark Evaluations,[],False,False,False,0.0,113.0,0.9955205437133148,0.9925315077018827,13,0.9926006528835691,True,True
10842,arXiv:2301.13867,"['Simon Frieder', 'Luca Pinchetti', 'Alexis Chevalier', 'Ryan-Rhys Griffiths', 'Tommaso Salvatori', 'Thomas Lukasiewicz', 'Philipp Christian Petersen', 'Julius Berner']",Mathematical Capabilities of ChatGPT,"['cs.LG', 'cs.AI', 'cs.CL']","  We investigate the mathematical capabilities of two iterations of ChatGPT
(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on
publicly available datasets, as well as hand-crafted ones, using a novel
methodology. In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, either cover
only elementary mathematics or are very small. We address this by publicly
releasing two new datasets: GHOSTS and miniGHOSTS. These are the first
natural-language datasets curated by working researchers in mathematics that
(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of
the mathematical capabilities of language models, and (3) distinguish multiple
dimensions of mathematical reasoning. These datasets also test whether ChatGPT
and GPT-4 can be helpful assistants to professional mathematicians by emulating
use cases that arise in the daily professional activities of mathematicians. We
benchmark the models on a range of fine-grained performance metrics. For
advanced mathematics, this is the most detailed evaluation effort to date. We
find that ChatGPT can be used most successfully as a mathematical assistant for
querying facts, acting as a mathematical search engine and knowledge base
interface. GPT-4 can additionally be used for undergraduate-level mathematics
but fails on graduate-level difficulty. Contrary to many positive reports in
the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of
selection bias), their overall mathematical performance is well below the level
of a graduate student. Hence, if your goal is to use ChatGPT to pass a
graduate-level math exam, you would be better off copying from your average
peer!
","[{'version': 'v1', 'created': 'Tue, 31 Jan 2023 18:59:03 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jul 2023 17:59:14 GMT'}]",cs.LG,2023-01-31 18:59:03,"['language model', 'ChatGPT', 'GPT-4']",True,Applications of LLMs/ChatGPT,['ox.ac.uk'],False,True,False,0.0,112.0,0.9852724594992637,0.9921425237280224,8,0.9920565832426551,False,False
12160,arXiv:2304.07193,"['Maxime Oquab', 'Timothée Darcet', 'Théo Moutakanni', 'Huy Vo', 'Marc Szafraniec', 'Vasil Khalidov', 'Pierre Fernandez', 'Daniel Haziza', 'Francisco Massa', 'Alaaeldin El-Nouby', 'Mahmoud Assran', 'Nicolas Ballas', 'Wojciech Galuba', 'Russell Howes', 'Po-Yao Huang', 'Shang-Wen Li', 'Ishan Misra', 'Michael Rabbat', 'Vasu Sharma', 'Gabriel Synnaeve', 'Hu Xu', 'Hervé Jegou', 'Julien Mairal', 'Patrick Labatut', 'Armand Joulin', 'Piotr Bojanowski']",DINOv2: Learning Robust Visual Features without Supervision,['cs.CV'],"  The recent breakthroughs in natural language processing for model pretraining
on large quantities of data have opened the way for similar foundation models
in computer vision. These models could greatly simplify the use of images in
any system by producing all-purpose visual features, i.e., features that work
across image distributions and tasks without finetuning. This work shows that
existing pretraining methods, especially self-supervised methods, can produce
such features if trained on enough curated data from diverse sources. We
revisit existing approaches and combine different techniques to scale our
pretraining in terms of data and model size. Most of the technical
contributions aim at accelerating and stabilizing the training at scale. In
terms of data, we propose an automatic pipeline to build a dedicated, diverse,
and curated image dataset instead of uncurated data, as typically done in the
self-supervised literature. In terms of models, we train a ViT model
(Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of
smaller models that surpass the best available all-purpose features, OpenCLIP
(Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.
","[{'version': 'v1', 'created': 'Fri, 14 Apr 2023 15:12:19 GMT'}]",cs.CV,2023-04-14 15:12:19,['foundation model'],True,Vision-Language Models,['fb.com'],True,False,False,0.0,112.0,0.9950571516836577,0.9921425237280224,26,0.9920565832426551,False,True
11081,arXiv:2302.07842,"['Grégoire Mialon', 'Roberto Dessì', 'Maria Lomeli', 'Christoforos Nalmpantis', 'Ram Pasunuru', 'Roberta Raileanu', 'Baptiste Rozière', 'Timo Schick', 'Jane Dwivedi-Yu', 'Asli Celikyilmaz', 'Edouard Grave', 'Yann LeCun', 'Thomas Scialom']",Augmented Language Models: a Survey,['cs.CL'],"  This survey reviews works in which language models (LMs) are augmented with
reasoning skills and the ability to use tools. The former is defined as
decomposing a potentially complex task into simpler subtasks while the latter
consists in calling external modules such as a code interpreter. LMs can
leverage these augmentations separately or in combination via heuristics, or
learn to do so from demonstrations. While adhering to a standard missing tokens
prediction objective, such augmented LMs can use various, possibly
non-parametric external modules to expand their context processing ability,
thus departing from the pure language modeling paradigm. We therefore refer to
them as Augmented Language Models (ALMs). The missing token objective allows
ALMs to learn to reason, use tools, and even act, while still performing
standard natural language tasks and even outperforming most regular LMs on
several benchmarks. In this work, after reviewing current advance in ALMs, we
conclude that this new research direction has the potential to address common
limitations of traditional LMs such as interpretability, consistency, and
scalability issues.
","[{'version': 'v1', 'created': 'Wed, 15 Feb 2023 18:25:52 GMT'}]",cs.CL,2023-02-15 18:25:52,['language model'],True,"LLMs, Reasoning, Chain-of-Thought",['fb.com'],True,False,False,0.3076923077,110.0,0.9837997054491899,0.9917535397541621,13,0.991512513601741,False,True
11856,arXiv:2303.17651,"['Aman Madaan', 'Niket Tandon', 'Prakhar Gupta', 'Skyler Hallinan', 'Luyu Gao', 'Sarah Wiegreffe', 'Uri Alon', 'Nouha Dziri', 'Shrimai Prabhumoye', 'Yiming Yang', 'Shashank Gupta', 'Bodhisattwa Prasad Majumder', 'Katherine Hermann', 'Sean Welleck', 'Amir Yazdanbakhsh', 'Peter Clark']",Self-Refine: Iterative Refinement with Self-Feedback,"['cs.CL', 'cs.AI', 'cs.LG']","  Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach.
","[{'version': 'v1', 'created': 'Thu, 30 Mar 2023 18:30:01 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 19:13:47 GMT'}]",cs.CL,2023-03-30 18:30:01,"['language model', 'GPT-3', 'GPT-4', 'large language model', 'ChatGPT']",True,Human Feedback & Interaction,"['allenai.org', 'cmu.edu']",False,True,False,0.2727272727,110.0,0.9837997054491899,0.9917535397541621,16,0.991512513601741,True,True
11138,arXiv:2302.09419,"['Ce Zhou', 'Qian Li', 'Chen Li', 'Jun Yu', 'Yixin Liu', 'Guangjing Wang', 'Kai Zhang', 'Cheng Ji', 'Qiben Yan', 'Lifang He', 'Hao Peng', 'Jianxin Li', 'Jia Wu', 'Ziwei Liu', 'Pengtao Xie', 'Caiming Xiong', 'Jian Pei', 'Philip S. Yu', 'Lichao Sun']","A Comprehensive Survey on Pretrained Foundation Models: A History from
  BERT to ChatGPT","['cs.AI', 'cs.CL', 'cs.LG']","  Pretrained Foundation Models (PFMs) are regarded as the foundation for
various downstream tasks with different data modalities. A PFM (e.g., BERT,
ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable
parameter initialization for a wide range of downstream applications. BERT
learns bidirectional encoder representations from Transformers, which are
trained on large datasets as contextual language models. Similarly, the
generative pretrained transformer (GPT) method employs Transformers as the
feature extractor and is trained using an autoregressive paradigm on large
datasets. Recently, ChatGPT shows promising success on large language models,
which applies an autoregressive language model with zero shot or few shot
prompting. The remarkable achievements of PFM have brought significant
breakthroughs to various fields of AI. Numerous studies have proposed different
methods, raising the demand for an updated survey. This study provides a
comprehensive review of recent research advancements, challenges, and
opportunities for PFMs in text, image, graph, as well as other data modalities.
The review covers the basic components and existing pretraining methods used in
natural language processing, computer vision, and graph learning. Additionally,
it explores advanced PFMs used for different data modalities and unified PFMs
that consider data quality and quantity. The review also discusses research
related to the fundamentals of PFMs, such as model efficiency and compression,
security, and privacy. Finally, the study provides key implications, future
research directions, challenges, and open problems in the field of PFMs.
Overall, this survey aims to shed light on the research of the PFMs on
scalability, security, logical reasoning ability, cross-domain learning
ability, and the user-friendly interactive ability for artificial general
intelligence.
","[{'version': 'v1', 'created': 'Sat, 18 Feb 2023 20:51:09 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Mar 2023 14:44:09 GMT'}, {'version': 'v3', 'created': 'Mon, 1 May 2023 07:48:05 GMT'}]",cs.AI,2023-02-18 20:51:09,"['language model', 'foundation model', 'GPT-4', 'large language model', 'BERT', 'ChatGPT']",True,Fine-Tuning & Domain Adaptation,['msu.edu'],False,True,False,0.0,110.0,0.9837997054491899,0.9917535397541621,19,0.991512513601741,False,True
10738,arXiv:2301.10226,"['John Kirchenbauer', 'Jonas Geiping', 'Yuxin Wen', 'Jonathan Katz', 'Ian Miers', 'Tom Goldstein']",A Watermark for Large Language Models,"['cs.LG', 'cs.CL', 'cs.CR']","  Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of ""green"" tokens
before a word is generated, and then softly promoting use of green tokens
during sampling. We propose a statistical test for detecting the watermark with
interpretable p-values, and derive an information-theoretic framework for
analyzing the sensitivity of the watermark. We test the watermark using a
multi-billion parameter model from the Open Pretrained Transformer (OPT)
family, and discuss robustness and security.
","[{'version': 'v1', 'created': 'Tue, 24 Jan 2023 18:52:59 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Jan 2023 18:54:34 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Jun 2023 17:50:01 GMT'}]",cs.LG,2023-01-24 18:52:59,"['language model', 'large language model']",True,Privacy & Adversarial Risks,['umd.edu'],False,True,False,0.0,103.0,0.9823269513991163,0.991442352575074,6,0.9910772578890098,False,True
10767,arXiv:2301.11305,"['Eric Mitchell', 'Yoonho Lee', 'Alexander Khazatsky', 'Christopher D. Manning', 'Chelsea Finn']","DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability
  Curvature","['cs.CL', 'cs.AI']","  The increasing fluency and widespread usage of large language models (LLMs)
highlight the desirability of corresponding tools aiding detection of
LLM-generated text. In this paper, we identify a property of the structure of
an LLM's probability function that is useful for such detection. Specifically,
we demonstrate that text sampled from an LLM tends to occupy negative curvature
regions of the model's log probability function. Leveraging this observation,
we then define a new curvature-based criterion for judging if a passage is
generated from a given LLM. This approach, which we call DetectGPT, does not
require training a separate classifier, collecting a dataset of real or
generated passages, or explicitly watermarking generated text. It uses only log
probabilities computed by the model of interest and random perturbations of the
passage from another generic pre-trained language model (e.g., T5). We find
DetectGPT is more discriminative than existing zero-shot methods for model
sample detection, notably improving detection of fake news articles generated
by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline
to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,
data, and other project information.
","[{'version': 'v1', 'created': 'Thu, 26 Jan 2023 18:44:06 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Jul 2023 04:18:36 GMT'}]",cs.CL,2023-01-26 18:44:06,"['language model', 'large language model', 'GPT-Neo']",True,Societal Implications of LLMs,['stanford.edu'],False,True,False,0.2,101.0,0.9815905743740795,0.9912867589855298,5,0.9908596300326442,True,True
11002,arXiv:2302.05442,"['Mostafa Dehghani', 'Josip Djolonga', 'Basil Mustafa', 'Piotr Padlewski', 'Jonathan Heek', 'Justin Gilmer', 'Andreas Steiner', 'Mathilde Caron', 'Robert Geirhos', 'Ibrahim Alabdulmohsin', 'Rodolphe Jenatton', 'Lucas Beyer', 'Michael Tschannen', 'Anurag Arnab', 'Xiao Wang', 'Carlos Riquelme', 'Matthias Minderer', 'Joan Puigcerver', 'Utku Evci', 'Manoj Kumar', 'Sjoerd van Steenkiste', 'Gamaleldin F. Elsayed', 'Aravindh Mahendran', 'Fisher Yu', 'Avital Oliver', 'Fantine Huot', 'Jasmijn Bastings', 'Mark Patrick Collier', 'Alexey Gritsenko', 'Vighnesh Birodkar', 'Cristina Vasconcelos', 'Yi Tay', 'Thomas Mensink', 'Alexander Kolesnikov', 'Filip Pavetić', 'Dustin Tran', 'Thomas Kipf', 'Mario Lučić', 'Xiaohua Zhai', 'Daniel Keysers', 'Jeremiah Harmsen', 'Neil Houlsby']",Scaling Vision Transformers to 22 Billion Parameters,"['cs.CV', 'cs.AI', 'cs.LG']","  The scaling of Transformers has driven breakthrough capabilities for language
models. At present, the largest large language models (LLMs) contain upwards of
100B parameters. Vision Transformers (ViT) have introduced the same
architecture to image and video modelling, but these have not yet been
successfully scaled to nearly the same degree; the largest dense ViT contains
4B parameters (Chen et al., 2022). We present a recipe for highly efficient and
stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of
experiments on the resulting model. When evaluated on downstream tasks (often
with a lightweight linear model on frozen features), ViT-22B demonstrates
increasing performance with scale. We further observe other interesting
benefits of scale, including an improved tradeoff between fairness and
performance, state-of-the-art alignment to human visual perception in terms of
shape/texture bias, and improved robustness. ViT-22B demonstrates the potential
for ""LLM-like"" scaling in vision, and provides key steps towards getting there.
","[{'version': 'v1', 'created': 'Fri, 10 Feb 2023 18:58:21 GMT'}]",cs.CV,2023-02-10 18:58:21,"['language model', 'large language model']",True,Vision-Language Models,['google.com'],True,False,False,0.1111111111,98.0,0.9808541973490427,0.9911311653959857,42,0.9906420021762785,True,True
11795,arXiv:2303.16199,"['Renrui Zhang', 'Jiaming Han', 'Chris Liu', 'Peng Gao', 'Aojun Zhou', 'Xiangfei Hu', 'Shilin Yan', 'Pan Lu', 'Hongsheng Li', 'Yu Qiao']","LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init
  Attention","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.MM']","  We present LLaMA-Adapter, a lightweight adaption method to efficiently
fine-tune LLaMA into an instruction-following model. Using 52K self-instruct
demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon
the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8
A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and
prepend them to the word tokens at higher transformer layers. Then, a
zero-initialized attention mechanism with zero gating is proposed, which
adaptively injects the new instructional cues into LLaMA, while effectively
preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter
can generate high-quality responses, comparable to Alpaca with fully fine-tuned
7B parameters. Besides language commands, our approach can be simply extended
to multi-modal instructions for learning image-conditioned LLaMA model, which
achieves superior reasoning performance on ScienceQA and COCO Caption
benchmarks. Furthermore, we also evaluate the zero-initialized attention
mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on
traditional vision and language tasks, demonstrating the superior
generalization capacity of our approach. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 17:59:12 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Jun 2023 17:31:32 GMT'}]",cs.CV,2023-03-28 17:59:12,"['language model', 'BERT', 'LLaMA']",True,Fine-Tuning & Domain Adaptation,['pjlab.org.cn'],False,True,True,0.5,94.0,0.9801178203240059,0.9909755718064416,10,0.9904243743199129,True,True
12874,arXiv:2305.06500,"['Wenliang Dai', 'Junnan Li', 'Dongxu Li', 'Anthony Meng Huat Tiong', 'Junqi Zhao', 'Weisheng Wang', 'Boyang Li', 'Pascale Fung', 'Steven Hoi']","InstructBLIP: Towards General-purpose Vision-Language Models with
  Instruction Tuning","['cs.CV', 'cs.LG']","  Large-scale pre-training and instruction tuning have been successful at
creating general-purpose language models with broad competence. However,
building general-purpose vision-language models is challenging due to the rich
input distributions and task diversity resulting from the additional visual
input. Although vision-language pretraining has been widely studied,
vision-language instruction tuning remains under-explored. In this paper, we
conduct a systematic and comprehensive study on vision-language instruction
tuning based on the pretrained BLIP-2 models. We gather 26 publicly available
datasets, covering a wide variety of tasks and capabilities, and transform them
into instruction tuning format. Additionally, we introduce an instruction-aware
Query Transformer, which extracts informative features tailored to the given
instruction. Trained on 13 held-in datasets, InstructBLIP attains
state-of-the-art zero-shot performance across all 13 held-out datasets,
substantially outperforming BLIP-2 and larger Flamingo models. Our models also
lead to state-of-the-art performance when finetuned on individual downstream
tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts).
Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over
concurrent multimodal models. All InstructBLIP models are open-sourced at
https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.
","[{'version': 'v1', 'created': 'Thu, 11 May 2023 00:38:10 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Jun 2023 08:00:18 GMT'}]",cs.CV,2023-05-11 00:38:10,['language model'],True,Vision-Language Models,['salesforce.com'],True,False,False,0.1428571429,89.0,0.9947482236638863,0.9908199782168975,9,0.9902067464635473,True,True
11570,arXiv:2303.10130,"['Tyna Eloundou', 'Sam Manning', 'Pamela Mishkin', 'Daniel Rock']","GPTs are GPTs: An Early Look at the Labor Market Impact Potential of
  Large Language Models","['econ.GN', 'cs.AI', 'cs.CY', 'q-fin.EC']","  We investigate the potential implications of large language models (LLMs),
such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market,
focusing on the increased capabilities arising from LLM-powered software
compared to LLMs on their own. Using a new rubric, we assess occupations based
on their alignment with LLM capabilities, integrating both human expertise and
GPT-4 classifications. Our findings reveal that around 80% of the U.S.
workforce could have at least 10% of their work tasks affected by the
introduction of LLMs, while approximately 19% of workers may see at least 50%
of their tasks impacted. We do not make predictions about the development or
adoption timeline of such LLMs. The projected effects span all wage levels,
with higher-income jobs potentially facing greater exposure to LLM capabilities
and LLM-powered software. Significantly, these impacts are not restricted to
industries with higher recent productivity growth. Our analysis suggests that,
with access to an LLM, about 15% of all worker tasks in the US could be
completed significantly faster at the same level of quality. When incorporating
software and tooling built on top of LLMs, this share increases to between 47
and 56% of all tasks. This finding implies that LLM-powered software will have
a substantial effect on scaling the economic impacts of the underlying models.
We conclude that LLMs such as GPTs exhibit traits of general-purpose
technologies, indicating that they could have considerable economic, social,
and policy implications.
","[{'version': 'v1', 'created': 'Fri, 17 Mar 2023 17:15:20 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Mar 2023 02:29:47 GMT'}, {'version': 'v3', 'created': 'Wed, 22 Mar 2023 03:32:25 GMT'}, {'version': 'v4', 'created': 'Thu, 23 Mar 2023 21:54:09 GMT'}, {'version': 'v5', 'created': 'Mon, 21 Aug 2023 07:58:25 GMT'}]",econ.GN,2023-03-17 17:15:20,"['language model', 'large language model', 'GPT-4']",True,Applications of LLMs/ChatGPT,['openai.com'],True,False,True,0.5,88.0,0.979381443298969,0.9906643846273534,4,0.9899891186071818,True,False
13535,arXiv:2305.14314,"['Tim Dettmers', 'Artidoro Pagnoni', 'Ari Holtzman', 'Luke Zettlemoyer']",QLoRA: Efficient Finetuning of Quantized LLMs,['cs.LG'],"  We present QLoRA, an efficient finetuning approach that reduces memory usage
enough to finetune a 65B parameter model on a single 48GB GPU while preserving
full 16-bit finetuning task performance. QLoRA backpropagates gradients through
a frozen, 4-bit quantized pretrained language model into Low Rank
Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all
previous openly released models on the Vicuna benchmark, reaching 99.3% of the
performance level of ChatGPT while only requiring 24 hours of finetuning on a
single GPU. QLoRA introduces a number of innovations to save memory without
sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is
information theoretically optimal for normally distributed weights (b) double
quantization to reduce the average memory footprint by quantizing the
quantization constants, and (c) paged optimziers to manage memory spikes. We
use QLoRA to finetune more than 1,000 models, providing a detailed analysis of
instruction following and chatbot performance across 8 instruction datasets,
multiple model types (LLaMA, T5), and model scales that would be infeasible to
run with regular finetuning (e.g. 33B and 65B parameter models). Our results
show that QLoRA finetuning on a small high-quality dataset leads to
state-of-the-art results, even when using smaller models than the previous
SoTA. We provide a detailed analysis of chatbot performance based on both human
and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable
alternative to human evaluation. Furthermore, we find that current chatbot
benchmarks are not trustworthy to accurately evaluate the performance levels of
chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to
ChatGPT. We release all of our models and code, including CUDA kernels for
4-bit training.
","[{'version': 'v1', 'created': 'Tue, 23 May 2023 17:50:33 GMT'}]",cs.LG,2023-05-23 17:50:33,"['language model', 'LLaMA', 'GPT-4', 'pretrained language model', 'ChatGPT']",True,Efficiency & Performance,['washington.edu'],False,True,False,0.0,87.0,0.9944392956441149,0.9905087910378092,4,0.989771490750816,True,True
11849,arXiv:2303.17564,"['Shijie Wu', 'Ozan Irsoy', 'Steven Lu', 'Vadim Dabravolski', 'Mark Dredze', 'Sebastian Gehrmann', 'Prabhanjan Kambadur', 'David Rosenberg', 'Gideon Mann']",BloombergGPT: A Large Language Model for Finance,"['cs.LG', 'cs.AI', 'cs.CL', 'q-fin.GN']","  The use of NLP in the realm of financial technology is broad and complex,
with applications ranging from sentiment analysis and named entity recognition
to question answering. Large Language Models (LLMs) have been shown to be
effective on a variety of tasks; however, no LLM specialized for the financial
domain has been reported in literature. In this work, we present BloombergGPT,
a 50 billion parameter language model that is trained on a wide range of
financial data. We construct a 363 billion token dataset based on Bloomberg's
extensive data sources, perhaps the largest domain-specific dataset yet,
augmented with 345 billion tokens from general purpose datasets. We validate
BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite
of internal benchmarks that most accurately reflect our intended usage. Our
mixed dataset training leads to a model that outperforms existing models on
financial tasks by significant margins without sacrificing performance on
general LLM benchmarks. Additionally, we explain our modeling choices, training
process, and evaluation methodology. We release Training Chronicles (Appendix
C) detailing our experience in training BloombergGPT.
","[{'version': 'v1', 'created': 'Thu, 30 Mar 2023 17:30:36 GMT'}, {'version': 'v2', 'created': 'Tue, 9 May 2023 16:06:35 GMT'}]",cs.LG,2023-03-30 17:30:36,"['language model', 'large language model']",True,Finance Applications,['bloomberg.net'],True,False,False,0.0,85.0,0.9786450662739322,0.9903531974482651,9,0.9895538628944505,True,False
11217,arXiv:2302.12813,"['Baolin Peng', 'Michel Galley', 'Pengcheng He', 'Hao Cheng', 'Yujia Xie', 'Yu Hu', 'Qiuyuan Huang', 'Lars Liden', 'Zhou Yu', 'Weizhu Chen', 'Jianfeng Gao']","Check Your Facts and Try Again: Improving Large Language Models with
  External Knowledge and Automated Feedback","['cs.CL', 'cs.AI']","  Large language models (LLMs), such as ChatGPT, are able to generate
human-like, fluent responses for many downstream tasks, e.g., task-oriented
dialog and question answering. However, applying LLMs to real-world,
mission-critical applications remains challenging mainly due to their tendency
to generate hallucinations and their inability to use external knowledge. This
paper proposes a LLM-Augmenter system, which augments a black-box LLM with a
set of plug-and-play modules. Our system makes the LLM generate responses
grounded in external knowledge, e.g., stored in task-specific databases. It
also iteratively revises LLM prompts to improve model responses using feedback
generated by utility functions, e.g., the factuality score of a LLM-generated
response. The effectiveness of LLM-Augmenter is empirically validated on two
types of scenarios, task-oriented dialog and open-domain question answering.
LLM-Augmenter significantly reduces ChatGPT's hallucinations without
sacrificing the fluency and informativeness of its responses. We make the
source code and models publicly available.
","[{'version': 'v1', 'created': 'Fri, 24 Feb 2023 18:48:43 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 17:21:48 GMT'}, {'version': 'v3', 'created': 'Wed, 8 Mar 2023 23:41:49 GMT'}]",cs.CL,2023-02-24 18:48:43,"['language model', 'large language model', 'ChatGPT']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.0,84.0,0.9779086892488954,0.990197603858721,11,0.9893362350380849,True,True
10585,arXiv:2301.02111,"['Chengyi Wang', 'Sanyuan Chen', 'Yu Wu', 'Ziqiang Zhang', 'Long Zhou', 'Shujie Liu', 'Zhuo Chen', 'Yanqing Liu', 'Huaming Wang', 'Jinyu Li', 'Lei He', 'Sheng Zhao', 'Furu Wei']",Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,"['cs.CL', 'cs.SD', 'eess.AS']","  We introduce a language modeling approach for text to speech synthesis (TTS).
Specifically, we train a neural codec language model (called Vall-E) using
discrete codes derived from an off-the-shelf neural audio codec model, and
regard TTS as a conditional language modeling task rather than continuous
signal regression as in previous work. During the pre-training stage, we scale
up the TTS training data to 60K hours of English speech which is hundreds of
times larger than existing systems. Vall-E emerges in-context learning
capabilities and can be used to synthesize high-quality personalized speech
with only a 3-second enrolled recording of an unseen speaker as an acoustic
prompt. Experiment results show that Vall-E significantly outperforms the
state-of-the-art zero-shot TTS system in terms of speech naturalness and
speaker similarity. In addition, we find Vall-E could preserve the speaker's
emotion and acoustic environment of the acoustic prompt in synthesis. See
https://aka.ms/valle for demos of our work.
","[{'version': 'v1', 'created': 'Thu, 5 Jan 2023 15:37:15 GMT'}]",cs.CL,2023-01-05 15:37:15,['language model'],True,Speech Recognition,['microsoft.com'],True,False,False,0.0,82.0,0.9768041237113402,0.9899642134744049,13,0.9890097932535364,True,True
11811,arXiv:2303.16634,"['Yang Liu', 'Dan Iter', 'Yichong Xu', 'Shuohang Wang', 'Ruochen Xu', 'Chenguang Zhu']",G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,"['cs.CL', 'cs.AI']","  The quality of texts generated by natural language generation (NLG) systems
is hard to measure automatically. Conventional reference-based metrics, such as
BLEU and ROUGE, have been shown to have relatively low correlation with human
judgments, especially for tasks that require creativity and diversity. Recent
studies suggest using large language models (LLMs) as reference-free metrics
for NLG evaluation, which have the benefit of being applicable to new tasks
that lack human references. However, these LLM-based evaluators still have
lower human correspondence than medium-size neural evaluators. In this work, we
present G-Eval, a framework of using large language models with
chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of
NLG outputs. We experiment with two generation tasks, text summarization and
dialogue generation. We show that G-Eval with GPT-4 as the backbone model
achieves a Spearman correlation of 0.514 with human on summarization task,
outperforming all previous methods by a large margin. We also propose
preliminary analysis on the behavior of LLM-based evaluators, and highlight the
potential issue of LLM-based evaluators having a bias towards the LLM-generated
texts. The code is at https://github.com/nlpyang/geval
","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 12:46:54 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 23:49:08 GMT'}, {'version': 'v3', 'created': 'Tue, 23 May 2023 22:12:16 GMT'}]",cs.CL,2023-03-29 12:46:54,"['language model', 'large language model', 'GPT-4']",True,Summarization and Evaluation,['microsoft.com'],True,False,False,0.0,82.0,0.9768041237113402,0.9899642134744049,6,0.9890097932535364,True,True
10694,arXiv:2301.08653,"['Dominik Sobania', 'Martin Briesch', 'Carol Hanna', 'Justyna Petke']",An Analysis of the Automatic Bug Fixing Performance of ChatGPT,['cs.SE'],"  To support software developers in finding and fixing software bugs, several
automated program repair techniques have been introduced. Given a test suite,
standard methods usually either synthesize a repair, or navigate a search space
of software edits to find test-suite passing variants. Recent program repair
methods are based on deep learning approaches. One of these novel methods,
which is not primarily intended for automated program repair, but is still
suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is
so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard
bug fixing benchmark set, QuixBugs, and compare the performance with the
results of several other approaches reported in the literature. We find that
ChatGPT's bug fixing performance is competitive to the common deep learning
approaches CoCoNut and Codex and notably better than the results reported for
the standard program repair approaches. In contrast to previous approaches,
ChatGPT offers a dialogue system through which further information, e.g., the
expected output for a certain input or an observed error message, can be
entered. By providing such hints to ChatGPT, its success rate can be further
increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.
","[{'version': 'v1', 'created': 'Fri, 20 Jan 2023 16:01:47 GMT'}]",cs.SE,2023-01-20 16:01:47,['ChatGPT'],True,Code Generation,"['uni-mainz.de', 'ucl.ac.uk']",False,True,True,0.5,81.0,0.975699558173785,0.9897308230900886,4,0.988683351468988,True,False
11947,arXiv:2304.01852,"['Yiheng Liu', 'Tianle Han', 'Siyuan Ma', 'Jiayue Zhang', 'Yuanyuan Yang', 'Jiaming Tian', 'Hao He', 'Antong Li', 'Mengshen He', 'Zhengliang Liu', 'Zihao Wu', 'Lin Zhao', 'Dajiang Zhu', 'Xiang Li', 'Ning Qiang', 'Dingang Shen', 'Tianming Liu', 'Bao Ge']","Summary of ChatGPT-Related Research and Perspective Towards the Future
  of Large Language Models",['cs.CL'],"  This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and
GPT-4) research, state-of-the-art large language models (LLM) from the GPT
series, and their prospective applications across diverse domains. Indeed, key
innovations such as large-scale pre-training that captures knowledge across the
entire world wide web, instruction fine-tuning and Reinforcement Learning from
Human Feedback (RLHF) have played significant roles in enhancing LLMs'
adaptability and performance. We performed an in-depth analysis of 194 relevant
papers on arXiv, encompassing trend analysis, word cloud representation, and
distribution analysis across various application domains. The findings reveal a
significant and increasing interest in ChatGPT-related research, predominantly
centered on direct natural language processing applications, while also
demonstrating considerable potential in areas ranging from education and
history to mathematics, medicine, and physics. This study endeavors to furnish
insights into ChatGPT's capabilities, potential implications, ethical concerns,
and offer direction for future advancements in this field.
","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 15:01:06 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Apr 2023 14:42:40 GMT'}, {'version': 'v3', 'created': 'Thu, 11 May 2023 03:50:53 GMT'}, {'version': 'v4', 'created': 'Tue, 22 Aug 2023 03:18:43 GMT'}]",cs.CL,2023-04-04 15:01:06,"['language model', 'GPT-3', 'GPT-4', 'large language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,['snnu.edu.cn'],False,False,False,0.1428571429,80.0,0.9941303676243435,0.9895752295005446,18,0.9884657236126224,False,False
10899,arXiv:2302.02083,['Michal Kosinski'],Theory of Mind Might Have Spontaneously Emerged in Large Language Models,"['cs.CL', 'cs.CY', 'cs.HC']","  We explore the intriguing possibility that theory of mind (ToM), or the
uniquely human ability to impute unobservable mental states to others, might
have spontaneously emerged in large language models (LLMs). We designed 40
false-belief tasks, considered a gold standard in testing ToM in humans, and
administered them to several LLMs. Each task included a false-belief scenario,
three closely matched true-belief controls, and the reversed versions of all
four. Smaller and older models solved no tasks; GPT-3-davinci-001 (from May
2020) and GPT-3-davinci-002 (from January 2022) solved 10%; and
GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023)
solved 35% of the tasks, mirroring the performance of three-year-old children.
ChatGPT-4 (from June 2023) solved 90% of the tasks, matching the performance of
seven-year-old children. These findings suggest the intriguing possibility that
ToM, previously considered exclusive to humans, may have spontaneously emerged
as a byproduct of LLMs' improving language skills.
","[{'version': 'v1', 'created': 'Sat, 4 Feb 2023 03:50:01 GMT'}, {'version': 'v2', 'created': 'Fri, 10 Feb 2023 19:01:49 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Mar 2023 18:49:26 GMT'}, {'version': 'v4', 'created': 'Tue, 29 Aug 2023 14:55:37 GMT'}]",cs.CL,2023-02-04 03:50:01,"['language model', 'GPT-3', 'GPT-4', 'large language model', 'ChatGPT']",True,Applications and Benchmark Evaluations,['stanford.edu'],False,True,,,79.0,0.9749631811487481,0.9894196359110005,1,0.9882480957562568,True,True
10671,arXiv:2301.06627,"['Kyle Mahowald', 'Anna A. Ivanova', 'Idan A. Blank', 'Nancy Kanwisher', 'Joshua B. Tenenbaum', 'Evelina Fedorenko']","Dissociating language and thought in large language models: a cognitive
  perspective","['cs.CL', 'cs.AI']","  Today's large language models (LLMs) routinely generate coherent, grammatical
and seemingly meaningful paragraphs of text. This achievement has led to
speculation that these networks are -- or will soon become -- ""thinking
machines"", capable of performing tasks that require abstract knowledge and
reasoning. Here, we review the capabilities of LLMs by considering their
performance on two different aspects of language use: 'formal linguistic
competence', which includes knowledge of rules and patterns of a given
language, and 'functional linguistic competence', a host of cognitive abilities
required for language understanding and use in the real world. Drawing on
evidence from cognitive neuroscience, we show that formal competence in humans
relies on specialized language processing mechanisms, whereas functional
competence recruits multiple extralinguistic capacities that comprise human
thought, such as formal reasoning, world knowledge, situation modeling, and
social cognition. In line with this distinction, LLMs show impressive (although
imperfect) performance on tasks requiring formal linguistic competence, but
fail on many tests requiring functional competence. Based on this evidence, we
argue that (1) contemporary LLMs should be taken seriously as models of formal
linguistic skills; (2) models that master real-life language use would need to
incorporate or develop not only a core language module, but also multiple
non-language-specific cognitive capacities required for modeling thought.
Overall, a distinction between formal and functional linguistic competence
helps clarify the discourse surrounding LLMs' potential and provides a path
toward building models that understand and use language in human-like ways.
","[{'version': 'v1', 'created': 'Mon, 16 Jan 2023 22:41:19 GMT'}]",cs.CL,2023-01-16 22:41:19,"['language model', 'large language model']",True,Applications and Benchmark Evaluations,"['utexas.edu', 'mit.edu', 'ucla.edu']",False,True,True,0.5,78.0,0.9742268041237113,0.9892640423214564,6,0.9880304678998912,True,True
11609,arXiv:2303.11381,"['Zhengyuan Yang', 'Linjie Li', 'Jianfeng Wang', 'Kevin Lin', 'Ehsan Azarnasab', 'Faisal Ahmed', 'Zicheng Liu', 'Ce Liu', 'Michael Zeng', 'Lijuan Wang']",MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,"['cs.CV', 'cs.CL', 'cs.LG']","  We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of
vision experts to achieve multimodal reasoning and action. In this paper, we
define and explore a comprehensive list of advanced vision tasks that are
intriguing to solve, but may exceed the capabilities of existing vision and
vision-language models. To achieve such advanced visual intelligence, MM-REACT
introduces a textual prompt design that can represent text descriptions,
textualized spatial coordinates, and aligned file names for dense visual
signals such as images and videos. MM-REACT's prompt design allows language
models to accept, associate, and process multimodal information, thereby
facilitating the synergetic combination of ChatGPT and various vision experts.
Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the
specified capabilities of interests and its wide application in different
scenarios that require advanced visual understanding. Furthermore, we discuss
and compare MM-REACT's system paradigm with an alternative approach that
extends language models for multimodal scenarios through joint finetuning.
Code, demo, video, and visualization are available at
https://multimodal-react.github.io/
","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 18:31:47 GMT'}]",cs.CV,2023-03-20 18:31:47,"['language model', 'ChatGPT']",True,"Software, Planning, Robotics",['microsoft.com'],True,False,False,0.1666666667,77.0,0.9734904270986745,0.9890306519371402,10,0.9877040261153428,True,True
12853,arXiv:2305.06161,"['Raymond Li', 'Loubna Ben Allal', 'Yangtian Zi', 'Niklas Muennighoff', 'Denis Kocetkov', 'Chenghao Mou', 'Marc Marone', 'Christopher Akiki', 'Jia Li', 'Jenny Chim', 'Qian Liu', 'Evgenii Zheltonozhskii', 'Terry Yue Zhuo', 'Thomas Wang', 'Olivier Dehaene', 'Mishig Davaadorj', 'Joel Lamy-Poirier', 'João Monteiro', 'Oleh Shliazhko', 'Nicolas Gontier', 'Nicholas Meade', 'Armel Zebaze', 'Ming-Ho Yee', 'Logesh Kumar Umapathi', 'Jian Zhu', 'Benjamin Lipkin', 'Muhtasham Oblokulov', 'Zhiruo Wang', 'Rudra Murthy', 'Jason Stillerman', 'Siva Sankalp Patel', 'Dmitry Abulkhanov', 'Marco Zocca', 'Manan Dey', 'Zhihan Zhang', 'Nour Fahmy', 'Urvashi Bhattacharyya', 'Wenhao Yu', 'Swayam Singh', 'Sasha Luccioni', 'Paulo Villegas', 'Maxim Kunakov', 'Fedor Zhdanov', 'Manuel Romero', 'Tony Lee', 'Nadav Timor', 'Jennifer Ding', 'Claire Schlesinger', 'Hailey Schoelkopf', 'Jan Ebert', 'Tri Dao', 'Mayank Mishra', 'Alex Gu', 'Jennifer Robinson', 'Carolyn Jane Anderson', 'Brendan Dolan-Gavitt', 'Danish Contractor', 'Siva Reddy', 'Daniel Fried', 'Dzmitry Bahdanau', 'Yacine Jernite', 'Carlos Muñoz Ferrandis', 'Sean Hughes', 'Thomas Wolf', 'Arjun Guha', 'Leandro von Werra', 'Harm de Vries']",StarCoder: may the source be with you!,"['cs.CL', 'cs.AI', 'cs.PL', 'cs.SE']","  The BigCode community, an open-scientific collaboration working on the
responsible development of Large Language Models for Code (Code LLMs),
introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context
length, infilling capabilities and fast large-batch inference enabled by
multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced
from The Stack, a large collection of permissively licensed GitHub repositories
with inspection tools and an opt-out process. We fine-tuned StarCoderBase on
35B Python tokens, resulting in the creation of StarCoder. We perform the most
comprehensive evaluation of Code LLMs to date and show that StarCoderBase
outperforms every open Code LLM that supports multiple programming languages
and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,
StarCoder outperforms every model that is fine-tuned on Python, can be prompted
to achieve 40\% pass@1 on HumanEval, and still retains its performance on other
programming languages. We take several important steps towards a safe
open-access model release, including an improved PII redaction pipeline and a
novel attribution tracing tool, and make the StarCoder models publicly
available under a more commercially viable version of the Open Responsible AI
Model license.
","[{'version': 'v1', 'created': 'Tue, 9 May 2023 08:16:42 GMT'}]",cs.CL,2023-05-09 08:16:42,"['language model', 'large language model']",True,Code Generation,['bigcode-project.org'],False,False,False,0.1764705882,77.0,0.9938214396045721,0.9890306519371402,67,0.9877040261153428,True,True
12471,arXiv:2304.14178,"['Qinghao Ye', 'Haiyang Xu', 'Guohai Xu', 'Jiabo Ye', 'Ming Yan', 'Yiyang Zhou', 'Junyang Wang', 'Anwen Hu', 'Pengcheng Shi', 'Yaya Shi', 'Chenliang Li', 'Yuanhong Xu', 'Hehong Chen', 'Junfeng Tian', 'Qian Qi', 'Ji Zhang', 'Fei Huang']","mPLUG-Owl: Modularization Empowers Large Language Models with
  Multimodality","['cs.CL', 'cs.CV', 'cs.LG']","  Large language models (LLMs) have demonstrated impressive zero-shot abilities
on a variety of open-ended tasks, while recent research has also explored the
use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,
a novel training paradigm that equips LLMs with multi-modal abilities through
modularized learning of foundation LLM, a visual knowledge module, and a visual
abstractor module. This approach can support multiple modalities and facilitate
diverse unimodal and multimodal abilities through modality collaboration. The
training paradigm of mPLUG-Owl involves a two-stage method for aligning image
and text, which learns visual knowledge with the assistance of LLM while
maintaining and even improving the generation abilities of LLM. In the first
stage, the visual knowledge module and abstractor module are trained with a
frozen LLM module to align the image and text. In the second stage,
language-only and multi-modal supervised datasets are used to jointly fine-tune
a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing
the visual knowledge module. We carefully build a visually-related instruction
evaluation set OwlEval. Experimental results show that our model outperforms
existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction
and visual understanding ability, multi-turn conversation ability, and
knowledge reasoning ability. Besides, we observe some unexpected and exciting
abilities such as multi-image correlation and scene text understanding, which
makes it possible to leverage it for harder real scenarios, such as vision-only
document comprehension. Our code, pre-trained model, instruction-tuned models,
and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The
online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.
","[{'version': 'v1', 'created': 'Thu, 27 Apr 2023 13:27:01 GMT'}]",cs.CL,2023-04-27 13:27:01,"['language model', 'large language model']",True,Vision-Language Models,['alibaba-inc.com'],True,False,False,0.33333333330000003,76.0,0.9935125115848007,0.988797261552824,17,0.9873775843307944,False,True
11391,arXiv:2303.04226,"['Yihan Cao', 'Siyu Li', 'Yixin Liu', 'Zhiling Yan', 'Yutong Dai', 'Philip S. Yu', 'Lichao Sun']","A Comprehensive Survey of AI-Generated Content (AIGC): A History of
  Generative AI from GAN to ChatGPT","['cs.AI', 'cs.CL', 'cs.LG']","  Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant
attention from society. As a result, many individuals have become interested in
related resources and are seeking to uncover the background and secrets behind
its impressive performance. In fact, ChatGPT and other Generative AI (GAI)
techniques belong to the category of Artificial Intelligence Generated Content
(AIGC), which involves the creation of digital content, such as images, music,
and natural language, through AI models. The goal of AIGC is to make the
content creation process more efficient and accessible, allowing for the
production of high-quality content at a faster pace. AIGC is achieved by
extracting and understanding intent information from instructions provided by
human, and generating the content according to its knowledge and the intent
information. In recent years, large-scale models have become increasingly
important in AIGC as they provide better intent extraction and thus, improved
generation results. With the growth of data and the size of the models, the
distribution that the model can learn becomes more comprehensive and closer to
reality, leading to more realistic and high-quality content generation. This
survey provides a comprehensive review on the history of generative models, and
basic components, recent advances in AIGC from unimodal interaction and
multimodal interaction. From the perspective of unimodality, we introduce the
generation tasks and relative models of text and image. From the perspective of
multimodality, we introduce the cross-application between the modalities
mentioned above. Finally, we discuss the existing open problems and future
challenges in AIGC.
","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 20:36:13 GMT'}]",cs.AI,2023-03-07 20:36:13,['ChatGPT'],True,Applications of LLMs/ChatGPT,"['cmu.edu', 'uic.edu', 'lehigh.edu']",False,True,False,0.0,73.0,0.9727540500736377,0.9886416679632799,7,0.9871599564744288,False,True
