,id,authors,title,categories,abstract,versions,first_category,v1_date,LM_related_terms,mentions_LM_keyword,cluster,domains,industry,academic,above_pred_female_threshold,inferred_female_frac_nqg_uncertainty_threshold_0.100,citationCount,percentile_rank_in_3_month_window,percentile_rank_in_12_month_window,n_authors,percentile_rank_in_6_month_window
18,arXiv:1802.05365,"['Matthew E. Peters', 'Mark Neumann', 'Mohit Iyyer', 'Matt Gardner', 'Christopher Clark', 'Kenton Lee', 'Luke Zettlemoyer']",Deep contextualized word representations,['cs.CL'],"  We introduce a new type of deep contextualized word representation that
models both (1) complex characteristics of word use (e.g., syntax and
semantics), and (2) how these uses vary across linguistic contexts (i.e., to
model polysemy). Our word vectors are learned functions of the internal states
of a deep bidirectional language model (biLM), which is pre-trained on a large
text corpus. We show that these representations can be easily added to existing
models and significantly improve the state of the art across six challenging
NLP problems, including question answering, textual entailment and sentiment
analysis. We also present an analysis showing that exposing the deep internals
of the pre-trained network is crucial, allowing downstream models to mix
different types of semi-supervision signals.
","[{'version': 'v1', 'created': 'Thu, 15 Feb 2018 00:05:11 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Mar 2018 21:59:40 GMT'}]",cs.CL,2018-02-15 00:05:11,['language model'],True,"Representations, Syntax, Semantics","['allenai.org', 'washington.edu']",False,True,False,0.0,9927.0,1.0,0.9940652818991098,7,1.0
20,arXiv:1802.05814,"['Dawen Liang', 'Rahul G. Krishnan', 'Matthew D. Hoffman', 'Tony Jebara']",Variational Autoencoders for Collaborative Filtering,"['stat.ML', 'cs.IR', 'cs.LG']","  We extend variational autoencoders (VAEs) to collaborative filtering for
implicit feedback. This non-linear probabilistic model enables us to go beyond
the limited modeling capacity of linear factor models which still largely
dominate collaborative filtering research.We introduce a generative model with
multinomial likelihood and use Bayesian inference for parameter estimation.
Despite widespread use in language modeling and economics, the multinomial
likelihood receives less attention in the recommender systems literature. We
introduce a different regularization parameter for the learning objective,
which proves to be crucial for achieving competitive performance. Remarkably,
there is an efficient way to tune the parameter using annealing. The resulting
model and learning algorithm has information-theoretic connections to maximum
entropy discrimination and the information bottleneck principle. Empirically,
we show that the proposed approach significantly outperforms several
state-of-the-art baselines, including two recently-proposed neural network
approaches, on several real-world datasets. We also provide extended
experiments comparing the multinomial likelihood with other commonly used
likelihood functions in the latent factor collaborative filtering literature
and show favorable results. Finally, we identify the pros and cons of employing
a principled Bayesian inference approach and characterize settings where it
provides the most significant improvements.
","[{'version': 'v1', 'created': 'Fri, 16 Feb 2018 01:41:20 GMT'}]",stat.ML,2018-02-16 01:41:20,['language model'],True,Entity Extraction & RecSys,"['netflix.com', 'mit.edu', 'google.com']",True,True,False,0.0,838.0,0.9814814814814815,0.9851632047477745,4,0.9855072463768116
49,arXiv:1803.10609,"['Jon Barker', 'Shinji Watanabe', 'Emmanuel Vincent', 'Jan Trmal']","The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset,
  task and baselines","['cs.SD', 'cs.AI', 'eess.AS']","  The CHiME challenge series aims to advance robust automatic speech
recognition (ASR) technology by promoting research at the interface of speech
and language processing, signal processing , and machine learning. This paper
introduces the 5th CHiME Challenge, which considers the task of distant
multi-microphone conversational ASR in real home environments. Speech material
was elicited using a dinner party scenario with efforts taken to capture data
that is representative of natural conversational speech and recorded by 6
Kinect microphone arrays and 4 binaural microphone pairs. The challenge
features a single-array track and a multiple-array track and, for each track,
distinct rankings will be produced for systems focusing on robustness with
respect to distant-microphone capture vs. systems attempting to address all
aspects of the task including conversational language modeling. We discuss the
rationale for the challenge and provide a detailed description of the data
collection procedure, the task, and the baseline systems for array
synchronization, speech enhancement, and conventional and end-to-end ASR.
","[{'version': 'v1', 'created': 'Wed, 28 Mar 2018 13:51:09 GMT'}]",cs.SD,2018-03-28 13:51:09,['language model'],True,Speech Recognition,"['sheffield.ac.uk', 'jhu.edu', 'inria.fr']",False,True,False,0.0,817.0,0.9629629629629629,0.9821958456973294,4,0.9782608695652174
5,arXiv:1801.07736,"['William Fedus', 'Ian Goodfellow', 'Andrew M. Dai']",MaskGAN: Better Text Generation via Filling in the______,"['stat.ML', 'cs.AI', 'cs.LG']","  Neural text generation models are often autoregressive language models or
seq2seq models. These models generate text by sampling words sequentially, with
each word conditioned on the previous word, and are state-of-the-art for
several machine translation and summarization benchmarks. These benchmarks are
often defined by validation perplexity even though this is not a direct measure
of the quality of the generated text. Additionally, these models are typically
trained via maxi- mum likelihood and teacher forcing. These methods are
well-suited to optimizing perplexity but can result in poor sample quality
since generating text requires conditioning on sequences of words that may have
never been observed at training time. We propose to improve sample quality
using Generative Adversarial Networks (GANs), which explicitly train the
generator to produce high quality samples and have shown a lot of success in
image generation. GANs were originally designed to output differentiable
values, so discrete language generation is challenging for them. We claim that
validation perplexity alone is not indicative of the quality of text generated
by a model. We introduce an actor-critic conditional GAN that fills in missing
text conditioned on the surrounding context. We show qualitatively and
quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.
","[{'version': 'v1', 'created': 'Tue, 23 Jan 2018 19:22:21 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Feb 2018 16:26:04 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Mar 2018 15:30:09 GMT'}]",stat.ML,2018-01-23 19:22:21,['language model'],True,Text Generation,['google.com'],True,False,False,0.0,430.0,0.9444444444444444,0.9584569732937686,3,0.9420289855072463
53,arXiv:1803.11138,"['Kristina Gulordava', 'Piotr Bojanowski', 'Edouard Grave', 'Tal Linzen', 'Marco Baroni']",Colorless green recurrent networks dream hierarchically,['cs.CL'],"  Recurrent neural networks (RNNs) have achieved impressive results in a
variety of linguistic processing tasks, suggesting that they can induce
non-trivial properties of language. We investigate here to what extent RNNs
learn to track abstract hierarchical syntactic structure. We test whether RNNs
trained with a generic language modeling objective in four languages (Italian,
English, Hebrew, Russian) can predict long-distance number agreement in various
constructions. We include in our evaluation nonsensical sentences where RNNs
cannot rely on semantic or lexical cues (""The colorless green ideas I ate with
the chair sleep furiously""), and, for Italian, we compare model performance to
human intuitions. Our language-model-trained RNNs make reliable predictions
about long-distance agreement, and do not lag much behind human performance. We
thus bring support to the hypothesis that RNNs are not just shallow-pattern
extractors, but they also acquire deeper grammatical competence.
","[{'version': 'v1', 'created': 'Thu, 29 Mar 2018 16:27:36 GMT'}]",cs.CL,2018-03-29 16:27:36,['language model'],True,"Representations, Syntax, Semantics","['jhu.edu', 'unige.ch', 'fb.com']",True,True,False,0.25,416.0,0.9259259259259259,0.9554896142433235,5,0.9347826086956522
133,arXiv:1806.09055,"['Hanxiao Liu', 'Karen Simonyan', 'Yiming Yang']",DARTS: Differentiable Architecture Search,"['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']","  This paper addresses the scalability challenge of architecture search by
formulating the task in a differentiable manner. Unlike conventional approaches
of applying evolution or reinforcement learning over a discrete and
non-differentiable search space, our method is based on the continuous
relaxation of the architecture representation, allowing efficient search of the
architecture using gradient descent. Extensive experiments on CIFAR-10,
ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in
discovering high-performance convolutional architectures for image
classification and recurrent architectures for language modeling, while being
orders of magnitude faster than state-of-the-art non-differentiable techniques.
Our implementation has been made publicly available to facilitate further
research on efficient architecture search algorithms.
","[{'version': 'v1', 'created': 'Sun, 24 Jun 2018 00:06:13 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Apr 2019 06:29:32 GMT'}]",cs.LG,2018-06-24 00:06:13,['language model'],True,Efficiency & Performance,"['cmu.edu', 'google.com']",True,True,True,1.0,3245.0,1.0,0.9910979228486647,3,0.9927536231884058
73,arXiv:1804.10959,['Taku Kudo'],"Subword Regularization: Improving Neural Network Translation Models with
  Multiple Subword Candidates",['cs.CL'],"  Subword units are an effective way to alleviate the open vocabulary problems
in neural machine translation (NMT). While sentences are usually converted into
unique subword sequences, subword segmentation is potentially ambiguous and
multiple segmentations are possible even with the same vocabulary. The question
addressed in this paper is whether it is possible to harness the segmentation
ambiguity as a noise to improve the robustness of NMT. We present a simple
regularization method, subword regularization, which trains the model with
multiple subword segmentations probabilistically sampled during training. In
addition, for better subword sampling, we propose a new subword segmentation
algorithm based on a unigram language model. We experiment with multiple
corpora and report consistent improvements especially on low resource and
out-of-domain settings.
","[{'version': 'v1', 'created': 'Sun, 29 Apr 2018 16:13:44 GMT'}]",cs.CL,2018-04-29 16:13:44,['language model'],True,"Representations, Syntax, Semantics",['google.com'],True,False,False,0.0,811.0,0.9880952380952381,0.9792284866468842,1,0.9710144927536232
85,arXiv:1805.04770,"['Tommaso Furlanello', 'Zachary C. Lipton', 'Michael Tschannen', 'Laurent Itti', 'Anima Anandkumar']",Born Again Neural Networks,"['stat.ML', 'cs.AI', 'cs.LG']","  Knowledge distillation (KD) consists of transferring knowledge from one
machine learning model (the teacher}) to another (the student). Commonly, the
teacher is a high-capacity model with formidable performance, while the student
is more compact. By transferring knowledge, one hopes to benefit from the
student's compactness. %we desire a compact model with performance close to the
teacher's. We study KD from a new perspective: rather than compressing models,
we train students parameterized identically to their teachers. Surprisingly,
these {Born-Again Networks (BANs), outperform their teachers significantly,
both on computer vision and language modeling tasks. Our experiments with BANs
based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10
(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional
experiments explore two distillation objectives: (i) Confidence-Weighted by
Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).
Both methods elucidate the essential components of KD, demonstrating a role of
the teacher outputs on both predicted and non-predicted classes. We present
experiments with students of various capacities, focusing on the under-explored
case where students overpower teachers. Our experiments show significant
advantages from transferring knowledge between DenseNets and ResNets in either
direction.
","[{'version': 'v1', 'created': 'Sat, 12 May 2018 19:48:50 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Jun 2018 10:46:28 GMT'}]",stat.ML,2018-05-12 19:48:50,['language model'],True,Knowledge Distillation,['usc.edu'],False,True,False,0.2,774.0,0.9761904761904762,0.9762611275964391,5,0.9637681159420289
62,arXiv:1804.07755,"['Guillaume Lample', 'Myle Ott', 'Alexis Conneau', 'Ludovic Denoyer', ""Marc'Aurelio Ranzato""]",Phrase-Based & Neural Unsupervised Machine Translation,['cs.CL'],"  Machine translation systems achieve near human-level performance on some
languages, yet their effectiveness strongly relies on the availability of large
amounts of parallel sentences, which hinders their applicability to the
majority of language pairs. This work investigates how to learn to translate
when having access to only large monolingual corpora in each language. We
propose two model variants, a neural and a phrase-based model. Both versions
leverage a careful initialization of the parameters, the denoising effect of
language models and automatic generation of parallel data by iterative
back-translation. These models are significantly better than methods from the
literature, while being simpler and having fewer hyper-parameters. On the
widely used WMT'14 English-French and WMT'16 German-English benchmarks, our
models respectively obtain 28.1 and 25.2 BLEU points without using a single
parallel sentence, outperforming the state of the art by more than 11 BLEU
points. On low-resource languages like English-Urdu and English-Romanian, our
methods achieve even better results than semi-supervised and supervised
approaches leveraging the paucity of available bitexts. Our code for NMT and
PBSMT is publicly available.
","[{'version': 'v1', 'created': 'Fri, 20 Apr 2018 17:59:13 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Aug 2018 22:50:37 GMT'}]",cs.CL,2018-04-20 17:59:13,['language model'],True,Translation & Low-Resource Languages,"['lip6.fr', 'fb.com']",True,False,False,0.0,614.0,0.9642857142857143,0.9703264094955489,5,0.9565217391304348
90,arXiv:1805.06201,['Sosuke Kobayashi'],"Contextual Augmentation: Data Augmentation by Words with Paradigmatic
  Relations","['cs.CL', 'cs.LG']","  We propose a novel data augmentation for labeled sentences called contextual
augmentation. We assume an invariance that sentences are natural even if the
words in the sentences are replaced with other words with paradigmatic
relations. We stochastically replace words with other words that are predicted
by a bi-directional language model at the word positions. Words predicted
according to a context are numerous but appropriate for the augmentation of the
original words. Furthermore, we retrofit a language model with a
label-conditional architecture, which allows the model to augment sentences
without breaking the label-compatibility. Through the experiments for six
various different text classification tasks, we demonstrate that the proposed
method improves classifiers based on the convolutional or recurrent neural
networks.
","[{'version': 'v1', 'created': 'Wed, 16 May 2018 09:10:21 GMT'}]",cs.CL,2018-05-16 09:10:21,['language model'],True,"Representations, Syntax, Semantics",['preferred.jp'],False,False,False,0.0,462.0,0.9523809523809523,0.9614243323442137,1,0.9492753623188406
170,arXiv:1808.05326,"['Rowan Zellers', 'Yonatan Bisk', 'Roy Schwartz', 'Yejin Choi']","SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense
  Inference",['cs.CL'],"  Given a partial description like ""she opened the hood of the car,"" humans can
reason about the situation and anticipate what might come next (""then, she
examined the engine""). In this paper, we introduce the task of grounded
commonsense inference, unifying natural language inference and commonsense
reasoning.
  We present SWAG, a new dataset with 113k multiple choice questions about a
rich spectrum of grounded situations. To address the recurring challenges of
the annotation artifacts and human biases found in many existing datasets, we
propose Adversarial Filtering (AF), a novel procedure that constructs a
de-biased dataset by iteratively training an ensemble of stylistic classifiers,
and using them to filter the data. To account for the aggressive adversarial
filtering, we use state-of-the-art language models to massively oversample a
diverse set of potential counterfactuals. Empirical results demonstrate that
while humans can solve the resulting inference problems with high accuracy
(88%), various competitive models struggle on our task. We provide
comprehensive analysis that indicates significant opportunities for future
research.
","[{'version': 'v1', 'created': 'Thu, 16 Aug 2018 02:21:01 GMT'}]",cs.CL,2018-08-16 02:21:01,['language model'],True,Knowledge Graphs and Commonsense,['washington.edu'],False,True,False,0.33333333330000003,585.0,1.0,0.9673590504451038,4,0.9798994974874372
174,arXiv:1808.07233,"['Renqian Luo', 'Fei Tian', 'Tao Qin', 'Enhong Chen', 'Tie-Yan Liu']",Neural Architecture Optimization,"['cs.LG', 'stat.ML']","  Automatic neural architecture design has shown its potential in discovering
powerful neural network architectures. Existing methods, no matter based on
reinforcement learning or evolutionary algorithms (EA), conduct architecture
search in a discrete space, which is highly inefficient. In this paper, we
propose a simple and efficient method to automatic neural architecture design
based on continuous optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed approach:
(1) An encoder embeds/maps neural network architectures into a continuous
space. (2) A predictor takes the continuous representation of a network as
input and predicts its accuracy. (3) A decoder maps a continuous representation
of a network back to its architecture. The performance predictor and the
encoder enable us to perform gradient based optimization in the continuous
space to find the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the decoder.
Experiments show that the architecture discovered by our method is very
competitive for image classification task on CIFAR-10 and language modeling
task on PTB, outperforming or on par with the best results of previous
architecture search methods with a significantly reduction of computational
resources. Specifically we obtain 1.93% test set error rate for CIFAR-10 image
classification task and 56.0 test set perplexity of PTB language modeling task.
Furthermore, combined with the recent proposed weight sharing mechanism, we
discover powerful architecture on CIFAR-10 (with error rate 2.93%) and on PTB
(with test set perplexity 56.6), with very limited computational resources
(less than 10 GPU hours) for both tasks.
","[{'version': 'v1', 'created': 'Wed, 22 Aug 2018 06:07:03 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Sep 2018 12:02:37 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Sep 2018 14:10:46 GMT'}, {'version': 'v4', 'created': 'Wed, 31 Oct 2018 04:20:39 GMT'}, {'version': 'v5', 'created': 'Wed, 4 Sep 2019 12:53:35 GMT'}]",cs.LG,2018-08-22 06:07:03,['language model'],True,Efficiency & Performance,"['microsoft.com', 'ustc.edu.cn']",True,True,False,0.0,541.0,0.9895833333333334,0.9643916913946587,5,0.9748743718592965
183,arXiv:1808.08949,"['Matthew E. Peters', 'Mark Neumann', 'Luke Zettlemoyer', 'Wen-tau Yih']",Dissecting Contextual Word Embeddings: Architecture and Representation,['cs.CL'],"  Contextual word representations derived from pre-trained bidirectional
language models (biLMs) have recently been shown to provide significant
improvements to the state of the art for a wide range of NLP tasks. However,
many questions remain as to how and why these models are so effective. In this
paper, we present a detailed empirical study of how the choice of neural
architecture (e.g. LSTM, CNN, or self attention) influences both end task
accuracy and qualitative properties of the representations that are learned. We
show there is a tradeoff between speed and accuracy, but all architectures
learn high quality contextual representations that outperform word embeddings
for four challenging NLP tasks. Additionally, all architectures learn
representations that vary with network depth, from exclusively morphological
based at the word embedding layer through local syntax based in the lower
contextual layers to longer range semantics such coreference at the upper
layers. Together, these results suggest that unsupervised biLMs, independent of
architecture, are learning much more about the structure of language than
previously appreciated.
","[{'version': 'v1', 'created': 'Mon, 27 Aug 2018 17:54:29 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Sep 2018 23:36:12 GMT'}]",cs.CL,2018-08-27 17:54:29,['language model'],True,"Representations, Syntax, Semantics","['allenai.org', 'washington.edu']",False,True,False,0.0,320.0,0.9791666666666666,0.9436201780415431,4,0.964824120603015
186,arXiv:1808.09031,"['Rebecca Marvin', 'Tal Linzen']",Targeted Syntactic Evaluation of Language Models,['cs.CL'],"  We present a dataset for evaluating the grammaticality of the predictions of
a language model. We automatically construct a large number of minimally
different pairs of English sentences, each consisting of a grammatical and an
ungrammatical sentence. The sentence pairs represent different variations of
structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and
negative polarity items. We expect a language model to assign a higher
probability to the grammatical sentence than the ungrammatical one. In an
experiment using this data set, an LSTM language model performed poorly on many
of the constructions. Multi-task training with a syntactic objective (CCG
supertagging) improved the LSTM's accuracy, but a large gap remained between
its performance and the accuracy of human participants recruited online. This
suggests that there is considerable room for improvement over LSTMs in
capturing syntax in a language model.
","[{'version': 'v1', 'created': 'Mon, 27 Aug 2018 20:42:51 GMT'}]",cs.CL,2018-08-27 20:42:51,['language model'],True,"Representations, Syntax, Semantics",['jhu.edu'],False,True,True,1.0,313.0,0.96875,0.9406528189910979,2,0.9597989949748744
230,arXiv:1809.10853,"['Alexei Baevski', 'Michael Auli']",Adaptive Input Representations for Neural Language Modeling,['cs.CL'],"  We introduce adaptive input representations for neural language modeling
which extend the adaptive softmax of Grave et al. (2017) to input
representations of variable capacity. There are several choices on how to
factorize the input and output layers, and whether to model words, characters
or sub-word units. We perform a systematic comparison of popular choices for a
self-attentional architecture. Our experiments show that models equipped with
adaptive embeddings are more than twice as fast to train than the popular
character input CNN while having a lower number of parameters. On the
WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5
perplexity compared to the previously best published result and on the Billion
Word benchmark, we achieve 23.02 perplexity.
","[{'version': 'v1', 'created': 'Fri, 28 Sep 2018 04:30:11 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Oct 2018 02:01:50 GMT'}, {'version': 'v3', 'created': 'Fri, 22 Feb 2019 23:41:46 GMT'}]",cs.CL,2018-09-28 04:30:11,['language model'],True,"Representations, Syntax, Semantics",[],False,False,False,0.0,305.0,0.9583333333333334,0.9347181008902077,2,0.9547738693467337
239,arXiv:1810.04805,"['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova']","BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding",['cs.CL'],"  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
","[{'version': 'v1', 'created': 'Thu, 11 Oct 2018 00:50:01 GMT'}, {'version': 'v2', 'created': 'Fri, 24 May 2019 20:37:26 GMT'}]",cs.CL,2018-10-11 00:50:01,['BERT'],True,Multilingual Transfer Learning,['google.com'],True,False,False,0.33333333330000003,59446.0,0.9902912621359223,0.9970326409495549,4,0.9949748743718593
288,arXiv:1811.03604,"['Andrew Hard', 'Kanishka Rao', 'Rajiv Mathews', 'Swaroop Ramaswamy', 'Françoise Beaufays', 'Sean Augenstein', 'Hubert Eichner', 'Chloé Kiddon', 'Daniel Ramage']",Federated Learning for Mobile Keyboard Prediction,['cs.CL'],"  We train a recurrent neural network language model using a distributed,
on-device learning framework called federated learning for the purpose of
next-word prediction in a virtual keyboard for smartphones. Server-based
training using stochastic gradient descent is compared with training on client
devices using the Federated Averaging algorithm. The federated algorithm, which
enables training on a higher-quality dataset for this use case, is shown to
achieve better prediction recall. This work demonstrates the feasibility and
benefit of training language models on client devices without exporting
sensitive user data to servers. The federated learning environment gives users
greater control over the use of their data and simplifies the task of
incorporating privacy by default with distributed training and aggregation
across a population of client devices.
","[{'version': 'v1', 'created': 'Thu, 8 Nov 2018 18:37:03 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Feb 2019 21:07:51 GMT'}]",cs.CL,2018-11-08 18:37:03,['language model'],True,Fine-Tuning & Domain Adaptation,['google.com'],True,False,False,0.2857142857,1037.0,0.9805825242718447,0.9881305637982196,9,0.9899497487437185
274,arXiv:1811.00937,"['Alon Talmor', 'Jonathan Herzig', 'Nicholas Lourie', 'Jonathan Berant']","CommonsenseQA: A Question Answering Challenge Targeting Commonsense
  Knowledge","['cs.CL', 'cs.AI', 'cs.LG']","  When answering a question, people often draw upon their rich world knowledge
in addition to the particular context. Recent work has focused primarily on
answering questions given some relevant document or context, and required very
little general background. To investigate question answering with prior
knowledge, we present CommonsenseQA: a challenging new dataset for commonsense
question answering. To capture common sense beyond associations, we extract
from ConceptNet (Speer et al., 2017) multiple target concepts that have the
same semantic relation to a single source concept. Crowd-workers are asked to
author multiple-choice questions that mention the source concept and
discriminate in turn between each of the target concepts. This encourages
workers to create questions with complex semantics that often require prior
knowledge. We create 12,247 questions through this procedure and demonstrate
the difficulty of our task with a large number of strong baselines. Our best
baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,
well below human performance, which is 89%.
","[{'version': 'v1', 'created': 'Fri, 2 Nov 2018 15:34:29 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Mar 2019 18:02:58 GMT'}]",cs.CL,2018-11-02 15:34:29,['BERT'],True,Question Answering & Retrieval,['allenai.org'],False,False,False,0.0,692.0,0.970873786407767,0.973293768545994,4,0.9849246231155779
277,arXiv:1811.01088,"['Jason Phang', 'Thibault Févry', 'Samuel R. Bowman']","Sentence Encoders on STILTs: Supplementary Training on Intermediate
  Labeled-data Tasks",['cs.CL'],"  Pretraining sentence encoders with language modeling and related unsupervised
tasks has recently been shown to be very effective for language understanding
tasks. By supplementing language model-style pretraining with further training
on data-rich supervised tasks, such as natural language inference, we obtain
additional performance improvements on the GLUE benchmark. Applying
supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of
81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over
BERT. We also observe reduced variance across random restarts in this setting.
Our approach yields similar improvements when applied to ELMo (Peters et al.,
2018a) and Radford et al. (2018)'s model. In addition, the benefits of
supplementary training are particularly pronounced in data-constrained regimes,
as we show in experiments with artificially limited training data.
","[{'version': 'v1', 'created': 'Fri, 2 Nov 2018 21:04:24 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Feb 2019 19:07:16 GMT'}]",cs.CL,2018-11-02 21:04:24,"['language model', 'BERT']",True,Pretrained LMs & Text Classification,['nyu.edu'],False,True,False,0.0,378.0,0.9611650485436893,0.9525222551928784,3,0.9698492462311558
281,arXiv:1811.02084,"['Noam Shazeer', 'Youlong Cheng', 'Niki Parmar', 'Dustin Tran', 'Ashish Vaswani', 'Penporn Koanantakool', 'Peter Hawkins', 'HyoukJoong Lee', 'Mingsheng Hong', 'Cliff Young', 'Ryan Sepassi', 'Blake Hechtman']",Mesh-TensorFlow: Deep Learning for Supercomputers,"['cs.LG', 'cs.DC', 'stat.ML']","  Batch-splitting (data-parallelism) is the dominant distributed Deep Neural
Network (DNN) training strategy, due to its universal applicability and its
amenability to Single-Program-Multiple-Data (SPMD) programming. However,
batch-splitting suffers from problems including the inability to train very
large models (due to memory constraints), high latency, and inefficiency at
small batch sizes. All of these can be solved by more general distribution
strategies (model-parallelism). Unfortunately, efficient model-parallel
algorithms tend to be complicated to discover, describe, and to implement,
particularly on large clusters. We introduce Mesh-TensorFlow, a language for
specifying a general class of distributed tensor computations. Where
data-parallelism can be viewed as splitting tensors and operations along the
""batch"" dimension, in Mesh-TensorFlow, the user can specify any
tensor-dimensions to be split across any dimensions of a multi-dimensional mesh
of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting
of parallel operations coupled with collective communication primitives such as
Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel,
model-parallel version of the Transformer sequence-to-sequence model. Using TPU
meshes of up to 512 cores, we train Transformer models with up to 5 billion
parameters, surpassing state of the art results on WMT'14 English-to-French
translation task and the one-billion-word language modeling benchmark.
Mesh-Tensorflow is available at https://github.com/tensorflow/mesh .
","[{'version': 'v1', 'created': 'Mon, 5 Nov 2018 23:25:02 GMT'}]",cs.LG,2018-11-05 23:25:02,['language model'],True,Efficiency & Performance,['google.com'],True,False,False,0.0,290.0,0.9514563106796117,0.9287833827893175,12,0.9447236180904522
353,arXiv:1901.08746,"['Jinhyuk Lee', 'Wonjin Yoon', 'Sungdong Kim', 'Donghyeon Kim', 'Sunkyu Kim', 'Chan Ho So', 'Jaewoo Kang']","BioBERT: a pre-trained biomedical language representation model for
  biomedical text mining",['cs.CL'],"  Biomedical text mining is becoming increasingly important as the number of
biomedical documents rapidly grows. With the progress in natural language
processing (NLP), extracting valuable information from biomedical literature
has gained popularity among researchers, and deep learning has boosted the
development of effective biomedical text mining models. However, directly
applying the advancements in NLP to biomedical text mining often yields
unsatisfactory results due to a word distribution shift from general domain
corpora to biomedical corpora. In this article, we investigate how the recently
introduced pre-trained language model BERT can be adapted for biomedical
corpora. We introduce BioBERT (Bidirectional Encoder Representations from
Transformers for Biomedical Text Mining), which is a domain-specific language
representation model pre-trained on large-scale biomedical corpora. With almost
the same architecture across tasks, BioBERT largely outperforms BERT and
previous state-of-the-art models in a variety of biomedical text mining tasks
when pre-trained on biomedical corpora. While BERT obtains performance
comparable to that of previous state-of-the-art models, BioBERT significantly
outperforms them on the following three representative biomedical text mining
tasks: biomedical named entity recognition (0.62% F1 score improvement),
biomedical relation extraction (2.80% F1 score improvement) and biomedical
question answering (12.24% MRR improvement). Our analysis results show that
pre-training BERT on biomedical corpora helps it to understand complex
biomedical texts. We make the pre-trained weights of BioBERT freely available
at https://github.com/naver/biobert-pretrained, and the source code for
fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.
","[{'version': 'v1', 'created': 'Fri, 25 Jan 2019 05:57:24 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Jan 2019 06:43:51 GMT'}, {'version': 'v3', 'created': 'Sun, 3 Feb 2019 09:06:53 GMT'}, {'version': 'v4', 'created': 'Fri, 18 Oct 2019 02:51:31 GMT'}]",cs.CL,2019-01-25 05:57:24,"['BERT', 'language model']",True,NLP for Healthcare,"['korea.ac.kr', 'oup.com']",False,True,False,0.0,3374.0,0.9905660377358491,0.9890710382513661,7,0.989041095890411
341,arXiv:1901.02860,"['Zihang Dai', 'Zhilin Yang', 'Yiming Yang', 'Jaime Carbonell', 'Quoc V. Le', 'Ruslan Salakhutdinov']",Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,"['cs.LG', 'cs.CL', 'stat.ML']","  Transformers have a potential of learning longer-term dependency, but are
limited by a fixed-length context in the setting of language modeling. We
propose a novel neural architecture Transformer-XL that enables learning
dependency beyond a fixed length without disrupting temporal coherence. It
consists of a segment-level recurrence mechanism and a novel positional
encoding scheme. Our method not only enables capturing longer-term dependency,
but also resolves the context fragmentation problem. As a result,
Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer
than vanilla Transformers, achieves better performance on both short and long
sequences, and is up to 1,800+ times faster than vanilla Transformers during
evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity
to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion
Word, and 54.5 on Penn Treebank (without finetuning). When trained only on
WikiText-103, Transformer-XL manages to generate reasonably coherent, novel
text articles with thousands of tokens. Our code, pretrained models, and
hyperparameters are available in both Tensorflow and PyTorch.
","[{'version': 'v1', 'created': 'Wed, 9 Jan 2019 18:28:19 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Jan 2019 18:38:00 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Jun 2019 21:21:48 GMT'}]",cs.LG,2019-01-09 18:28:19,['language model'],True,"Transformers, RNNs, Attention","['cmu.edu', 'google.com']",True,True,False,0.0,2667.0,0.9811320754716981,0.98816029143898,6,0.9863013698630136
351,arXiv:1901.07291,"['Guillaume Lample', 'Alexis Conneau']",Cross-lingual Language Model Pretraining,['cs.CL'],"  Recent studies have demonstrated the efficiency of generative pretraining for
English natural language understanding. In this work, we extend this approach
to multiple languages and show the effectiveness of cross-lingual pretraining.
We propose two methods to learn cross-lingual language models (XLMs): one
unsupervised that only relies on monolingual data, and one supervised that
leverages parallel data with a new cross-lingual language model objective. We
obtain state-of-the-art results on cross-lingual classification, unsupervised
and supervised machine translation. On XNLI, our approach pushes the state of
the art by an absolute gain of 4.9% accuracy. On unsupervised machine
translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the
previous state of the art by more than 9 BLEU. On supervised machine
translation, we obtain a new state of the art of 38.5 BLEU on WMT'16
Romanian-English, outperforming the previous best approach by more than 4 BLEU.
Our code and pretrained models will be made publicly available.
","[{'version': 'v1', 'created': 'Tue, 22 Jan 2019 13:22:34 GMT'}]",cs.CL,2019-01-22 13:22:34,['language model'],True,Multilingual Transfer Learning,['fb.com'],True,False,False,0.0,2121.0,0.9716981132075472,0.9845173041894353,2,0.9780821917808219
432,arXiv:1903.10676,"['Iz Beltagy', 'Kyle Lo', 'Arman Cohan']",SciBERT: A Pretrained Language Model for Scientific Text,['cs.CL'],"  Obtaining large-scale annotated data for NLP tasks in the scientific domain
is challenging and expensive. We release SciBERT, a pretrained language model
based on BERT (Devlin et al., 2018) to address the lack of high-quality,
large-scale labeled scientific data. SciBERT leverages unsupervised pretraining
on a large multi-domain corpus of scientific publications to improve
performance on downstream scientific NLP tasks. We evaluate on a suite of tasks
including sequence tagging, sentence classification and dependency parsing,
with datasets from a variety of scientific domains. We demonstrate
statistically significant improvements over BERT and achieve new
state-of-the-art results on several of these tasks. The code and pretrained
models are available at https://github.com/allenai/scibert/.
","[{'version': 'v1', 'created': 'Tue, 26 Mar 2019 05:11:46 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Aug 2019 19:23:59 GMT'}, {'version': 'v3', 'created': 'Tue, 10 Sep 2019 18:10:35 GMT'}]",cs.CL,2019-03-26 05:11:46,"['pretrained language model', 'language model', 'BERT']",True,NLP for Healthcare,['allenai.org'],False,False,False,0.0,1832.0,0.9622641509433962,0.9836065573770492,3,0.9753424657534246
368,arXiv:1902.00751,"['Neil Houlsby', 'Andrei Giurgiu', 'Stanislaw Jastrzebski', 'Bruna Morrone', 'Quentin de Laroussilhe', 'Andrea Gesmundo', 'Mona Attariyan', 'Sylvain Gelly']",Parameter-Efficient Transfer Learning for NLP,"['cs.LG', 'cs.CL', 'stat.ML']","  Fine-tuning large pre-trained models is an effective transfer mechanism in
NLP. However, in the presence of many downstream tasks, fine-tuning is
parameter inefficient: an entire new model is required for every task. As an
alternative, we propose transfer with adapter modules. Adapter modules yield a
compact and extensible model; they add only a few trainable parameters per
task, and new tasks can be added without revisiting previous ones. The
parameters of the original network remain fixed, yielding a high degree of
parameter sharing. To demonstrate adapter's effectiveness, we transfer the
recently proposed BERT Transformer model to 26 diverse text classification
tasks, including the GLUE benchmark. Adapters attain near state-of-the-art
performance, whilst adding only a few parameters per task. On GLUE, we attain
within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters
per task. By contrast, fine-tuning trains 100% of the parameters per task.
","[{'version': 'v1', 'created': 'Sat, 2 Feb 2019 16:29:47 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Jun 2019 17:48:30 GMT'}]",cs.LG,2019-02-02 16:29:47,['BERT'],True,Fine-Tuning & Domain Adaptation,['google.com'],True,False,False,0.2857142857,1577.0,0.9528301886792453,0.9808743169398907,8,0.9698630136986301
677,arXiv:1906.08237,"['Zhilin Yang', 'Zihang Dai', 'Yiming Yang', 'Jaime Carbonell', 'Ruslan Salakhutdinov', 'Quoc V. Le']",XLNet: Generalized Autoregressive Pretraining for Language Understanding,"['cs.CL', 'cs.LG']","  With the capability of modeling bidirectional contexts, denoising
autoencoding based pretraining like BERT achieves better performance than
pretraining approaches based on autoregressive language modeling. However,
relying on corrupting the input with masks, BERT neglects dependency between
the masked positions and suffers from a pretrain-finetune discrepancy. In light
of these pros and cons, we propose XLNet, a generalized autoregressive
pretraining method that (1) enables learning bidirectional contexts by
maximizing the expected likelihood over all permutations of the factorization
order and (2) overcomes the limitations of BERT thanks to its autoregressive
formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the
state-of-the-art autoregressive model, into pretraining. Empirically, under
comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a
large margin, including question answering, natural language inference,
sentiment analysis, and document ranking.
","[{'version': 'v1', 'created': 'Wed, 19 Jun 2019 17:35:48 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Jan 2020 12:48:08 GMT'}]",cs.CL,2019-06-19 17:35:48,"['XLNet', 'BERT', 'language model']",True,Pretrained LMs & Text Classification,"['cmu.edu', 'google.com']",True,True,False,0.0,6271.0,0.9922779922779923,0.994535519125683,6,0.9917808219178083
490,arXiv:1904.08779,"['Daniel S. Park', 'William Chan', 'Yu Zhang', 'Chung-Cheng Chiu', 'Barret Zoph', 'Ekin D. Cubuk', 'Quoc V. Le']","SpecAugment: A Simple Data Augmentation Method for Automatic Speech
  Recognition","['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD', 'stat.ML']","  We present SpecAugment, a simple data augmentation method for speech
recognition. SpecAugment is applied directly to the feature inputs of a neural
network (i.e., filter bank coefficients). The augmentation policy consists of
warping the features, masking blocks of frequency channels, and masking blocks
of time steps. We apply SpecAugment on Listen, Attend and Spell networks for
end-to-end speech recognition tasks. We achieve state-of-the-art performance on
the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.
On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language
model, and 5.8% WER with shallow fusion with a language model. This compares to
the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we
achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set
without the use of a language model, and 6.8%/14.1% with shallow fusion, which
compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.
","[{'version': 'v1', 'created': 'Thu, 18 Apr 2019 17:53:38 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Jul 2019 21:56:06 GMT'}, {'version': 'v3', 'created': 'Tue, 3 Dec 2019 18:19:07 GMT'}]",eess.AS,2019-04-18 17:53:38,['language model'],True,Speech Recognition,['google.com'],True,False,False,0.0,2455.0,0.9884169884169884,0.9872495446265938,7,0.9835616438356164
501,arXiv:1904.09675,"['Tianyi Zhang', 'Varsha Kishore', 'Felix Wu', 'Kilian Q. Weinberger', 'Yoav Artzi']",BERTScore: Evaluating Text Generation with BERT,['cs.CL'],"  We propose BERTScore, an automatic evaluation metric for text generation.
Analogously to common metrics, BERTScore computes a similarity score for each
token in the candidate sentence with each token in the reference sentence.
However, instead of exact matches, we compute token similarity using contextual
embeddings. We evaluate using the outputs of 363 machine translation and image
captioning systems. BERTScore correlates better with human judgments and
provides stronger model selection performance than existing metrics. Finally,
we use an adversarial paraphrase detection task to show that BERTScore is more
robust to challenging examples when compared to existing metrics.
","[{'version': 'v1', 'created': 'Sun, 21 Apr 2019 23:08:53 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Oct 2019 16:52:00 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2020 18:59:28 GMT'}]",cs.CL,2019-04-21 23:08:53,['BERT'],True,Summarization and Evaluation,"['cornell.edu', 'asapp.com']",True,True,False,0.25,2310.0,0.9845559845559846,0.9854280510018215,5,0.9808219178082191
503,arXiv:1904.09751,"['Ari Holtzman', 'Jan Buys', 'Li Du', 'Maxwell Forbes', 'Yejin Choi']",The Curious Case of Neural Text Degeneration,['cs.CL'],"  Despite considerable advancements with deep neural language models, the
enigma of neural text degeneration persists when these models are tested as
text generators. The counter-intuitive empirical observation is that even
though the use of likelihood as training objective leads to high quality models
for a broad range of language understanding tasks, using likelihood as a
decoding objective leads to text that is bland and strangely repetitive.
  In this paper, we reveal surprising distributional differences between human
text and machine text. In addition, we find that decoding strategies alone can
dramatically effect the quality of machine text, even when generated from
exactly the same neural language model. Our findings motivate Nucleus Sampling,
a simple but effective method to draw the best out of neural generation. By
sampling text from the dynamic nucleus of the probability distribution, which
allows for diversity while effectively truncating the less reliable tail of the
distribution, the resulting text better demonstrates the quality of human text,
yielding enhanced diversity without sacrificing fluency and coherence.
","[{'version': 'v1', 'created': 'Mon, 22 Apr 2019 07:17:18 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Feb 2020 21:56:30 GMT'}]",cs.CL,2019-04-22 07:17:18,['language model'],True,Text Generation,['washington.edu'],False,True,False,0.33333333330000003,1717.0,0.9806949806949807,0.982695810564663,5,0.9726027397260274
512,arXiv:1904.12848,"['Qizhe Xie', 'Zihang Dai', 'Eduard Hovy', 'Minh-Thang Luong', 'Quoc V. Le']",Unsupervised Data Augmentation for Consistency Training,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'stat.ML']","  Semi-supervised learning lately has shown much promise in improving deep
learning models when labeled data is scarce. Common among recent approaches is
the use of consistency training on a large amount of unlabeled data to
constrain model predictions to be invariant to input noise. In this work, we
present a new perspective on how to effectively noise unlabeled examples and
argue that the quality of noising, specifically those produced by advanced data
augmentation methods, plays a crucial role in semi-supervised learning. By
substituting simple noising operations with advanced data augmentation methods
such as RandAugment and back-translation, our method brings substantial
improvements across six language and three vision tasks under the same
consistency training framework. On the IMDb text classification dataset, with
only 20 labeled examples, our method achieves an error rate of 4.20,
outperforming the state-of-the-art model trained on 25,000 labeled examples. On
a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms
all previous approaches and achieves an error rate of 5.43 with only 250
examples. Our method also combines well with transfer learning, e.g., when
finetuning from BERT, and yields improvements in high-data regime, such as
ImageNet, whether when there is only 10% labeled data or when a full labeled
set with 1.3M extra unlabeled examples is used. Code is available at
https://github.com/google-research/uda.
","[{'version': 'v1', 'created': 'Mon, 29 Apr 2019 17:56:59 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Jul 2019 17:53:48 GMT'}, {'version': 'v3', 'created': 'Thu, 26 Sep 2019 15:32:11 GMT'}, {'version': 'v4', 'created': 'Mon, 30 Sep 2019 15:40:40 GMT'}, {'version': 'v5', 'created': 'Thu, 25 Jun 2020 17:58:43 GMT'}, {'version': 'v6', 'created': 'Thu, 5 Nov 2020 15:11:02 GMT'}]",cs.LG,2019-04-29 17:56:59,['BERT'],True,Pretrained LMs & Text Classification,"['cmu.edu', 'google.com']",True,True,False,0.0,1576.0,0.9768339768339769,0.9799635701275046,5,0.9671232876712329
761,arXiv:1907.11692,"['Yinhan Liu', 'Myle Ott', 'Naman Goyal', 'Jingfei Du', 'Mandar Joshi', 'Danqi Chen', 'Omer Levy', 'Mike Lewis', 'Luke Zettlemoyer', 'Veselin Stoyanov']",RoBERTa: A Robustly Optimized BERT Pretraining Approach,['cs.CL'],"  Language model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code.
","[{'version': 'v1', 'created': 'Fri, 26 Jul 2019 17:48:29 GMT'}]",cs.CL,2019-07-26 17:48:29,"['BERT', 'language model']",True,Efficiency & Performance,"['washington.edu', 'fb.com']",True,True,False,0.1428571429,14421.0,0.9972826086956522,0.9954462659380692,10,0.9972714870395635
873,arXiv:1908.10084,"['Nils Reimers', 'Iryna Gurevych']",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,['cs.CL'],"  BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new
state-of-the-art performance on sentence-pair regression tasks like semantic
textual similarity (STS). However, it requires that both sentences are fed into
the network, which causes a massive computational overhead: Finding the most
similar pair in a collection of 10,000 sentences requires about 50 million
inference computations (~65 hours) with BERT. The construction of BERT makes it
unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering.
  In this publication, we present Sentence-BERT (SBERT), a modification of the
pretrained BERT network that use siamese and triplet network structures to
derive semantically meaningful sentence embeddings that can be compared using
cosine-similarity. This reduces the effort for finding the most similar pair
from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while
maintaining the accuracy from BERT.
  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning
tasks, where it outperforms other state-of-the-art sentence embeddings methods.
","[{'version': 'v1', 'created': 'Tue, 27 Aug 2019 08:50:17 GMT'}]",cs.CL,2019-08-27 08:50:17,['BERT'],True,"Representations, Syntax, Semantics",[],False,False,True,0.5,4883.0,0.9945652173913043,0.9927140255009107,2,0.9945429740791268
1054,arXiv:1909.11942,"['Zhenzhong Lan', 'Mingda Chen', 'Sebastian Goodman', 'Kevin Gimpel', 'Piyush Sharma', 'Radu Soricut']","ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations","['cs.CL', 'cs.AI']","  Increasing model size when pretraining natural language representations often
results in improved performance on downstream tasks. However, at some point
further model increases become harder due to GPU/TPU memory limitations and
longer training times. To address these problems, we present two
parameter-reduction techniques to lower memory consumption and increase the
training speed of BERT. Comprehensive empirical evidence shows that our
proposed methods lead to models that scale much better compared to the original
BERT. We also use a self-supervised loss that focuses on modeling
inter-sentence coherence, and show it consistently helps downstream tasks with
multi-sentence inputs. As a result, our best model establishes new
state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having
fewer parameters compared to BERT-large. The code and the pretrained models are
available at https://github.com/google-research/ALBERT.
","[{'version': 'v1', 'created': 'Thu, 26 Sep 2019 07:06:13 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Oct 2019 03:22:00 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Oct 2019 02:19:07 GMT'}, {'version': 'v4', 'created': 'Fri, 10 Jan 2020 19:00:02 GMT'}, {'version': 'v5', 'created': 'Mon, 3 Feb 2020 04:01:33 GMT'}, {'version': 'v6', 'created': 'Sun, 9 Feb 2020 03:00:18 GMT'}]",cs.CL,2019-09-26 07:06:13,['BERT'],True,Efficiency & Performance,"['google.com', 'ttic.edu']",True,True,False,0.0,4478.0,0.9918478260869565,0.9918032786885246,6,0.9931787175989086
784,arXiv:1908.02265,"['Jiasen Lu', 'Dhruv Batra', 'Devi Parikh', 'Stefan Lee']","ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for
  Vision-and-Language Tasks","['cs.CV', 'cs.CL']","  We present ViLBERT (short for Vision-and-Language BERT), a model for learning
task-agnostic joint representations of image content and natural language. We
extend the popular BERT architecture to a multi-modal two-stream model,
pro-cessing both visual and textual inputs in separate streams that interact
through co-attentional transformer layers. We pretrain our model through two
proxy tasks on the large, automatically collected Conceptual Captions dataset
and then transfer it to multiple established vision-and-language tasks --
visual question answering, visual commonsense reasoning, referring expressions,
and caption-based image retrieval -- by making only minor additions to the base
architecture. We observe significant improvements across tasks compared to
existing task-specific models -- achieving state-of-the-art on all four tasks.
Our work represents a shift away from learning groundings between vision and
language only as part of task training and towards treating visual grounding as
a pretrainable and transferable capability.
","[{'version': 'v1', 'created': 'Tue, 6 Aug 2019 17:33:52 GMT'}]",cs.CV,2019-08-06 17:33:52,['BERT'],True,Vision-Language Models,[],False,False,False,0.0,2391.0,0.9891304347826086,0.9863387978142076,4,0.9890859481582538
837,arXiv:1908.07490,"['Hao Tan', 'Mohit Bansal']","LXMERT: Learning Cross-Modality Encoder Representations from
  Transformers","['cs.CL', 'cs.CV', 'cs.LG']","  Vision-and-language reasoning requires an understanding of visual concepts,
language semantics, and, most importantly, the alignment and relationships
between these two modalities. We thus propose the LXMERT (Learning
Cross-Modality Encoder Representations from Transformers) framework to learn
these vision-and-language connections. In LXMERT, we build a large-scale
Transformer model that consists of three encoders: an object relationship
encoder, a language encoder, and a cross-modality encoder. Next, to endow our
model with the capability of connecting vision and language semantics, we
pre-train the model with large amounts of image-and-sentence pairs, via five
diverse representative pre-training tasks: masked language modeling, masked
object prediction (feature regression and label classification), cross-modality
matching, and image question answering. These tasks help in learning both
intra-modality and cross-modality relationships. After fine-tuning from our
pre-trained parameters, our model achieves the state-of-the-art results on two
visual question answering datasets (i.e., VQA and GQA). We also show the
generalizability of our pre-trained cross-modality model by adapting it to a
challenging visual-reasoning task, NLVR2, and improve the previous best result
by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies
to prove that both our novel model components and pre-training strategies
significantly contribute to our strong results; and also present several
attention visualizations for the different encoders. Code and pre-trained
models publicly available at: https://github.com/airsplay/lxmert
","[{'version': 'v1', 'created': 'Tue, 20 Aug 2019 17:05:18 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Aug 2019 17:54:29 GMT'}, {'version': 'v3', 'created': 'Tue, 3 Dec 2019 19:30:19 GMT'}]",cs.CL,2019-08-20 17:05:18,['language model'],True,Vision-Language Models,['unc.edu'],False,True,False,0.0,1666.0,0.9864130434782609,0.9817850637522769,2,0.9877216916780355
1188,arXiv:1910.13461,"['Mike Lewis', 'Yinhan Liu', 'Naman Goyal', 'Marjan Ghazvininejad', 'Abdelrahman Mohamed', 'Omer Levy', 'Ves Stoyanov', 'Luke Zettlemoyer']","BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
  Generation, Translation, and Comprehension","['cs.CL', 'cs.LG', 'stat.ML']","  We present BART, a denoising autoencoder for pretraining sequence-to-sequence
models. BART is trained by (1) corrupting text with an arbitrary noising
function, and (2) learning a model to reconstruct the original text. It uses a
standard Tranformer-based neural machine translation architecture which,
despite its simplicity, can be seen as generalizing BERT (due to the
bidirectional encoder), GPT (with the left-to-right decoder), and many other
more recent pretraining schemes. We evaluate a number of noising approaches,
finding the best performance by both randomly shuffling the order of the
original sentences and using a novel in-filling scheme, where spans of text are
replaced with a single mask token. BART is particularly effective when fine
tuned for text generation but also works well for comprehension tasks. It
matches the performance of RoBERTa with comparable training resources on GLUE
and SQuAD, achieves new state-of-the-art results on a range of abstractive
dialogue, question answering, and summarization tasks, with gains of up to 6
ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system
for machine translation, with only target language pretraining. We also report
ablation experiments that replicate other pretraining schemes within the BART
framework, to better measure which factors most influence end-task performance.
","[{'version': 'v1', 'created': 'Tue, 29 Oct 2019 18:01:00 GMT'}]",cs.CL,2019-10-29 18:01:00,['BERT'],True,"Representations, Syntax, Semantics",['fb.com'],True,False,False,0.1428571429,5886.0,0.9972602739726028,0.9936247723132969,8,0.9959072305593452
1076,arXiv:1910.01108,"['Victor Sanh', 'Lysandre Debut', 'Julien Chaumond', 'Thomas Wolf']","DistilBERT, a distilled version of BERT: smaller, faster, cheaper and
  lighter",['cs.CL'],"  As Transfer Learning from large-scale pre-trained models becomes more
prevalent in Natural Language Processing (NLP), operating these large models in
on-the-edge and/or under constrained computational training or inference
budgets remains challenging. In this work, we propose a method to pre-train a
smaller general-purpose language representation model, called DistilBERT, which
can then be fine-tuned with good performances on a wide range of tasks like its
larger counterparts. While most prior work investigated the use of distillation
for building task-specific models, we leverage knowledge distillation during
the pre-training phase and show that it is possible to reduce the size of a
BERT model by 40%, while retaining 97% of its language understanding
capabilities and being 60% faster. To leverage the inductive biases learned by
larger models during pre-training, we introduce a triple loss combining
language modeling, distillation and cosine-distance losses. Our smaller, faster
and lighter model is cheaper to pre-train and we demonstrate its capabilities
for on-device computations in a proof-of-concept experiment and a comparative
on-device study.
","[{'version': 'v1', 'created': 'Wed, 2 Oct 2019 17:56:28 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Oct 2019 14:52:02 GMT'}, {'version': 'v3', 'created': 'Fri, 24 Jan 2020 16:58:52 GMT'}, {'version': 'v4', 'created': 'Sun, 1 Mar 2020 02:57:50 GMT'}]",cs.CL,2019-10-02 17:56:28,"['BERT', 'language model']",True,Knowledge Distillation,['huggingface.co'],True,False,False,0.0,4138.0,0.9945205479452055,0.9908925318761385,4,0.9918144611186903
1225,arXiv:1911.02116,"['Alexis Conneau', 'Kartikay Khandelwal', 'Naman Goyal', 'Vishrav Chaudhary', 'Guillaume Wenzek', 'Francisco Guzmán', 'Edouard Grave', 'Myle Ott', 'Luke Zettlemoyer', 'Veselin Stoyanov']",Unsupervised Cross-lingual Representation Learning at Scale,['cs.CL'],"  This paper shows that pretraining multilingual language models at scale leads
to significant performance gains for a wide range of cross-lingual transfer
tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our
model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a
variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,
+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs
particularly well on low-resource languages, improving 15.7% in XNLI accuracy
for Swahili and 11.4% for Urdu over previous XLM models. We also present a
detailed empirical analysis of the key factors that are required to achieve
these gains, including the trade-offs between (1) positive transfer and
capacity dilution and (2) the performance of high and low resource languages at
scale. Finally, we show, for the first time, the possibility of multilingual
modeling without sacrificing per-language performance; XLM-R is very
competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make our code, data and models publicly available.
","[{'version': 'v1', 'created': 'Tue, 5 Nov 2019 22:42:00 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Apr 2020 01:02:17 GMT'}]",cs.CL,2019-11-05 22:42:00,"['language model', 'BERT']",True,Multilingual Transfer Learning,[],False,False,False,0.0,3593.0,0.9917808219178083,0.9899817850637522,10,0.990450204638472
1347,arXiv:1911.12543,"['Zhengbao Jiang', 'Frank F. Xu', 'Jun Araki', 'Graham Neubig']",How Can We Know What Language Models Know?,"['cs.CL', 'cs.LG']","  Recent work has presented intriguing results examining the knowledge
contained in language models (LM) by having the LM fill in the blanks of
prompts such as ""Obama is a _ by profession"". These prompts are usually
manually created, and quite possibly sub-optimal; another prompt such as ""Obama
worked as a _"" may result in more accurately predicting the correct profession.
Because of this, given an inappropriate prompt, we might fail to retrieve facts
that the LM does know, and thus any given prompt only provides a lower bound
estimate of the knowledge contained in an LM. In this paper, we attempt to more
accurately estimate the knowledge contained in LMs by automatically discovering
better prompts to use in this querying process. Specifically, we propose
mining-based and paraphrasing-based methods to automatically generate
high-quality and diverse prompts, as well as ensemble methods to combine
answers from different prompts. Extensive experiments on the LAMA benchmark for
extracting relational knowledge from LMs demonstrate that our methods can
improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what
LMs know. We have released the code and the resulting LM Prompt And Query
Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.
","[{'version': 'v1', 'created': 'Thu, 28 Nov 2019 05:55:42 GMT'}, {'version': 'v2', 'created': 'Sun, 3 May 2020 20:29:05 GMT'}]",cs.CL,2019-11-28 05:55:42,['language model'],True,Knowledge Graphs and Commonsense,"['cmu.edu', 'bosch.com']",True,True,False,0.0,703.0,0.989041095890411,0.9590163934426229,4,0.9740791268758526
1274,arXiv:1911.03894,"['Louis Martin', 'Benjamin Muller', 'Pedro Javier Ortiz Suárez', 'Yoann Dupont', 'Laurent Romary', 'Éric Villemonte de la Clergerie', 'Djamé Seddah', 'Benoît Sagot']",CamemBERT: a Tasty French Language Model,['cs.CL'],"  Pretrained language models are now ubiquitous in Natural Language Processing.
Despite their success, most available models have either been trained on
English data or on the concatenation of data in multiple languages. This makes
practical use of such models --in all languages except English-- very limited.
In this paper, we investigate the feasibility of training monolingual
Transformer-based language models for other languages, taking French as an
example and evaluating our language models on part-of-speech tagging,
dependency parsing, named entity recognition and natural language inference
tasks. We show that the use of web crawled data is preferable to the use of
Wikipedia data. More surprisingly, we show that a relatively small web crawled
dataset (4GB) leads to results that are as good as those obtained using larger
datasets (130+GB). Our best performing model CamemBERT reaches or improves the
state of the art in all four downstream tasks.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2019 10:46:37 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 17:07:02 GMT'}, {'version': 'v3', 'created': 'Thu, 21 May 2020 23:20:48 GMT'}]",cs.CL,2019-11-10 10:46:37,"['pretrained language model', 'language model', 'BERT']",True,Multilingual Transfer Learning,"['inria.fr', 'fb.com']",True,True,False,0.0,619.0,0.9863013698630136,0.9526411657559198,8,0.9713506139154161
1473,arXiv:2001.08361,"['Jared Kaplan', 'Sam McCandlish', 'Tom Henighan', 'Tom B. Brown', 'Benjamin Chess', 'Rewon Child', 'Scott Gray', 'Alec Radford', 'Jeffrey Wu', 'Dario Amodei']",Scaling Laws for Neural Language Models,"['cs.LG', 'stat.ML']","  We study empirical scaling laws for language model performance on the
cross-entropy loss. The loss scales as a power-law with model size, dataset
size, and the amount of compute used for training, with some trends spanning
more than seven orders of magnitude. Other architectural details such as
network width or depth have minimal effects within a wide range. Simple
equations govern the dependence of overfitting on model/dataset size and the
dependence of training speed on model size. These relationships allow us to
determine the optimal allocation of a fixed compute budget. Larger models are
significantly more sample-efficient, such that optimally compute-efficient
training involves training very large models on a relatively modest amount of
data and stopping significantly before convergence.
","[{'version': 'v1', 'created': 'Thu, 23 Jan 2020 03:59:20 GMT'}]",cs.LG,2020-01-23 03:59:20,['language model'],True,Efficiency & Performance,"['jhu.edu', 'openai.com']",True,True,False,0.0,1363.0,0.996,0.993032977241059,10,0.9931818181818182
1555,arXiv:2002.08155,"['Zhangyin Feng', 'Daya Guo', 'Duyu Tang', 'Nan Duan', 'Xiaocheng Feng', 'Ming Gong', 'Linjun Shou', 'Bing Qin', 'Ting Liu', 'Daxin Jiang', 'Ming Zhou']",CodeBERT: A Pre-Trained Model for Programming and Natural Languages,"['cs.CL', 'cs.PL']","  We present CodeBERT, a bimodal pre-trained model for programming language
(PL) and nat-ural language (NL). CodeBERT learns general-purpose
representations that support downstream NL-PL applications such as natural
language codesearch, code documentation generation, etc. We develop CodeBERT
with Transformer-based neural architecture, and train it with a hybrid
objective function that incorporates the pre-training task of replaced token
detection, which is to detect plausible alternatives sampled from generators.
This enables us to utilize both bimodal data of NL-PL pairs and unimodal data,
where the former provides input tokens for model training while the latter
helps to learn better generators. We evaluate CodeBERT on two NL-PL
applications by fine-tuning model parameters. Results show that CodeBERT
achieves state-of-the-art performance on both natural language code search and
code documentation generation tasks. Furthermore, to investigate what type of
knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and
evaluate in a zero-shot setting where parameters of pre-trained models are
fixed. Results show that CodeBERT performs better than previous pre-trained
models on NL-PL probing.
","[{'version': 'v1', 'created': 'Wed, 19 Feb 2020 13:09:07 GMT'}, {'version': 'v2', 'created': 'Sun, 5 Apr 2020 08:51:49 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Apr 2020 04:35:54 GMT'}, {'version': 'v4', 'created': 'Fri, 18 Sep 2020 15:38:12 GMT'}]",cs.CL,2020-02-19 13:09:07,['BERT'],True,Code Generation,['microsoft.com'],True,False,False,0.0,1113.0,0.992,0.9916395726892708,11,0.990909090909091
1585,arXiv:2002.12327,"['Anna Rogers', 'Olga Kovaleva', 'Anna Rumshisky']",A Primer in BERTology: What we know about how BERT works,['cs.CL'],"  Transformer-based models have pushed state of the art in many areas of NLP,
but our understanding of what is behind their success is still limited. This
paper is the first survey of over 150 studies of the popular BERT model. We
review the current state of knowledge about how BERT works, what kind of
information it learns and how it is represented, common modifications to its
training objectives and architecture, the overparameterization issue and
approaches to compression. We then outline directions for future research.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2020 18:46:42 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Nov 2020 22:01:57 GMT'}, {'version': 'v3', 'created': 'Mon, 9 Nov 2020 15:33:50 GMT'}]",cs.CL,2020-02-27 18:46:42,['BERT'],True,"Representations, Syntax, Semantics",['uml.edu'],False,True,True,1.0,954.0,0.988,0.990710636321412,3,0.9897727272727272
1559,arXiv:2002.08909,"['Kelvin Guu', 'Kenton Lee', 'Zora Tung', 'Panupong Pasupat', 'Ming-Wei Chang']",REALM: Retrieval-Augmented Language Model Pre-Training,"['cs.CL', 'cs.LG']","  Language model pre-training has been shown to capture a surprising amount of
world knowledge, crucial for NLP tasks such as question answering. However,
this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts.
  To capture knowledge in a more modular and interpretable way, we augment
language model pre-training with a latent knowledge retriever, which allows the
model to retrieve and attend over documents from a large corpus such as
Wikipedia, used during pre-training, fine-tuning and inference. For the first
time, we show how to pre-train such a knowledge retriever in an unsupervised
manner, using masked language modeling as the learning signal and
backpropagating through a retrieval step that considers millions of documents.
  We demonstrate the effectiveness of Retrieval-Augmented Language Model
pre-training (REALM) by fine-tuning on the challenging task of Open-domain
Question Answering (Open-QA). We compare against state-of-the-art models for
both explicit and implicit knowledge storage on three popular Open-QA
benchmarks, and find that we outperform all previous methods by a significant
margin (4-16% absolute accuracy), while also providing qualitative benefits
such as interpretability and modularity.
","[{'version': 'v1', 'created': 'Mon, 10 Feb 2020 18:40:59 GMT'}]",cs.CL,2020-02-10 18:40:59,['language model'],True,Knowledge Graphs and Commonsense,['google.com'],True,False,False,0.25,948.0,0.984,0.9902461681374826,5,0.9886363636363636
1469,arXiv:2001.07676,"['Timo Schick', 'Hinrich Schütze']","Exploiting Cloze Questions for Few Shot Text Classification and Natural
  Language Inference",['cs.CL'],"  Some NLP tasks can be solved in a fully unsupervised fashion by providing a
pretrained language model with ""task descriptions"" in natural language (e.g.,
Radford et al., 2019). While this approach underperforms its supervised
counterpart, we show in this work that the two ideas can be combined: We
introduce Pattern-Exploiting Training (PET), a semi-supervised training
procedure that reformulates input examples as cloze-style phrases to help
language models understand a given task. These phrases are then used to assign
soft labels to a large set of unlabeled examples. Finally, standard supervised
training is performed on the resulting training set. For several tasks and
languages, PET outperforms supervised training and strong semi-supervised
approaches in low-resource settings by a large margin.
","[{'version': 'v1', 'created': 'Tue, 21 Jan 2020 17:57:33 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Apr 2020 15:58:33 GMT'}, {'version': 'v3', 'created': 'Mon, 25 Jan 2021 10:56:45 GMT'}]",cs.CL,2020-01-21 17:57:33,"['pretrained language model', 'language model']",True,Pretrained LMs & Text Classification,"['sulzer.de', 'lmu.de']",False,True,False,0.0,887.0,0.98,0.9897816999535531,2,0.9875
2164,arXiv:2005.14165,"['Tom B. Brown', 'Benjamin Mann', 'Nick Ryder', 'Melanie Subbiah', 'Jared Kaplan', 'Prafulla Dhariwal', 'Arvind Neelakantan', 'Pranav Shyam', 'Girish Sastry', 'Amanda Askell', 'Sandhini Agarwal', 'Ariel Herbert-Voss', 'Gretchen Krueger', 'Tom Henighan', 'Rewon Child', 'Aditya Ramesh', 'Daniel M. Ziegler', 'Jeffrey Wu', 'Clemens Winter', 'Christopher Hesse', 'Mark Chen', 'Eric Sigler', 'Mateusz Litwin', 'Scott Gray', 'Benjamin Chess', 'Jack Clark', 'Christopher Berner', 'Sam McCandlish', 'Alec Radford', 'Ilya Sutskever', 'Dario Amodei']",Language Models are Few-Shot Learners,['cs.CL'],"  Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.
","[{'version': 'v1', 'created': 'Thu, 28 May 2020 17:29:03 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Jun 2020 17:08:53 GMT'}, {'version': 'v3', 'created': 'Fri, 5 Jun 2020 02:52:35 GMT'}, {'version': 'v4', 'created': 'Wed, 22 Jul 2020 19:47:17 GMT'}]",cs.CL,2020-05-28 17:29:03,"['language model', 'GPT-3']",True,Pretrained LMs & Text Classification,[],False,False,False,0.1379310345,14638.0,0.9984126984126984,0.9948908499767766,31,0.9977272727272727
1763,arXiv:2004.05150,"['Iz Beltagy', 'Matthew E. Peters', 'Arman Cohan']",Longformer: The Long-Document Transformer,['cs.CL'],"  Transformer-based models are unable to process long sequences due to their
self-attention operation, which scales quadratically with the sequence length.
To address this limitation, we introduce the Longformer with an attention
mechanism that scales linearly with sequence length, making it easy to process
documents of thousands of tokens or longer. Longformer's attention mechanism is
a drop-in replacement for the standard self-attention and combines a local
windowed attention with a task motivated global attention. Following prior work
on long-sequence transformers, we evaluate Longformer on character-level
language modeling and achieve state-of-the-art results on text8 and enwik8. In
contrast to most prior work, we also pretrain Longformer and finetune it on a
variety of downstream tasks. Our pretrained Longformer consistently outperforms
RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop
and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a
Longformer variant for supporting long document generative sequence-to-sequence
tasks, and demonstrate its effectiveness on the arXiv summarization dataset.
","[{'version': 'v1', 'created': 'Fri, 10 Apr 2020 17:54:09 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Dec 2020 17:52:35 GMT'}]",cs.CL,2020-04-10 17:54:09,"['language model', 'BERT']",True,"Transformers, RNNs, Attention",['allenai.org'],False,False,False,0.0,1952.0,0.9968253968253968,0.9944263817928471,3,0.9965909090909091
2092,arXiv:2005.08100,"['Anmol Gulati', 'James Qin', 'Chung-Cheng Chiu', 'Niki Parmar', 'Yu Zhang', 'Jiahui Yu', 'Wei Han', 'Shibo Wang', 'Zhengdong Zhang', 'Yonghui Wu', 'Ruoming Pang']",Conformer: Convolution-augmented Transformer for Speech Recognition,"['eess.AS', 'cs.LG', 'cs.SD']","  Recently Transformer and Convolution neural network (CNN) based models have
shown promising results in Automatic Speech Recognition (ASR), outperforming
Recurrent neural networks (RNNs). Transformer models are good at capturing
content-based global interactions, while CNNs exploit local features
effectively. In this work, we achieve the best of both worlds by studying how
to combine convolution neural networks and transformers to model both local and
global dependencies of an audio sequence in a parameter-efficient way. To this
regard, we propose the convolution-augmented transformer for speech
recognition, named Conformer. Conformer significantly outperforms the previous
Transformer and CNN based models achieving state-of-the-art accuracies. On the
widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without
using a language model and 1.9%/3.9% with an external language model on
test/testother. We also observe competitive performance of 2.7%/6.3% with a
small model of only 10M parameters.
","[{'version': 'v1', 'created': 'Sat, 16 May 2020 20:56:25 GMT'}]",eess.AS,2020-05-16 20:56:25,['language model'],True,Speech Recognition,['google.com'],True,False,False,0.0,1557.0,0.9952380952380953,0.9939619136089178,11,0.9954545454545455
1834,arXiv:2004.10964,"['Suchin Gururangan', 'Ana Marasović', 'Swabha Swayamdipta', 'Kyle Lo', 'Iz Beltagy', 'Doug Downey', 'Noah A. Smith']",Don't Stop Pretraining: Adapt Language Models to Domains and Tasks,"['cs.CL', 'cs.LG']","  Language models pretrained on text from a wide variety of sources form the
foundation of today's NLP. In light of the success of these broad-coverage
models, we investigate whether it is still helpful to tailor a pretrained model
to the domain of a target task. We present a study across four domains
(biomedical and computer science publications, news, and reviews) and eight
classification tasks, showing that a second phase of pretraining in-domain
(domain-adaptive pretraining) leads to performance gains, under both high- and
low-resource settings. Moreover, adapting to the task's unlabeled data
(task-adaptive pretraining) improves performance even after domain-adaptive
pretraining. Finally, we show that adapting to a task corpus augmented using
simple data selection strategies is an effective alternative, especially when
resources for domain-adaptive pretraining might be unavailable. Overall, we
consistently find that multi-phase adaptive pretraining offers large gains in
task performance.
","[{'version': 'v1', 'created': 'Thu, 23 Apr 2020 04:21:19 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 05:07:34 GMT'}, {'version': 'v3', 'created': 'Tue, 5 May 2020 22:00:44 GMT'}]",cs.CL,2020-04-23 04:21:19,['language model'],True,Pretrained LMs & Text Classification,['allenai.org'],False,False,False,0.25,1467.0,0.9936507936507937,0.9934974454249884,7,0.9943181818181818
2203,arXiv:2006.03654,"['Pengcheng He', 'Xiaodong Liu', 'Jianfeng Gao', 'Weizhu Chen']",DeBERTa: Decoding-enhanced BERT with Disentangled Attention,"['cs.CL', 'cs.LG']","  Recent progress in pre-trained neural language models has significantly
improved the performance of many natural language processing (NLP) tasks. In
this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT
with disentangled attention) that improves the BERT and RoBERTa models using
two novel techniques. The first is the disentangled attention mechanism, where
each word is represented using two vectors that encode its content and
position, respectively, and the attention weights among words are computed
using disentangled matrices on their contents and relative positions,
respectively. Second, an enhanced mask decoder is used to incorporate absolute
positions in the decoding layer to predict the masked tokens in model
pre-training. In addition, a new virtual adversarial training method is used
for fine-tuning to improve models' generalization. We show that these
techniques significantly improve the efficiency of model pre-training and the
performance of both natural language understanding (NLU) and natural langauge
generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model
trained on half of the training data performs consistently better on a wide
range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),
on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).
Notably, we scale up DeBERTa by training a larger version that consists of 48
Transform layers with 1.5 billion parameters. The significant performance boost
makes the single DeBERTa model surpass the human performance on the SuperGLUE
benchmark (Wang et al., 2019a) for the first time in terms of macro-average
score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the
SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline
by a decent margin (90.3 versus 89.8).
","[{'version': 'v1', 'created': 'Fri, 5 Jun 2020 19:54:34 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Jan 2021 05:36:50 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Jan 2021 22:54:26 GMT'}, {'version': 'v4', 'created': 'Thu, 18 Mar 2021 02:27:00 GMT'}, {'version': 'v5', 'created': 'Mon, 4 Oct 2021 06:31:54 GMT'}, {'version': 'v6', 'created': 'Wed, 6 Oct 2021 21:02:00 GMT'}]",cs.CL,2020-06-05 19:54:34,"['BERT', 'language model']",True,"Transformers, RNNs, Attention",['microsoft.com'],True,False,False,0.0,1142.0,0.9920634920634921,0.9921040408732001,4,0.9920454545454546
2417,arXiv:2007.14062,"['Manzil Zaheer', 'Guru Guruganesh', 'Avinava Dubey', 'Joshua Ainslie', 'Chris Alberti', 'Santiago Ontanon', 'Philip Pham', 'Anirudh Ravula', 'Qifan Wang', 'Li Yang', 'Amr Ahmed']",Big Bird: Transformers for Longer Sequences,"['cs.LG', 'cs.CL', 'stat.ML']","  Transformers-based models, such as BERT, have been one of the most successful
deep learning models for NLP. Unfortunately, one of their core limitations is
the quadratic dependency (mainly in terms of memory) on the sequence length due
to their full attention mechanism. To remedy this, we propose, BigBird, a
sparse attention mechanism that reduces this quadratic dependency to linear. We
show that BigBird is a universal approximator of sequence functions and is
Turing complete, thereby preserving these properties of the quadratic, full
attention model. Along the way, our theoretical analysis reveals some of the
benefits of having $O(1)$ global tokens (such as CLS), that attend to the
entire sequence as part of the sparse attention mechanism. The proposed sparse
attention can handle sequences of length up to 8x of what was previously
possible using similar hardware. As a consequence of the capability to handle
longer context, BigBird drastically improves performance on various NLP tasks
such as question answering and summarization. We also propose novel
applications to genomics data.
","[{'version': 'v1', 'created': 'Tue, 28 Jul 2020 08:34:04 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Jan 2021 07:41:50 GMT'}]",cs.LG,2020-07-28 08:34:04,['BERT'],True,"Transformers, RNNs, Attention",['google.com'],True,False,False,0.0,1143.0,0.982532751091703,0.9925685090571296,11,0.992930086410055
2430,arXiv:2007.15779,"['Yu Gu', 'Robert Tinn', 'Hao Cheng', 'Michael Lucas', 'Naoto Usuyama', 'Xiaodong Liu', 'Tristan Naumann', 'Jianfeng Gao', 'Hoifung Poon']","Domain-Specific Language Model Pretraining for Biomedical Natural
  Language Processing","['cs.CL', 'cs.LG']","  Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding & Reasoning
Benchmark) at https://aka.ms/BLURB.
","[{'version': 'v1', 'created': 'Fri, 31 Jul 2020 00:04:15 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Aug 2020 02:01:07 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Aug 2020 22:26:29 GMT'}, {'version': 'v4', 'created': 'Thu, 11 Feb 2021 19:13:59 GMT'}, {'version': 'v5', 'created': 'Fri, 25 Jun 2021 00:38:22 GMT'}, {'version': 'v6', 'created': 'Thu, 16 Sep 2021 21:26:07 GMT'}]",cs.CL,2020-07-31 00:04:15,"['language model', 'BERT']",True,NLP for Healthcare,['microsoft.com'],True,False,False,0.0,761.0,0.980349344978166,0.988388295401765,9,0.99057344854674
2313,arXiv:2007.00808,"['Lee Xiong', 'Chenyan Xiong', 'Ye Li', 'Kwok-Fung Tang', 'Jialin Liu', 'Paul Bennett', 'Junaid Ahmed', 'Arnold Overwijk']","Approximate Nearest Neighbor Negative Contrastive Learning for Dense
  Text Retrieval","['cs.IR', 'cs.CL', 'cs.LG']","  Conducting text retrieval in a dense learned representation space has many
intriguing advantages over sparse retrieval. Yet the effectiveness of dense
retrieval (DR) often requires combination with sparse retrieval. In this paper,
we identify that the main bottleneck is in the training mechanisms, where the
negative instances used in training are not representative of the irrelevant
documents in testing. This paper presents Approximate nearest neighbor Negative
Contrastive Estimation (ANCE), a training mechanism that constructs negatives
from an Approximate Nearest Neighbor (ANN) index of the corpus, which is
parallelly updated with the learning process to select more realistic negative
training instances. This fundamentally resolves the discrepancy between the
data distribution used in the training and testing of DR. In our experiments,
ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and
sparse retrieval baselines. It nearly matches the accuracy of
sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned
representation space and provides almost 100x speed-up.
","[{'version': 'v1', 'created': 'Wed, 1 Jul 2020 23:15:56 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 22:17:19 GMT'}]",cs.IR,2020-07-01 23:15:56,['BERT'],True,"Search, Ranking, Retrieval",['microsoft.com'],True,False,False,0.0,664.0,0.9781659388646288,0.9865304226660474,8,0.9890023566378633
2650,arXiv:2009.07118,"['Timo Schick', 'Hinrich Schütze']","It's Not Just Size That Matters: Small Language Models Are Also Few-Shot
  Learners","['cs.CL', 'cs.AI', 'cs.LG']","  When scaled to hundreds of billions of parameters, pretrained language models
such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance.
However, enormous amounts of compute are required for training and applying
such big models, resulting in a large carbon footprint and making it difficult
for researchers and practitioners to use them. We show that performance similar
to GPT-3 can be obtained with language models that are much ""greener"" in that
their parameter count is several orders of magnitude smaller. This is achieved
by converting textual inputs into cloze questions that contain a task
description, combined with gradient-based optimization; exploiting unlabeled
data gives further improvements. We identify key factors required for
successful natural language understanding with small language models.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 14:18:53 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Apr 2021 08:16:59 GMT'}]",cs.CL,2020-09-15 14:18:53,"['pretrained language model', 'language model', 'GPT-3']",True,Efficiency & Performance,['sulzer.de'],False,False,False,0.0,600.0,0.9759825327510917,0.9856014862981886,2,0.9874312647289867
2579,arXiv:2009.01325,"['Nisan Stiennon', 'Long Ouyang', 'Jeff Wu', 'Daniel M. Ziegler', 'Ryan Lowe', 'Chelsea Voss', 'Alec Radford', 'Dario Amodei', 'Paul Christiano']",Learning to summarize from human feedback,"['cs.CL', 'cs.AI', 'cs.LG']","  As language models become more powerful, training and evaluation are
increasingly bottlenecked by the data and metrics used for a particular task.
For example, summarization models are often trained to predict human reference
summaries and evaluated using ROUGE, but both of these metrics are rough
proxies for what we really care about -- summary quality. In this work, we show
that it is possible to significantly improve summary quality by training a
model to optimize for human preferences. We collect a large, high-quality
dataset of human comparisons between summaries, train a model to predict the
human-preferred summary, and use that model as a reward function to fine-tune a
summarization policy using reinforcement learning. We apply our method to a
version of the TL;DR dataset of Reddit posts and find that our models
significantly outperform both human reference summaries and much larger models
fine-tuned with supervised learning alone. Our models also transfer to CNN/DM
news articles, producing summaries nearly as good as the human reference
without any news-specific fine-tuning. We conduct extensive analyses to
understand our human feedback dataset and fine-tuned models We establish that
our reward model generalizes to new datasets, and that optimizing our reward
model results in better summaries than optimizing ROUGE according to humans. We
hope the evidence from our paper motivates machine learning researchers to pay
closer attention to how their training loss affects the model behavior they
actually want.
","[{'version': 'v1', 'created': 'Wed, 2 Sep 2020 19:54:41 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 22:19:53 GMT'}, {'version': 'v3', 'created': 'Tue, 15 Feb 2022 19:09:36 GMT'}]",cs.CL,2020-09-02 19:54:41,['language model'],True,Summarization and Evaluation,[],False,False,False,0.125,492.0,0.9737991266375546,0.9823502090106828,9,0.9866457187745483
3575,arXiv:2012.15723,"['Tianyu Gao', 'Adam Fisch', 'Danqi Chen']",Making Pre-trained Language Models Better Few-shot Learners,"['cs.CL', 'cs.LG']","  The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot
performance solely by leveraging a natural-language prompt and a few task
demonstrations as input context. Inspired by their findings, we study few-shot
learning in a more practical scenario, where we use smaller language models for
which fine-tuning is computationally efficient. We present LM-BFF--better
few-shot fine-tuning of language models--a suite of simple and complementary
techniques for fine-tuning language models on a small number of annotated
examples. Our approach includes (1) prompt-based fine-tuning together with a
novel pipeline for automating prompt generation; and (2) a refined strategy for
dynamically and selectively incorporating demonstrations into each context.
Finally, we present a systematic evaluation for analyzing few-shot performance
on a range of NLP tasks, including classification and regression. Our
experiments demonstrate that our methods combine to dramatically outperform
standard fine-tuning procedures in this low resource setting, achieving up to
30% absolute improvement, and 11% on average across all tasks. Our approach
makes minimal assumptions on task resources and domain expertise, and hence
constitutes a strong task-agnostic method for few-shot learning.
","[{'version': 'v1', 'created': 'Thu, 31 Dec 2020 17:21:26 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Jun 2021 12:41:36 GMT'}]",cs.CL,2020-12-31 17:21:26,"['language model', 'GPT-3']",True,Prompts & In-Context Learning,"['mit.edu', 'princeton.edu']",False,True,False,0.0,989.0,0.9987730061349693,0.9911751045053414,3,0.9921445404556166
3379,arXiv:2012.00364,"['Hanting Chen', 'Yunhe Wang', 'Tianyu Guo', 'Chang Xu', 'Yiping Deng', 'Zhenhua Liu', 'Siwei Ma', 'Chunjing Xu', 'Chao Xu', 'Wen Gao']",Pre-Trained Image Processing Transformer,"['cs.CV', 'cs.LG']","  As the computing power of modern hardware is increasing strongly, pre-trained
deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have
shown their effectiveness over conventional methods. The big progress is mainly
contributed to the representation ability of transformer and its variant
architectures. In this paper, we study the low-level computer vision task
(e.g., denoising, super-resolution and deraining) and develop a new pre-trained
model, namely, image processing transformer (IPT). To maximally excavate the
capability of transformer, we present to utilize the well-known ImageNet
benchmark for generating a large amount of corrupted image pairs. The IPT model
is trained on these images with multi-heads and multi-tails. In addition, the
contrastive learning is introduced for well adapting to different image
processing tasks. The pre-trained model can therefore efficiently employed on
desired task after fine-tuning. With only one pre-trained model, IPT
outperforms the current state-of-the-art methods on various low-level
benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT
and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT
","[{'version': 'v1', 'created': 'Tue, 1 Dec 2020 09:42:46 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Dec 2020 05:02:15 GMT'}, {'version': 'v3', 'created': 'Fri, 28 May 2021 02:41:58 GMT'}, {'version': 'v4', 'created': 'Mon, 8 Nov 2021 07:08:21 GMT'}]",cs.CV,2020-12-01 09:42:46,"['BERT', 'GPT-3']",True,Vision-Language Models,"['sydney.edu.au', 'huawei.com', 'pku.edu.cn']",True,True,False,0.0,856.0,0.9975460122699387,0.9893172317696238,10,0.9913589945011784
3462,arXiv:2012.07805,"['Nicholas Carlini', 'Florian Tramer', 'Eric Wallace', 'Matthew Jagielski', 'Ariel Herbert-Voss', 'Katherine Lee', 'Adam Roberts', 'Tom Brown', 'Dawn Song', 'Ulfar Erlingsson', 'Alina Oprea', 'Colin Raffel']",Extracting Training Data from Large Language Models,"['cs.CR', 'cs.CL', 'cs.LG']","  It has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.
  We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.
  We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models.
","[{'version': 'v1', 'created': 'Mon, 14 Dec 2020 18:39:09 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Jun 2021 17:45:26 GMT'}]",cs.CR,2020-12-14 18:39:09,"['language model', 'large language model', 'GPT-2']",True,Privacy & Adversarial Risks,['---.------------.com'],False,False,False,0.2727272727,701.0,0.996319018404908,0.9874593590339061,12,0.9897879025923016
3581,arXiv:2101.00027,"['Leo Gao', 'Stella Biderman', 'Sid Black', 'Laurence Golding', 'Travis Hoppe', 'Charles Foster', 'Jason Phang', 'Horace He', 'Anish Thite', 'Noa Nabeshima', 'Shawn Presser', 'Connor Leahy']",The Pile: An 800GB Dataset of Diverse Text for Language Modeling,['cs.CL'],"  Recent work has demonstrated that increased training dataset diversity
improves general cross-domain knowledge and downstream generalization
capability for large-scale language models. With this in mind, we present
\textit{the Pile}: an 825 GiB English text corpus targeted at training
large-scale language models. The Pile is constructed from 22 diverse
high-quality subsets -- both existing and newly constructed -- many of which
derive from academic or professional sources. Our evaluation of the untuned
performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on
many of its components, such as academic writing. Conversely, models trained on
the Pile improve significantly over both Raw CC and CC-100 on all components of
the Pile, while improving performance on downstream evaluations. Through an
in-depth exploratory analysis, we document potentially concerning aspects of
the data for prospective users. We make publicly available the code used in its
construction.
","[{'version': 'v1', 'created': 'Thu, 31 Dec 2020 19:00:10 GMT'}]",cs.CL,2020-12-31 19:00:10,"['language model', 'GPT-3', 'GPT-2']",True,Datasets & Benchmarks,['eleuther.ai'],False,False,False,0.1,655.0,0.9950920245398773,0.986065954482118,12,0.9882168106834249
2798,arXiv:2010.01057,"['Ikuya Yamada', 'Akari Asai', 'Hiroyuki Shindo', 'Hideaki Takeda', 'Yuji Matsumoto']","LUKE: Deep Contextualized Entity Representations with Entity-aware
  Self-attention","['cs.CL', 'cs.LG']","  Entity representations are useful in natural language tasks involving
entities. In this paper, we propose new pretrained contextualized
representations of words and entities based on the bidirectional transformer.
The proposed model treats words and entities in a given text as independent
tokens, and outputs contextualized representations of them. Our model is
trained using a new pretraining task based on the masked language model of
BERT. The task involves predicting randomly masked words and entities in a
large entity-annotated corpus retrieved from Wikipedia. We also propose an
entity-aware self-attention mechanism that is an extension of the
self-attention mechanism of the transformer, and considers the types of tokens
(words or entities) when computing attention scores. The proposed model
achieves impressive empirical performance on a wide range of entity-related
tasks. In particular, it obtains state-of-the-art results on five well-known
datasets: Open Entity (entity typing), TACRED (relation classification),
CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),
and SQuAD 1.1 (extractive question answering). Our source code and pretrained
representations are available at https://github.com/studio-ousia/luke.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 15:38:03 GMT'}]",cs.CL,2020-10-02 15:38:03,"['language model', 'BERT']",True,Entity Extraction & RecSys,"['ousia.jp', 'nii.ac.jp', 'is.naist.jp', 'washington.edu']",False,True,False,0.2,435.0,0.9938650306748467,0.9800278680910358,5,0.983503534956795
3589,arXiv:2101.00190,"['Xiang Lisa Li', 'Percy Liang']",Prefix-Tuning: Optimizing Continuous Prompts for Generation,['cs.CL'],"  Fine-tuning is the de facto way to leverage large pretrained language models
to perform downstream tasks. However, it modifies all the language model
parameters and therefore necessitates storing a full copy for each task. In
this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning
for natural language generation tasks, which keeps language model parameters
frozen, but optimizes a small continuous task-specific vector (called the
prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent
tokens to attend to this prefix as if it were ""virtual tokens"". We apply
prefix-tuning to GPT-2 for table-to-text generation and to BART for
summarization. We find that by learning only 0.1\% of the parameters,
prefix-tuning obtains comparable performance in the full data setting,
outperforms fine-tuning in low-data settings, and extrapolates better to
examples with topics unseen during training.
","[{'version': 'v1', 'created': 'Fri, 1 Jan 2021 08:00:36 GMT'}]",cs.CL,2021-01-01 08:00:36,"['pretrained language model', 'language model', 'GPT-2']",True,Pretrained LMs & Text Classification,['stanford.edu'],False,True,False,0.0,1496.0,0.9943181818181818,0.9969919786096256,2,0.9964362081254454
3744,arXiv:2101.11986,"['Li Yuan', 'Yunpeng Chen', 'Tao Wang', 'Weihao Yu', 'Yujun Shi', 'Zihang Jiang', 'Francis EH Tay', 'Jiashi Feng', 'Shuicheng Yan']","Tokens-to-Token ViT: Training Vision Transformers from Scratch on
  ImageNet",['cs.CV'],"  Transformers, which are popular for language modeling, have been explored for
solving vision tasks recently, e.g., the Vision Transformer (ViT) for image
classification. The ViT model splits each image into a sequence of tokens with
fixed length and then applies multiple Transformer layers to model their global
relation for classification. However, ViT achieves inferior performance to CNNs
when trained from scratch on a midsize dataset like ImageNet. We find it is
because: 1) the simple tokenization of input images fails to model the
important local structure such as edges and lines among neighboring pixels,
leading to low training sample efficiency; 2) the redundant attention backbone
design of ViT leads to limited feature richness for fixed computation budgets
and limited training samples. To overcome such limitations, we propose a new
Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a
layer-wise Tokens-to-Token (T2T) transformation to progressively structurize
the image to tokens by recursively aggregating neighboring Tokens into one
Token (Tokens-to-Token), such that local structure represented by surrounding
tokens can be modeled and tokens length can be reduced; 2) an efficient
backbone with a deep-narrow structure for vision transformer motivated by CNN
architecture design after empirical study. Notably, T2T-ViT reduces the
parameter count and MACs of vanilla ViT by half, while achieving more than
3.0\% improvement when trained from scratch on ImageNet. It also outperforms
ResNets and achieves comparable performance with MobileNets by directly
training on ImageNet. For example, T2T-ViT with comparable size to ResNet50
(21.5M parameters) can achieve 83.3\% top1 accuracy in image resolution
384$\times$384 on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)
","[{'version': 'v1', 'created': 'Thu, 28 Jan 2021 13:25:28 GMT'}, {'version': 'v2', 'created': 'Mon, 22 Mar 2021 11:58:10 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Nov 2021 12:30:01 GMT'}]",cs.CV,2021-01-28 13:25:28,['language model'],True,Vision-Language Models,"['yitu-inc.com', 'nus.edu.sg']",False,True,False,0.0,1124.0,0.9924242424242424,0.9949866310160428,9,0.9935851746258019
3642,arXiv:2101.03961,"['William Fedus', 'Barret Zoph', 'Noam Shazeer']","Switch Transformers: Scaling to Trillion Parameter Models with Simple
  and Efficient Sparsity","['cs.LG', 'cs.AI']","  In deep learning, models typically reuse the same parameters for all inputs.
Mixture of Experts (MoE) defies this and instead selects different parameters
for each incoming example. The result is a sparsely-activated model -- with
outrageous numbers of parameters -- but a constant computational cost. However,
despite several notable successes of MoE, widespread adoption has been hindered
by complexity, communication costs and training instability -- we address these
with the Switch Transformer. We simplify the MoE routing algorithm and design
intuitive improved models with reduced communication and computational costs.
Our proposed training techniques help wrangle the instabilities and we show
large sparse models may be trained, for the first time, with lower precision
(bfloat16) formats. We design models based off T5-Base and T5-Large to obtain
up to 7x increases in pre-training speed with the same computational resources.
These improvements extend into multilingual settings where we measure gains
over the mT5-Base version across all 101 languages. Finally, we advance the
current scale of language models by pre-training up to trillion parameter
models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the
T5-XXL model.
","[{'version': 'v1', 'created': 'Mon, 11 Jan 2021 16:11:52 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Apr 2022 00:02:01 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Jun 2022 20:36:07 GMT'}]",cs.LG,2021-01-11 16:11:52,['language model'],True,Efficiency & Performance,['google.com'],True,False,False,0.0,850.0,0.990530303030303,0.9936497326203209,3,0.9914468995010691
3865,arXiv:2102.09690,"['Tony Z. Zhao', 'Eric Wallace', 'Shi Feng', 'Dan Klein', 'Sameer Singh']",Calibrate Before Use: Improving Few-Shot Performance of Language Models,"['cs.CL', 'cs.LG']","  GPT-3 can perform numerous tasks when provided a natural language prompt that
contains a few training examples. We show that this type of few-shot learning
can be unstable: the choice of prompt format, training examples, and even the
order of the training examples can cause accuracy to vary from near chance to
near state-of-the-art. We demonstrate that this instability arises from the
bias of language models towards predicting certain answers, e.g., those that
are placed near the end of the prompt or are common in the pre-training data.
To mitigate this, we first estimate the model's bias towards each answer by
asking for its prediction when given the training prompt and a content-free
test input such as ""N/A"". We then fit calibration parameters that cause the
prediction for this input to be uniform across answers. On a diverse set of
tasks, this contextual calibration procedure substantially improves GPT-3 and
GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across
different choices of the prompt.
","[{'version': 'v1', 'created': 'Fri, 19 Feb 2021 00:23:59 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Jun 2021 18:20:59 GMT'}]",cs.CL,2021-02-19 00:23:59,"['language model', 'GPT-3', 'GPT-2']",True,Prompts & In-Context Learning,['berkeley.edu'],False,True,False,0.0,528.0,0.9886363636363636,0.991644385026738,5,0.9893086243763364
4039,arXiv:2103.10385,"['Xiao Liu', 'Yanan Zheng', 'Zhengxiao Du', 'Ming Ding', 'Yujie Qian', 'Zhilin Yang', 'Jie Tang']","GPT Understands, Too","['cs.CL', 'cs.LG']","  While GPTs with traditional fine-tuning fail to achieve strong results on
natural language understanding (NLU), we show that GPTs can be better than or
comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning --
which employs trainable continuous prompt embeddings. On the knowledge probing
(LAMA) benchmark, the best GPT recovers 64\% (P@1) of world knowledge without
any additional text provided during test time, which substantially improves the
previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs
achieve comparable and sometimes better performance to similar-sized BERTs in
supervised learning. Importantly, we find that P-tuning also improves BERTs'
performance in both few-shot and supervised settings while largely reducing the
need for prompt engineering. Consequently, P-tuning outperforms the
state-of-the-art approaches on the few-shot SuperGlue benchmark.
","[{'version': 'v1', 'created': 'Thu, 18 Mar 2021 17:13:50 GMT'}]",cs.CL,2021-03-18 17:13:50,['BERT'],True,Pretrained LMs & Text Classification,"['rcrai.com', 'tsinghua.edu.cn']",False,True,False,0.33333333330000003,503.0,0.9867424242424242,0.990975935828877,7,0.9885958660014256
4341,arXiv:2104.08821,"['Tianyu Gao', 'Xingcheng Yao', 'Danqi Chen']",SimCSE: Simple Contrastive Learning of Sentence Embeddings,"['cs.CL', 'cs.LG']","  This paper presents SimCSE, a simple contrastive learning framework that
greatly advances state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We find that dropout acts as minimal data augmentation, and
removing it leads to a representation collapse. Then, we propose a supervised
approach, which incorporates annotated pairs from natural language inference
datasets into our contrastive learning framework by using ""entailment"" pairs as
positives and ""contradiction"" pairs as hard negatives. We evaluate SimCSE on
standard semantic textual similarity (STS) tasks, and our unsupervised and
supervised models using BERT base achieve an average of 76.3% and 81.6%
Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to
the previous best results. We also show -- both theoretically and empirically
-- that the contrastive learning objective regularizes pre-trained embeddings'
anisotropic space to be more uniform, and it better aligns positive pairs when
supervised signals are available.
","[{'version': 'v1', 'created': 'Sun, 18 Apr 2021 11:27:08 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Aug 2021 17:47:06 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Sep 2021 22:09:51 GMT'}, {'version': 'v4', 'created': 'Wed, 18 May 2022 12:29:49 GMT'}]",cs.CL,2021-04-18 11:27:08,['BERT'],True,"Representations, Syntax, Semantics","['tsinghua.edu.cn', 'princeton.edu']",False,True,False,0.0,1450.0,0.9977142857142857,0.9966577540106952,3,0.9957234497505346
4327,arXiv:2104.08691,"['Brian Lester', 'Rami Al-Rfou', 'Noah Constant']",The Power of Scale for Parameter-Efficient Prompt Tuning,['cs.CL'],"  In this work, we explore ""prompt tuning"", a simple yet effective mechanism
for learning ""soft prompts"" to condition frozen language models to perform
specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft
prompts are learned through backpropagation and can be tuned to incorporate
signal from any number of labeled examples. Our end-to-end learned approach
outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably,
through ablations on model size using T5, we show that prompt tuning becomes
more competitive with scale: as models exceed billions of parameters, our
method ""closes the gap"" and matches the strong performance of model tuning
(where all model weights are tuned). This finding is especially relevant in
that large models are costly to share and serve, and the ability to reuse one
frozen model for multiple downstream tasks can ease this burden. Our method can
be seen as a simplification of the recently proposed ""prefix tuning"" of Li and
Liang (2021), and we provide a comparison to this and other similar approaches.
Finally, we show that conditioning a frozen model with soft prompts confers
benefits in robustness to domain transfer, as compared to full model tuning.
","[{'version': 'v1', 'created': 'Sun, 18 Apr 2021 03:19:26 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Sep 2021 17:34:41 GMT'}]",cs.CL,2021-04-18 03:19:26,"['language model', 'GPT-3']",True,Prompts & In-Context Learning,['google.com'],True,False,False,0.0,1359.0,0.9965714285714286,0.9959893048128342,3,0.9950106913756237
4882,arXiv:2106.08254,"['Hangbo Bao', 'Li Dong', 'Songhao Piao', 'Furu Wei']",BEiT: BERT Pre-Training of Image Transformers,"['cs.CV', 'cs.LG']","  We introduce a self-supervised vision representation model BEiT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT developed in the natural language processing area, we propose a masked
image modeling task to pretrain vision Transformers. Specifically, each image
has two views in our pre-training, i.e, image patches (such as 16x16 pixels),
and visual tokens (i.e., discrete tokens). We first ""tokenize"" the original
image into visual tokens. Then we randomly mask some image patches and fed them
into the backbone Transformer. The pre-training objective is to recover the
original visual tokens based on the corrupted image patches. After pre-training
BEiT, we directly fine-tune the model parameters on downstream tasks by
appending task layers upon the pretrained encoder. Experimental results on
image classification and semantic segmentation show that our model achieves
competitive results with previous pre-training methods. For example, base-size
BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming
from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size
BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with
supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models
are available at https://aka.ms/beit.
","[{'version': 'v1', 'created': 'Tue, 15 Jun 2021 16:02:37 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Sep 2022 14:11:33 GMT'}]",cs.CV,2021-06-15 16:02:37,['BERT'],True,Vision-Language Models,['microsoft.com'],True,False,False,0.0,1309.0,0.9954285714285714,0.9953208556149733,4,0.9942979330007128
4903,arXiv:2106.09685,"['Edward J. Hu', 'Yelong Shen', 'Phillip Wallis', 'Zeyuan Allen-Zhu', 'Yuanzhi Li', 'Shean Wang', 'Lu Wang', 'Weizhu Chen']",LoRA: Low-Rank Adaptation of Large Language Models,"['cs.CL', 'cs.AI', 'cs.LG']","  An important paradigm of natural language processing consists of large-scale
pre-training on general domain data and adaptation to particular tasks or
domains. As we pre-train larger models, full fine-tuning, which retrains all
model parameters, becomes less feasible. Using GPT-3 175B as an example --
deploying independent instances of fine-tuned models, each with 175B
parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or
LoRA, which freezes the pre-trained model weights and injects trainable rank
decomposition matrices into each layer of the Transformer architecture, greatly
reducing the number of trainable parameters for downstream tasks. Compared to
GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable
parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA
performs on-par or better than fine-tuning in model quality on RoBERTa,
DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher
training throughput, and, unlike adapters, no additional inference latency. We
also provide an empirical investigation into rank-deficiency in language model
adaptation, which sheds light on the efficacy of LoRA. We release a package
that facilitates the integration of LoRA with PyTorch models and provide our
implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at
https://github.com/microsoft/LoRA.
","[{'version': 'v1', 'created': 'Thu, 17 Jun 2021 17:37:18 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Oct 2021 18:40:34 GMT'}]",cs.CL,2021-06-17 17:37:18,"['language model', 'GPT-3', 'large language model', 'GPT-2', 'BERT']",True,Efficiency & Performance,['microsoft.com'],True,False,False,0.0,989.0,0.9942857142857143,0.9943181818181818,8,0.992872416250891
4864,arXiv:2106.07447,"['Wei-Ning Hsu', 'Benjamin Bolte', 'Yao-Hung Hubert Tsai', 'Kushal Lakhotia', 'Ruslan Salakhutdinov', 'Abdelrahman Mohamed']","HuBERT: Self-Supervised Speech Representation Learning by Masked
  Prediction of Hidden Units","['cs.CL', 'cs.AI', 'cs.LG', 'eess.AS']","  Self-supervised approaches for speech representation learning are challenged
by three unique problems: (1) there are multiple sound units in each input
utterance, (2) there is no lexicon of input sound units during the pre-training
phase, and (3) sound units have variable lengths with no explicit segmentation.
To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)
approach for self-supervised speech representation learning, which utilizes an
offline clustering step to provide aligned target labels for a BERT-like
prediction loss. A key ingredient of our approach is applying the prediction
loss over the masked regions only, which forces the model to learn a combined
acoustic and language model over the continuous inputs. HuBERT relies primarily
on the consistency of the unsupervised clustering step rather than the
intrinsic quality of the assigned cluster labels. Starting with a simple
k-means teacher of 100 clusters, and using two iterations of clustering, the
HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0
performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with
10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,
HuBERT shows up to 19% and 13% relative WER reduction on the more challenging
dev-other and test-other evaluation subsets.
","[{'version': 'v1', 'created': 'Mon, 14 Jun 2021 14:14:28 GMT'}]",cs.CL,2021-06-14 14:14:28,"['BERT', 'language model']",True,Speech Recognition,[],False,False,False,0.0,931.0,0.9931428571428571,0.9939839572192514,6,0.99215965787598
5231,arXiv:2108.07258,"['Rishi Bommasani', 'Drew A. Hudson', 'Ehsan Adeli', 'Russ Altman', 'Simran Arora', 'Sydney von Arx', 'Michael S. Bernstein', 'Jeannette Bohg', 'Antoine Bosselut', 'Emma Brunskill', 'Erik Brynjolfsson', 'Shyamal Buch', 'Dallas Card', 'Rodrigo Castellon', 'Niladri Chatterji', 'Annie Chen', 'Kathleen Creel', 'Jared Quincy Davis', 'Dora Demszky', 'Chris Donahue', 'Moussa Doumbouya', 'Esin Durmus', 'Stefano Ermon', 'John Etchemendy', 'Kawin Ethayarajh', 'Li Fei-Fei', 'Chelsea Finn', 'Trevor Gale', 'Lauren Gillespie', 'Karan Goel', 'Noah Goodman', 'Shelby Grossman', 'Neel Guha', 'Tatsunori Hashimoto', 'Peter Henderson', 'John Hewitt', 'Daniel E. Ho', 'Jenny Hong', 'Kyle Hsu', 'Jing Huang', 'Thomas Icard', 'Saahil Jain', 'Dan Jurafsky', 'Pratyusha Kalluri', 'Siddharth Karamcheti', 'Geoff Keeling', 'Fereshte Khani', 'Omar Khattab', 'Pang Wei Koh', 'Mark Krass', 'Ranjay Krishna', 'Rohith Kuditipudi', 'Ananya Kumar', 'Faisal Ladhak', 'Mina Lee', 'Tony Lee', 'Jure Leskovec', 'Isabelle Levent', 'Xiang Lisa Li', 'Xuechen Li', 'Tengyu Ma', 'Ali Malik', 'Christopher D. Manning', 'Suvir Mirchandani', 'Eric Mitchell', 'Zanele Munyikwa', 'Suraj Nair', 'Avanika Narayan', 'Deepak Narayanan', 'Ben Newman', 'Allen Nie', 'Juan Carlos Niebles', 'Hamed Nilforoshan', 'Julian Nyarko', 'Giray Ogut', 'Laurel Orr', 'Isabel Papadimitriou', 'Joon Sung Park', 'Chris Piech', 'Eva Portelance', 'Christopher Potts', 'Aditi Raghunathan', 'Rob Reich', 'Hongyu Ren', 'Frieda Rong', 'Yusuf Roohani', 'Camilo Ruiz', 'Jack Ryan', 'Christopher Ré', 'Dorsa Sadigh', 'Shiori Sagawa', 'Keshav Santhanam', 'Andy Shih', 'Krishnan Srinivasan', 'Alex Tamkin', 'Rohan Taori', 'Armin W. Thomas', 'Florian Tramèr', 'Rose E. Wang', 'William Wang', 'Bohan Wu', 'Jiajun Wu', 'Yuhuai Wu', 'Sang Michael Xie', 'Michihiro Yasunaga', 'Jiaxuan You', 'Matei Zaharia', 'Michael Zhang', 'Tianyi Zhang', 'Xikun Zhang', 'Yuhui Zhang', 'Lucia Zheng', 'Kaitlyn Zhou', 'Percy Liang']",On the Opportunities and Risks of Foundation Models,"['cs.LG', 'cs.AI', 'cs.CY']","  AI is undergoing a paradigm shift with the rise of models (e.g., BERT,
DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a
wide range of downstream tasks. We call these models foundation models to
underscore their critically central yet incomplete character. This report
provides a thorough account of the opportunities and risks of foundation
models, ranging from their capabilities (e.g., language, vision, robotics,
reasoning, human interaction) and technical principles(e.g., model
architectures, training procedures, data, systems, security, evaluation,
theory) to their applications (e.g., law, healthcare, education) and societal
impact (e.g., inequity, misuse, economic and environmental impact, legal and
ethical considerations). Though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent
capabilities,and their effectiveness across so many tasks incentivizes
homogenization. Homogenization provides powerful leverage but demands caution,
as the defects of the foundation model are inherited by all the adapted models
downstream. Despite the impending widespread deployment of foundation models,
we currently lack a clear understanding of how they work, when they fail, and
what they are even capable of due to their emergent properties. To tackle these
questions, we believe much of the critical research on foundation models will
require deep interdisciplinary collaboration commensurate with their
fundamentally sociotechnical nature.
","[{'version': 'v1', 'created': 'Mon, 16 Aug 2021 17:50:08 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Aug 2021 17:07:22 GMT'}, {'version': 'v3', 'created': 'Tue, 12 Jul 2022 23:45:14 GMT'}]",cs.LG,2021-08-16 17:50:08,"['BERT', 'GPT-3', 'foundation model']",True,Applications of LLMs/ChatGPT,['stanford.edu'],False,True,False,0.26373626370000003,1505.0,0.9975062344139651,0.9973262032085561,114,0.9981120201384519
5020,arXiv:2107.03374,"['Mark Chen', 'Jerry Tworek', 'Heewoo Jun', 'Qiming Yuan', 'Henrique Ponde de Oliveira Pinto', 'Jared Kaplan', 'Harri Edwards', 'Yuri Burda', 'Nicholas Joseph', 'Greg Brockman', 'Alex Ray', 'Raul Puri', 'Gretchen Krueger', 'Michael Petrov', 'Heidy Khlaaf', 'Girish Sastry', 'Pamela Mishkin', 'Brooke Chan', 'Scott Gray', 'Nick Ryder', 'Mikhail Pavlov', 'Alethea Power', 'Lukasz Kaiser', 'Mohammad Bavarian', 'Clemens Winter', 'Philippe Tillet', 'Felipe Petroski Such', 'Dave Cummings', 'Matthias Plappert', 'Fotios Chantzis', 'Elizabeth Barnes', 'Ariel Herbert-Voss', 'William Hebgen Guss', 'Alex Nichol', 'Alex Paino', 'Nikolas Tezak', 'Jie Tang', 'Igor Babuschkin', 'Suchir Balaji', 'Shantanu Jain', 'William Saunders', 'Christopher Hesse', 'Andrew N. Carr', 'Jan Leike', 'Josh Achiam', 'Vedant Misra', 'Evan Morikawa', 'Alec Radford', 'Matthew Knight', 'Miles Brundage', 'Mira Murati', 'Katie Mayer', 'Peter Welinder', 'Bob McGrew', 'Dario Amodei', 'Sam McCandlish', 'Ilya Sutskever', 'Wojciech Zaremba']",Evaluating Large Language Models Trained on Code,['cs.LG'],"  We introduce Codex, a GPT language model fine-tuned on publicly available
code from GitHub, and study its Python code-writing capabilities. A distinct
production version of Codex powers GitHub Copilot. On HumanEval, a new
evaluation set we release to measure functional correctness for synthesizing
programs from docstrings, our model solves 28.8% of the problems, while GPT-3
solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
from the model is a surprisingly effective strategy for producing working
solutions to difficult prompts. Using this method, we solve 70.2% of our
problems with 100 samples per problem. Careful investigation of our model
reveals its limitations, including difficulty with docstrings describing long
chains of operations and with binding operations to variables. Finally, we
discuss the potential broader impacts of deploying powerful code generation
technologies, covering safety, security, and economics.
","[{'version': 'v1', 'created': 'Wed, 7 Jul 2021 17:41:24 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Jul 2021 17:16:02 GMT'}]",cs.LG,2021-07-07 17:41:24,"['language model', 'GPT-J', 'large language model', 'GPT-3']",True,Code Generation,['openai.com'],True,False,False,0.14814814810000002,1382.0,0.9962593516209476,0.9963235294117647,58,0.9974826935179358
5133,arXiv:2107.13586,"['Pengfei Liu', 'Weizhe Yuan', 'Jinlan Fu', 'Zhengbao Jiang', 'Hiroaki Hayashi', 'Graham Neubig']","Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods
  in Natural Language Processing","['cs.CL', 'cs.AI', 'cs.LG']","  This paper surveys and organizes research works in a new paradigm in natural
language processing, which we dub ""prompt-based learning"". Unlike traditional
supervised learning, which trains a model to take in an input x and predict an
output y as P(y|x), prompt-based learning is based on language models that
model the probability of text directly. To use these models to perform
prediction tasks, the original input x is modified using a template into a
textual string prompt x' that has some unfilled slots, and then the language
model is used to probabilistically fill the unfilled information to obtain a
final string x, from which the final output y can be derived. This framework is
powerful and attractive for a number of reasons: it allows the language model
to be pre-trained on massive amounts of raw text, and by defining a new
prompting function the model is able to perform few-shot or even zero-shot
learning, adapting to new scenarios with few or no labeled data. In this paper
we introduce the basics of this promising paradigm, describe a unified set of
mathematical notations that can cover a wide variety of existing work, and
organize existing work along several dimensions, e.g.the choice of pre-trained
models, prompts, and tuning strategies. To make the field more accessible to
interested beginners, we not only make a systematic review of existing works
and a highly structured typology of prompt-based concepts, but also release
other resources, e.g., a website http://pretrain.nlpedia.ai/ including
constantly-updated survey, and paperlist.
","[{'version': 'v1', 'created': 'Wed, 28 Jul 2021 18:09:46 GMT'}]",cs.CL,2021-07-28 18:09:46,['language model'],True,Prompts & In-Context Learning,['cmu.edu'],False,True,False,0.0,1325.0,0.9950124688279302,0.9956550802139037,6,0.9968533668974198
5400,arXiv:2109.01652,"['Jason Wei', 'Maarten Bosma', 'Vincent Y. Zhao', 'Kelvin Guu', 'Adams Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew M. Dai', 'Quoc V. Le']",Finetuned Language Models Are Zero-Shot Learners,['cs.CL'],"  This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially improves zero-shot performance on unseen tasks.
  We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of finetuning datasets, model scale, and natural language instructions are key
to the success of instruction tuning.
","[{'version': 'v1', 'created': 'Fri, 3 Sep 2021 17:55:52 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Oct 2021 02:13:12 GMT'}, {'version': 'v3', 'created': 'Thu, 4 Nov 2021 23:05:34 GMT'}, {'version': 'v4', 'created': 'Thu, 2 Dec 2021 00:45:29 GMT'}, {'version': 'v5', 'created': 'Tue, 8 Feb 2022 20:26:45 GMT'}]",cs.CL,2021-09-03 17:55:52,"['pretrained language model', 'language model', 'GPT-3']",True,Fine-Tuning & Domain Adaptation,[],False,False,False,0.0,1001.0,0.9937655860349127,0.9946524064171123,9,0.9962240402769037
5388,arXiv:2109.01134,"['Kaiyang Zhou', 'Jingkang Yang', 'Chen Change Loy', 'Ziwei Liu']",Learning to Prompt for Vision-Language Models,"['cs.CV', 'cs.AI', 'cs.LG']","  Large pre-trained vision-language models like CLIP have shown great potential
in learning representations that are transferable across a wide range of
downstream tasks. Different from the traditional representation learning that
is based mostly on discretized labels, vision-language pre-training aligns
images and texts in a common feature space, which allows zero-shot transfer to
a downstream task via prompting, i.e., classification weights are synthesized
from natural language describing classes of interest. In this work, we show
that a major challenge for deploying such models in practice is prompt
engineering, which requires domain expertise and is extremely time-consuming --
one needs to spend a significant amount of time on words tuning since a slight
change in wording could have a huge impact on performance. Inspired by recent
advances in prompt learning research in natural language processing (NLP), we
propose Context Optimization (CoOp), a simple approach specifically for
adapting CLIP-like vision-language models for downstream image recognition.
Concretely, CoOp models a prompt's context words with learnable vectors while
the entire pre-trained parameters are kept fixed. To handle different image
recognition tasks, we provide two implementations of CoOp: unified context and
class-specific context. Through extensive experiments on 11 datasets, we
demonstrate that CoOp requires as few as one or two shots to beat hand-crafted
prompts with a decent margin and is able to gain significant improvements over
prompt engineering with more shots, e.g., with 16 shots the average gain is
around 15% (with the highest reaching over 45%). Despite being a learning-based
approach, CoOp achieves superb domain generalization performance compared with
the zero-shot model using hand-crafted prompts.
","[{'version': 'v1', 'created': 'Thu, 2 Sep 2021 17:57:31 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Sep 2021 10:18:43 GMT'}, {'version': 'v3', 'created': 'Sun, 6 Feb 2022 12:10:40 GMT'}, {'version': 'v4', 'created': 'Sat, 30 Jul 2022 14:07:52 GMT'}, {'version': 'v5', 'created': 'Fri, 12 Aug 2022 08:12:06 GMT'}, {'version': 'v6', 'created': 'Thu, 6 Oct 2022 11:36:09 GMT'}]",cs.CV,2021-09-02 17:57:31,['language model'],True,Vision-Language Models,['ntu.edu.sg'],False,True,,,607.0,0.9925187032418953,0.9923128342245989,4,0.9943360604153556
5978,arXiv:2110.08207,"['Victor Sanh', 'Albert Webson', 'Colin Raffel', 'Stephen H. Bach', 'Lintang Sutawika', 'Zaid Alyafeai', 'Antoine Chaffin', 'Arnaud Stiegler', 'Teven Le Scao', 'Arun Raja', 'Manan Dey', 'M Saiful Bari', 'Canwen Xu', 'Urmish Thakker', 'Shanya Sharma Sharma', 'Eliza Szczechla', 'Taewoon Kim', 'Gunjan Chhablani', 'Nihal Nayak', 'Debajyoti Datta', 'Jonathan Chang', 'Mike Tian-Jian Jiang', 'Han Wang', 'Matteo Manica', 'Sheng Shen', 'Zheng Xin Yong', 'Harshit Pandey', 'Rachel Bawden', 'Thomas Wang', 'Trishala Neeraj', 'Jos Rozen', 'Abheesht Sharma', 'Andrea Santilli', 'Thibault Fevry', 'Jason Alan Fries', 'Ryan Teehan', 'Tali Bers', 'Stella Biderman', 'Leo Gao', 'Thomas Wolf', 'Alexander M. Rush']",Multitask Prompted Training Enables Zero-Shot Task Generalization,"['cs.LG', 'cs.CL']","  Large language models have recently been shown to attain reasonable zero-shot
generalization on a diverse set of tasks (Brown et al., 2020). It has been
hypothesized that this is a consequence of implicit multitask learning in
language models' pretraining (Radford et al., 2019). Can zero-shot
generalization instead be directly induced by explicit multitask learning? To
test this question at scale, we develop a system for easily mapping any natural
language tasks into a human-readable prompted form. We convert a large set of
supervised datasets, each with multiple prompts with diverse wording. These
prompted datasets allow for benchmarking the ability of a model to perform
completely held-out tasks. We fine-tune a pretrained encoder-decoder model
(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a
wide variety of tasks. The model attains strong zero-shot performance on
several standard datasets, often outperforming models up to 16x its size.
Further, our approach attains strong performance on a subset of tasks from the
BIG-bench benchmark, outperforming models up to 6x its size. All trained models
are available at https://github.com/bigscience-workshop/t-zero and all prompts
are available at https://github.com/bigscience-workshop/promptsource.
","[{'version': 'v1', 'created': 'Fri, 15 Oct 2021 17:08:57 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Dec 2021 03:31:00 GMT'}, {'version': 'v3', 'created': 'Thu, 17 Mar 2022 17:53:01 GMT'}]",cs.LG,2021-10-15 17:08:57,"['language model', 'large language model']",True,Prompts & In-Context Learning,[],False,False,False,0.1785714286,816.0,0.9987293519695044,0.9933155080213903,41,0.9955947136563876
6530,arXiv:2112.11446,"['Jack W. Rae', 'Sebastian Borgeaud', 'Trevor Cai', 'Katie Millican', 'Jordan Hoffmann', 'Francis Song', 'John Aslanides', 'Sarah Henderson', 'Roman Ring', 'Susannah Young', 'Eliza Rutherford', 'Tom Hennigan', 'Jacob Menick', 'Albin Cassirer', 'Richard Powell', 'George van den Driessche', 'Lisa Anne Hendricks', 'Maribeth Rauh', 'Po-Sen Huang', 'Amelia Glaese', 'Johannes Welbl', 'Sumanth Dathathri', 'Saffron Huang', 'Jonathan Uesato', 'John Mellor', 'Irina Higgins', 'Antonia Creswell', 'Nat McAleese', 'Amy Wu', 'Erich Elsen', 'Siddhant Jayakumar', 'Elena Buchatskaya', 'David Budden', 'Esme Sutherland', 'Karen Simonyan', 'Michela Paganini', 'Laurent Sifre', 'Lena Martens', 'Xiang Lorraine Li', 'Adhiguna Kuncoro', 'Aida Nematzadeh', 'Elena Gribovskaya', 'Domenic Donato', 'Angeliki Lazaridou', 'Arthur Mensch', 'Jean-Baptiste Lespiau', 'Maria Tsimpoukelli', 'Nikolai Grigorev', 'Doug Fritz', 'Thibault Sottiaux', 'Mantas Pajarskas', 'Toby Pohlen', 'Zhitao Gong', 'Daniel Toyama', ""Cyprien de Masson d'Autume"", 'Yujia Li', 'Tayfun Terzi', 'Vladimir Mikulik', 'Igor Babuschkin', 'Aidan Clark', 'Diego de Las Casas', 'Aurelia Guy', 'Chris Jones', 'James Bradbury', 'Matthew Johnson', 'Blake Hechtman', 'Laura Weidinger', 'Iason Gabriel', 'William Isaac', 'Ed Lockhart', 'Simon Osindero', 'Laura Rimell', 'Chris Dyer', 'Oriol Vinyals', 'Kareem Ayoub', 'Jeff Stanway', 'Lorrayne Bennett', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Geoffrey Irving']","Scaling Language Models: Methods, Analysis & Insights from Training
  Gopher","['cs.CL', 'cs.AI']","  Language modelling provides a step towards intelligent communication systems
by harnessing large repositories of written human knowledge to better predict
and understand the world. In this paper, we present an analysis of
Transformer-based language model performance across a wide range of model
scales -- from models with tens of millions of parameters up to a 280 billion
parameter model called Gopher. These models are evaluated on 152 diverse tasks,
achieving state-of-the-art performance across the majority. Gains from scale
are largest in areas such as reading comprehension, fact-checking, and the
identification of toxic language, but logical and mathematical reasoning see
less benefit. We provide a holistic analysis of the training dataset and
model's behaviour, covering the intersection of model scale with bias and
toxicity. Finally we discuss the application of language models to AI safety
and the mitigation of downstream harms.
","[{'version': 'v1', 'created': 'Wed, 8 Dec 2021 19:41:47 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Jan 2022 18:39:38 GMT'}]",cs.CL,2021-12-08 19:41:47,['language model'],True,Societal Implications of LLMs,['deepmind.com'],True,False,False,0.3428571429,619.0,0.9974587039390089,0.9926470588235294,80,0.9949653870358716
6075,arXiv:2110.14168,"['Karl Cobbe', 'Vineet Kosaraju', 'Mohammad Bavarian', 'Mark Chen', 'Heewoo Jun', 'Lukasz Kaiser', 'Matthias Plappert', 'Jerry Tworek', 'Jacob Hilton', 'Reiichiro Nakano', 'Christopher Hesse', 'John Schulman']",Training Verifiers to Solve Math Word Problems,"['cs.LG', 'cs.CL']","  State-of-the-art language models can match human performance on many tasks,
but they still struggle to robustly perform multi-step mathematical reasoning.
To diagnose the failures of current models and support research, we introduce
GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math
word problems. We find that even the largest transformer models fail to achieve
high test performance, despite the conceptual simplicity of this problem
distribution. To increase performance, we propose training verifiers to judge
the correctness of model completions. At test time, we generate many candidate
solutions and select the one ranked highest by the verifier. We demonstrate
that verification significantly improves performance on GSM8K, and we provide
strong empirical evidence that verification scales more effectively with
increased data than a finetuning baseline.
","[{'version': 'v1', 'created': 'Wed, 27 Oct 2021 04:49:45 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Nov 2021 00:23:45 GMT'}]",cs.LG,2021-10-27 04:49:45,['language model'],True,"LLMs, Reasoning, Chain-of-Thought",['openai.com'],True,False,False,0.0,452.0,0.9961880559085133,0.9903074866310161,12,0.9930774071743235
6260,arXiv:2111.11432,"['Lu Yuan', 'Dongdong Chen', 'Yi-Ling Chen', 'Noel Codella', 'Xiyang Dai', 'Jianfeng Gao', 'Houdong Hu', 'Xuedong Huang', 'Boxin Li', 'Chunyuan Li', 'Ce Liu', 'Mengchen Liu', 'Zicheng Liu', 'Yumao Lu', 'Yu Shi', 'Lijuan Wang', 'Jianfeng Wang', 'Bin Xiao', 'Zhen Xiao', 'Jianwei Yang', 'Michael Zeng', 'Luowei Zhou', 'Pengchuan Zhang']",Florence: A New Foundation Model for Computer Vision,"['cs.CV', 'cs.AI', 'cs.LG']","  Automated visual understanding of our diverse and open world demands computer
vision models to generalize well with minimal customization for specific tasks,
similar to human vision. Computer vision foundation models, which are trained
on diverse, large-scale dataset and can be adapted to a wide range of
downstream tasks, are critical for this mission to solve real-world computer
vision applications. While existing vision foundation models such as CLIP,
ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual
representations to a cross-modal shared representation, we introduce a new
computer vision foundation model, Florence, to expand the representations from
coarse (scene) to fine (object), from static (images) to dynamic (videos), and
from RGB to multiple modalities (caption, depth). By incorporating universal
visual-language representations from Web-scale image-text data, our Florence
model can be easily adapted for various computer vision tasks, such as
classification, retrieval, object detection, VQA, image caption, video
retrieval and action recognition. Moreover, Florence demonstrates outstanding
performance in many types of transfer learning: fully sampled fine-tuning,
linear probing, few-shot transfer and zero-shot transfer for novel images and
objects. All of these properties are critical for our vision foundation model
to serve general purpose vision tasks. Florence achieves new state-of-the-art
results in majority of 44 representative benchmarks, e.g., ImageNet-1K
zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of
97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.
","[{'version': 'v1', 'created': 'Mon, 22 Nov 2021 18:59:55 GMT'}]",cs.CV,2021-11-22 18:59:55,['foundation model'],True,Vision-Language Models,['microsoft.com'],True,False,False,0.1111111111,418.0,0.9949174078780177,0.9896390374331551,23,0.9918187539332913
6404,arXiv:2112.04426,"['Sebastian Borgeaud', 'Arthur Mensch', 'Jordan Hoffmann', 'Trevor Cai', 'Eliza Rutherford', 'Katie Millican', 'George van den Driessche', 'Jean-Baptiste Lespiau', 'Bogdan Damoc', 'Aidan Clark', 'Diego de Las Casas', 'Aurelia Guy', 'Jacob Menick', 'Roman Ring', 'Tom Hennigan', 'Saffron Huang', 'Loren Maggiore', 'Chris Jones', 'Albin Cassirer', 'Andy Brock', 'Michela Paganini', 'Geoffrey Irving', 'Oriol Vinyals', 'Simon Osindero', 'Karen Simonyan', 'Jack W. Rae', 'Erich Elsen', 'Laurent Sifre']",Improving language models by retrieving from trillions of tokens,"['cs.CL', 'cs.LG']","  We enhance auto-regressive language models by conditioning on document chunks
retrieved from a large corpus, based on local similarity with preceding tokens.
With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)
obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite
using 25$\times$ fewer parameters. After fine-tuning, RETRO performance
translates to downstream knowledge-intensive tasks such as question answering.
RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked
cross-attention mechanism to predict tokens based on an order of magnitude more
data than what is typically consumed during training. We typically train RETRO
from scratch, yet can also rapidly RETROfit pre-trained transformers with
retrieval and still achieve good performance. Our work opens up new avenues for
improving language models through explicit memory at unprecedented scale.
","[{'version': 'v1', 'created': 'Wed, 8 Dec 2021 17:32:34 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Jan 2022 09:14:18 GMT'}, {'version': 'v3', 'created': 'Mon, 7 Feb 2022 21:07:59 GMT'}]",cs.CL,2021-12-08 17:32:34,"['language model', 'GPT-3']",True,"Search, Ranking, Retrieval",['deepmind.com'],True,False,False,0.24,338.0,0.9936467598475223,0.9881350267379679,28,0.9905601006922593
6992,arXiv:2203.02155,"['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll L. Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Ray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']",Training language models to follow instructions with human feedback,"['cs.CL', 'cs.AI', 'cs.LG']","  Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.
","[{'version': 'v1', 'created': 'Fri, 4 Mar 2022 07:04:42 GMT'}]",cs.CL,2022-03-04 07:04:42,"['language model', 'large language model', 'GPT-3']",True,Prompts & In-Context Learning,['openai.com'],True,False,False,0.2777777778,2346.0,0.9972183588317107,0.9979859013091642,20,0.9982944855031268
6741,arXiv:2201.11903,"['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'Brian Ichter', 'Fei Xia', 'Ed Chi', 'Quoc Le', 'Denny Zhou']",Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"['cs.CL', 'cs.AI']","  We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.
","[{'version': 'v1', 'created': 'Fri, 28 Jan 2022 02:33:07 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Apr 2022 03:51:50 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Jun 2022 00:10:30 GMT'}, {'version': 'v4', 'created': 'Mon, 13 Jun 2022 21:44:34 GMT'}, {'version': 'v5', 'created': 'Mon, 10 Oct 2022 20:21:17 GMT'}, {'version': 'v6', 'created': 'Tue, 10 Jan 2023 23:07:57 GMT'}]",cs.CL,2022-01-28 02:33:07,"['language model', 'large language model', 'GPT-3']",True,"LLMs, Reasoning, Chain-of-Thought",['google.com'],True,False,False,0.0,1606.0,0.9958275382475661,0.9972306143001007,9,0.9965889710062535
6682,arXiv:2201.08239,"['Romal Thoppilan', 'Daniel De Freitas', 'Jamie Hall', 'Noam Shazeer', 'Apoorv Kulshreshtha', 'Heng-Tze Cheng', 'Alicia Jin', 'Taylor Bos', 'Leslie Baker', 'Yu Du', 'YaGuang Li', 'Hongrae Lee', 'Huaixiu Steven Zheng', 'Amin Ghafouri', 'Marcelo Menegali', 'Yanping Huang', 'Maxim Krikun', 'Dmitry Lepikhin', 'James Qin', 'Dehao Chen', 'Yuanzhong Xu', 'Zhifeng Chen', 'Adam Roberts', 'Maarten Bosma', 'Vincent Zhao', 'Yanqi Zhou', 'Chung-Ching Chang', 'Igor Krivokon', 'Will Rusch', 'Marc Pickett', 'Pranesh Srinivasan', 'Laichee Man', 'Kathleen Meier-Hellstern', 'Meredith Ringel Morris', 'Tulsee Doshi', 'Renelito Delos Santos', 'Toju Duke', 'Johnny Soraker', 'Ben Zevenbergen', 'Vinodkumar Prabhakaran', 'Mark Diaz', 'Ben Hutchinson', 'Kristen Olson', 'Alejandra Molina', 'Erin Hoffman-John', 'Josh Lee', 'Lora Aroyo', 'Ravi Rajakumar', 'Alena Butryna', 'Matthew Lamm', 'Viktoriya Kuzmina', 'Joe Fenton', 'Aaron Cohen', 'Rachel Bernstein', 'Ray Kurzweil', 'Blaise Aguera-Arcas', 'Claire Cui', 'Marian Croak', 'Ed Chi', 'Quoc Le']",LaMDA: Language Models for Dialog Applications,"['cs.CL', 'cs.AI']","  We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency.
","[{'version': 'v1', 'created': 'Thu, 20 Jan 2022 15:44:37 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Jan 2022 19:41:03 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Feb 2022 16:30:11 GMT'}]",cs.CL,2022-01-20 15:44:37,['language model'],True,Societal Implications of LLMs,[],False,False,False,0.25,705.0,0.9944367176634215,0.9964753272910373,60,0.9948834565093804
7248,arXiv:2203.15556,"['Jordan Hoffmann', 'Sebastian Borgeaud', 'Arthur Mensch', 'Elena Buchatskaya', 'Trevor Cai', 'Eliza Rutherford', 'Diego de Las Casas', 'Lisa Anne Hendricks', 'Johannes Welbl', 'Aidan Clark', 'Tom Hennigan', 'Eric Noland', 'Katie Millican', 'George van den Driessche', 'Bogdan Damoc', 'Aurelia Guy', 'Simon Osindero', 'Karen Simonyan', 'Erich Elsen', 'Jack W. Rae', 'Oriol Vinyals', 'Laurent Sifre']",Training Compute-Optimal Large Language Models,"['cs.CL', 'cs.LG']","  We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.
","[{'version': 'v1', 'created': 'Tue, 29 Mar 2022 13:38:03 GMT'}]",cs.CL,2022-03-29 13:38:03,"['language model', 'large language model', 'GPT-3']",True,Efficiency & Performance,['deepmind.com'],True,False,False,0.2857142857,629.0,0.9930458970792768,0.9957200402819738,22,0.9931779420125071
7169,arXiv:2203.11171,"['Xuezhi Wang', 'Jason Wei', 'Dale Schuurmans', 'Quoc Le', 'Ed Chi', 'Sharan Narang', 'Aakanksha Chowdhery', 'Denny Zhou']",Self-Consistency Improves Chain of Thought Reasoning in Language Models,"['cs.CL', 'cs.AI']","  Chain-of-thought prompting combined with pre-trained large language models
has achieved encouraging results on complex reasoning tasks. In this paper, we
propose a new decoding strategy, self-consistency, to replace the naive greedy
decoding used in chain-of-thought prompting. It first samples a diverse set of
reasoning paths instead of only taking the greedy one, and then selects the
most consistent answer by marginalizing out the sampled reasoning paths.
Self-consistency leverages the intuition that a complex reasoning problem
typically admits multiple different ways of thinking leading to its unique
correct answer. Our extensive empirical evaluation shows that self-consistency
boosts the performance of chain-of-thought prompting with a striking margin on
a range of popular arithmetic and commonsense reasoning benchmarks, including
GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and
ARC-challenge (+3.9%).
","[{'version': 'v1', 'created': 'Mon, 21 Mar 2022 17:48:52 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Apr 2022 04:40:11 GMT'}, {'version': 'v3', 'created': 'Tue, 4 Oct 2022 16:46:29 GMT'}, {'version': 'v4', 'created': 'Tue, 7 Mar 2023 17:57:37 GMT'}]",cs.CL,2022-03-21 17:48:52,"['language model', 'large language model']",True,"LLMs, Reasoning, Chain-of-Thought",['google.com'],True,False,False,0.2,489.0,0.9916550764951322,0.9947129909365559,8,0.992040932347925
7321,arXiv:2204.02311,"['Aakanksha Chowdhery', 'Sharan Narang', 'Jacob Devlin', 'Maarten Bosma', 'Gaurav Mishra', 'Adam Roberts', 'Paul Barham', 'Hyung Won Chung', 'Charles Sutton', 'Sebastian Gehrmann', 'Parker Schuh', 'Kensen Shi', 'Sasha Tsvyashchenko', 'Joshua Maynez', 'Abhishek Rao', 'Parker Barnes', 'Yi Tay', 'Noam Shazeer', 'Vinodkumar Prabhakaran', 'Emily Reif', 'Nan Du', 'Ben Hutchinson', 'Reiner Pope', 'James Bradbury', 'Jacob Austin', 'Michael Isard', 'Guy Gur-Ari', 'Pengcheng Yin', 'Toju Duke', 'Anselm Levskaya', 'Sanjay Ghemawat', 'Sunipa Dev', 'Henryk Michalewski', 'Xavier Garcia', 'Vedant Misra', 'Kevin Robinson', 'Liam Fedus', 'Denny Zhou', 'Daphne Ippolito', 'David Luan', 'Hyeontaek Lim', 'Barret Zoph', 'Alexander Spiridonov', 'Ryan Sepassi', 'David Dohan', 'Shivani Agrawal', 'Mark Omernick', 'Andrew M. Dai', 'Thanumalayan Sankaranarayana Pillai', 'Marie Pellat', 'Aitor Lewkowycz', 'Erica Moreira', 'Rewon Child', 'Oleksandr Polozov', 'Katherine Lee', 'Zongwei Zhou', 'Xuezhi Wang', 'Brennan Saeta', 'Mark Diaz', 'Orhan Firat', 'Michele Catasta', 'Jason Wei', 'Kathy Meier-Hellstern', 'Douglas Eck', 'Jeff Dean', 'Slav Petrov', 'Noah Fiedel']",PaLM: Scaling Language Modeling with Pathways,['cs.CL'],"  Large language models have been shown to achieve remarkable performance
across a variety of natural language tasks using few-shot learning, which
drastically reduces the number of task-specific training examples needed to
adapt the model to a particular application. To further our understanding of
the impact of scale on few-shot learning, we trained a 540-billion parameter,
densely activated, Transformer language model, which we call Pathways Language
Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML
system which enables highly efficient training across multiple TPU Pods. We
demonstrate continued benefits of scaling by achieving state-of-the-art
few-shot learning results on hundreds of language understanding and generation
benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough
performance, outperforming the finetuned state-of-the-art on a suite of
multi-step reasoning tasks, and outperforming average human performance on the
recently released BIG-bench benchmark. A significant number of BIG-bench tasks
showed discontinuous improvements from model scale, meaning that performance
steeply increased as we scaled to our largest model. PaLM also has strong
capabilities in multilingual tasks and source code generation, which we
demonstrate on a wide array of benchmarks. We additionally provide a
comprehensive analysis on bias and toxicity, and study the extent of training
data memorization with respect to model scale. Finally, we discuss the ethical
considerations related to large language models and discuss potential
mitigation strategies.
","[{'version': 'v1', 'created': 'Tue, 5 Apr 2022 16:11:45 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Apr 2022 16:38:01 GMT'}, {'version': 'v3', 'created': 'Tue, 19 Apr 2022 05:28:38 GMT'}, {'version': 'v4', 'created': 'Thu, 29 Sep 2022 13:22:22 GMT'}, {'version': 'v5', 'created': 'Wed, 5 Oct 2022 06:02:24 GMT'}]",cs.CL,2022-04-05 16:11:45,"['language model', 'large language model', 'PaLM']",True,Pretrained LMs & Text Classification,['google.com'],True,False,False,0.1698113208,1836.0,0.9990384615384615,0.9977341389728097,67,0.9977259806708357
7900,arXiv:2205.11487,"['Chitwan Saharia', 'William Chan', 'Saurabh Saxena', 'Lala Li', 'Jay Whang', 'Emily Denton', 'Seyed Kamyar Seyed Ghasemipour', 'Burcu Karagol Ayan', 'S. Sara Mahdavi', 'Rapha Gontijo Lopes', 'Tim Salimans', 'Jonathan Ho', 'David J Fleet', 'Mohammad Norouzi']","Photorealistic Text-to-Image Diffusion Models with Deep Language
  Understanding","['cs.CV', 'cs.LG']","  We present Imagen, a text-to-image diffusion model with an unprecedented
degree of photorealism and a deep level of language understanding. Imagen
builds on the power of large transformer language models in understanding text
and hinges on the strength of diffusion models in high-fidelity image
generation. Our key discovery is that generic large language models (e.g. T5),
pretrained on text-only corpora, are surprisingly effective at encoding text
for image synthesis: increasing the size of the language model in Imagen boosts
both sample fidelity and image-text alignment much more than increasing the
size of the image diffusion model. Imagen achieves a new state-of-the-art FID
score of 7.27 on the COCO dataset, without ever training on COCO, and human
raters find Imagen samples to be on par with the COCO data itself in image-text
alignment. To assess text-to-image models in greater depth, we introduce
DrawBench, a comprehensive and challenging benchmark for text-to-image models.
With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,
Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen
over other models in side-by-side comparisons, both in terms of sample quality
and image-text alignment. See https://imagen.research.google/ for an overview
of the results.
","[{'version': 'v1', 'created': 'Mon, 23 May 2022 17:42:53 GMT'}]",cs.CV,2022-05-23 17:42:53,"['language model', 'large language model']",True,Video & Multimodal Models,['google.com'],True,False,False,0.2727272727,1633.0,0.9980769230769231,0.9974823766364552,14,0.9971574758385446
7652,arXiv:2205.01068,"['Susan Zhang', 'Stephen Roller', 'Naman Goyal', 'Mikel Artetxe', 'Moya Chen', 'Shuohui Chen', 'Christopher Dewan', 'Mona Diab', 'Xian Li', 'Xi Victoria Lin', 'Todor Mihaylov', 'Myle Ott', 'Sam Shleifer', 'Kurt Shuster', 'Daniel Simig', 'Punit Singh Koura', 'Anjali Sridhar', 'Tianlu Wang', 'Luke Zettlemoyer']",OPT: Open Pre-trained Transformer Language Models,"['cs.CL', 'cs.LG']","  Large language models, which are often trained for hundreds of thousands of
compute days, have shown remarkable capabilities for zero- and few-shot
learning. Given their computational cost, these models are difficult to
replicate without significant capital. For the few that are available through
APIs, no access is granted to the full model weights, making them difficult to
study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only
pre-trained transformers ranging from 125M to 175B parameters, which we aim to
fully and responsibly share with interested researchers. We show that OPT-175B
is comparable to GPT-3, while requiring only 1/7th the carbon footprint to
develop. We are also releasing our logbook detailing the infrastructure
challenges we faced, along with code for experimenting with all of the released
models.
","[{'version': 'v1', 'created': 'Mon, 2 May 2022 17:49:50 GMT'}, {'version': 'v2', 'created': 'Tue, 3 May 2022 15:04:06 GMT'}, {'version': 'v3', 'created': 'Thu, 5 May 2022 11:44:30 GMT'}, {'version': 'v4', 'created': 'Tue, 21 Jun 2022 17:04:40 GMT'}]",cs.CL,2022-05-02 17:49:50,"['language model', 'large language model', 'GPT-3']",True,Efficiency & Performance,['fb.com'],True,False,False,0.2857142857,1042.0,0.9971153846153846,0.9969788519637462,19,0.9960204661739624
7619,arXiv:2204.14198,"['Jean-Baptiste Alayrac', 'Jeff Donahue', 'Pauline Luc', 'Antoine Miech', 'Iain Barr', 'Yana Hasson', 'Karel Lenc', 'Arthur Mensch', 'Katie Millican', 'Malcolm Reynolds', 'Roman Ring', 'Eliza Rutherford', 'Serkan Cabi', 'Tengda Han', 'Zhitao Gong', 'Sina Samangooei', 'Marianne Monteiro', 'Jacob Menick', 'Sebastian Borgeaud', 'Andrew Brock', 'Aida Nematzadeh', 'Sahand Sharifzadeh', 'Mikolaj Binkowski', 'Ricardo Barreira', 'Oriol Vinyals', 'Andrew Zisserman', 'Karen Simonyan']",Flamingo: a Visual Language Model for Few-Shot Learning,"['cs.CV', 'cs.AI', 'cs.LG']","  Building models that can be rapidly adapted to novel tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. We propose key architectural innovations to: (i)
bridge powerful pretrained vision-only and language-only models, (ii) handle
sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
our models, exploring and measuring their ability to rapidly adapt to a variety
of image and video tasks. These include open-ended tasks such as visual
question-answering, where the model is prompted with a question which it has to
answer; captioning tasks, which evaluate the ability to describe a scene or an
event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve
a new state of the art with few-shot learning, simply by prompting the model
with task-specific examples. On numerous benchmarks, Flamingo outperforms
models fine-tuned on thousands of times more task-specific data.
","[{'version': 'v1', 'created': 'Fri, 29 Apr 2022 16:29:01 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Nov 2022 23:07:37 GMT'}]",cs.CV,2022-04-29 16:29:01,['language model'],True,Video & Multimodal Models,['deepmind.com'],True,False,False,0.2916666667,815.0,0.9961538461538462,0.9967270896273918,27,0.9954519613416714
7918,arXiv:2205.11916,"['Takeshi Kojima', 'Shixiang Shane Gu', 'Machel Reid', 'Yutaka Matsuo', 'Yusuke Iwasawa']",Large Language Models are Zero-Shot Reasoners,"['cs.CL', 'cs.AI', 'cs.LG']","  Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding ""Let's think step by step""
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
large InstructGPT model (text-davinci-002), as well as similar magnitudes of
improvements with another off-the-shelf large model, 540B parameter PaLM. The
versatility of this single prompt across very diverse reasoning tasks hints at
untapped and understudied fundamental zero-shot capabilities of LLMs,
suggesting high-level, multi-task broad cognitive capabilities may be extracted
by simple prompting. We hope our work not only serves as the minimal strongest
zero-shot baseline for the challenging reasoning benchmarks, but also
highlights the importance of carefully exploring and analyzing the enormous
zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or
few-shot exemplars.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 09:22:26 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Jun 2022 09:27:35 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Oct 2022 07:12:50 GMT'}, {'version': 'v4', 'created': 'Sun, 29 Jan 2023 05:14:17 GMT'}]",cs.CL,2022-05-24 09:22:26,"['language model', 'large language model', 'PaLM']",True,"LLMs, Reasoning, Chain-of-Thought",['u-tokyo.ac'],False,False,False,0.0,684.0,0.9951923076923077,0.9962235649546828,5,0.9943149516770893
8875,arXiv:2209.06794,"['Xi Chen', 'Xiao Wang', 'Soravit Changpinyo', 'AJ Piergiovanni', 'Piotr Padlewski', 'Daniel Salz', 'Sebastian Goodman', 'Adam Grycner', 'Basil Mustafa', 'Lucas Beyer', 'Alexander Kolesnikov', 'Joan Puigcerver', 'Nan Ding', 'Keran Rong', 'Hassan Akbari', 'Gaurav Mishra', 'Linting Xue', 'Ashish Thapliyal', 'James Bradbury', 'Weicheng Kuo', 'Mojtaba Seyedhosseini', 'Chao Jia', 'Burcu Karagol Ayan', 'Carlos Riquelme', 'Andreas Steiner', 'Anelia Angelova', 'Xiaohua Zhai', 'Neil Houlsby', 'Radu Soricut']",PaLI: A Jointly-Scaled Multilingual Language-Image Model,"['cs.CV', 'cs.CL']","  Effective scaling and a flexible task interface enable large language models
to excel at many tasks. We present PaLI (Pathways Language and Image model), a
model that extends this approach to the joint modeling of language and vision.
PaLI generates text based on visual and textual inputs, and with this interface
performs many vision, language, and multimodal tasks, in many languages. To
train PaLI, we make use of large pre-trained encoder-decoder language models
and Vision Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the
benefits from even larger-capacity vision models. To train PaLI, we create a
large multilingual mix of pretraining tasks, based on a new image-text training
set containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.
","[{'version': 'v1', 'created': 'Wed, 14 Sep 2022 17:24:07 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Sep 2022 17:44:29 GMT'}, {'version': 'v3', 'created': 'Sun, 28 May 2023 23:46:10 GMT'}, {'version': 'v4', 'created': 'Mon, 5 Jun 2023 17:55:12 GMT'}]",cs.CV,2022-09-14 17:24:07,"['language model', 'large language model']",True,Vision-Language Models,['google.com'],True,False,False,0.1,181.0,1.0,0.9871601208459214,29,0.9936737460460913
8584,arXiv:2208.03299,"['Gautier Izacard', 'Patrick Lewis', 'Maria Lomeli', 'Lucas Hosseini', 'Fabio Petroni', 'Timo Schick', 'Jane Dwivedi-Yu', 'Armand Joulin', 'Sebastian Riedel', 'Edouard Grave']",Atlas: Few-shot Learning with Retrieval Augmented Language Models,['cs.CL'],"  Large language models have shown impressive few-shot results on a wide range
of tasks. However, when knowledge is key for such results, as is the case for
tasks such as question answering and fact checking, massive parameter counts to
store knowledge seem to be needed. Retrieval augmented models are known to
excel at knowledge intensive tasks without the need for as many parameters, but
it is unclear whether they work in few-shot settings. In this work we present
Atlas, a carefully designed and pre-trained retrieval augmented language model
able to learn knowledge intensive tasks with very few training examples. We
perform evaluations on a wide range of tasks, including MMLU, KILT and
NaturalQuestions, and study the impact of the content of the document index,
showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy
on Natural Questions using only 64 examples, outperforming a 540B parameters
model by 3% despite having 50x fewer parameters.
","[{'version': 'v1', 'created': 'Fri, 5 Aug 2022 17:39:22 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Aug 2022 15:01:33 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Nov 2022 16:38:18 GMT'}]",cs.CL,2022-08-05 17:39:22,"['language model', 'large language model']",True,Knowledge Graphs and Commonsense,['fb.com'],True,False,False,0.2,160.0,0.9986244841815681,0.9861530715005036,10,0.9927699954812472
9020,arXiv:2209.14375,"['Amelia Glaese', 'Nat McAleese', 'Maja Trębacz', 'John Aslanides', 'Vlad Firoiu', 'Timo Ewalds', 'Maribeth Rauh', 'Laura Weidinger', 'Martin Chadwick', 'Phoebe Thacker', 'Lucy Campbell-Gillingham', 'Jonathan Uesato', 'Po-Sen Huang', 'Ramona Comanescu', 'Fan Yang', 'Abigail See', 'Sumanth Dathathri', 'Rory Greig', 'Charlie Chen', 'Doug Fritz', 'Jaume Sanchez Elias', 'Richard Green', 'Soňa Mokrá', 'Nicholas Fernando', 'Boxi Wu', 'Rachel Foley', 'Susannah Young', 'Iason Gabriel', 'William Isaac', 'John Mellor', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Lisa Anne Hendricks', 'Geoffrey Irving']",Improving alignment of dialogue agents via targeted human judgements,"['cs.LG', 'cs.CL']","  We present Sparrow, an information-seeking dialogue agent trained to be more
helpful, correct, and harmless compared to prompted language model baselines.
We use reinforcement learning from human feedback to train our models with two
new additions to help human raters judge agent behaviour. First, to make our
agent more helpful and harmless, we break down the requirements for good
dialogue into natural language rules the agent should follow, and ask raters
about each rule separately. We demonstrate that this breakdown enables us to
collect more targeted human judgements of agent behaviour and allows for more
efficient rule-conditional reward models. Second, our agent provides evidence
from sources supporting factual claims when collecting preference judgements
over model statements. For factual questions, evidence provided by Sparrow
supports the sampled response 78% of the time. Sparrow is preferred more often
than baselines while being more resilient to adversarial probing by humans,
violating our rules only 8% of the time when probed. Finally, we conduct
extensive analyses showing that though our model learns to follow our rules it
can exhibit distributional biases.
","[{'version': 'v1', 'created': 'Wed, 28 Sep 2022 19:04:43 GMT'}]",cs.LG,2022-09-28 19:04:43,['language model'],True,Dialogue & Conversational AI,['deepmind.com'],True,False,False,0.4444444444,151.0,0.9972489683631361,0.9848942598187311,34,0.9918662449164031
8633,arXiv:2208.07339,"['Tim Dettmers', 'Mike Lewis', 'Younes Belkada', 'Luke Zettlemoyer']",LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,"['cs.LG', 'cs.AI']","  Large language models have been widely adopted but require significant GPU
memory for inference. We develop a procedure for Int8 matrix multiplication for
feed-forward and attention projection layers in transformers, which cut the
memory needed for inference by half while retaining full precision performance.
With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted
to Int8, and used immediately without performance degradation. This is made
possible by understanding and working around properties of highly systematic
emergent features in transformer language models that dominate attention and
transformer predictive performance. To cope with these features, we develop a
two-part quantization procedure, LLM.int8(). We first use vector-wise
quantization with separate normalization constants for each inner product in
the matrix multiplication, to quantize most of the features. However, for the
emergent outliers, we also include a new mixed-precision decomposition scheme,
which isolates the outlier feature dimensions into a 16-bit matrix
multiplication while still more than 99.9% of values are multiplied in 8-bit.
Using LLM.int8(), we show empirically it is possible to perform inference in
LLMs with up to 175B parameters without any performance degradation. This
result makes such models much more accessible, for example making it possible
to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our
software.
","[{'version': 'v1', 'created': 'Mon, 15 Aug 2022 17:08:50 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Nov 2022 18:14:31 GMT'}]",cs.LG,2022-08-15 17:08:50,"['language model', 'large language model']",True,Efficiency & Performance,[],False,False,False,0.0,141.0,0.9958734525447043,0.9841389728096677,4,0.9909624943515589
8410,arXiv:2207.05221,"['Saurav Kadavath', 'Tom Conerly', 'Amanda Askell', 'Tom Henighan', 'Dawn Drain', 'Ethan Perez', 'Nicholas Schiefer', 'Zac Hatfield-Dodds', 'Nova DasSarma', 'Eli Tran-Johnson', 'Scott Johnston', 'Sheer El-Showk', 'Andy Jones', 'Nelson Elhage', 'Tristan Hume', 'Anna Chen', 'Yuntao Bai', 'Sam Bowman', 'Stanislav Fort', 'Deep Ganguli', 'Danny Hernandez', 'Josh Jacobson', 'Jackson Kernion', 'Shauna Kravec', 'Liane Lovitt', 'Kamal Ndousse', 'Catherine Olsson', 'Sam Ringer', 'Dario Amodei', 'Tom Brown', 'Jack Clark', 'Nicholas Joseph', 'Ben Mann', 'Sam McCandlish', 'Chris Olah', 'Jared Kaplan']",Language Models (Mostly) Know What They Know,"['cs.CL', 'cs.AI', 'cs.LG']","  We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability ""P(True)"" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict ""P(IK)"", the probability
that ""I know"" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and in the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing.
","[{'version': 'v1', 'created': 'Mon, 11 Jul 2022 22:59:39 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Jul 2022 18:36:41 GMT'}, {'version': 'v3', 'created': 'Sat, 16 Jul 2022 15:24:01 GMT'}, {'version': 'v4', 'created': 'Mon, 21 Nov 2022 16:38:35 GMT'}]",cs.CL,2022-07-11 22:59:39,['language model'],True,Interpretability & Reasoning,['anthropic.com'],True,False,False,0.2258064516,125.0,0.9944979367262724,0.9828801611278952,36,0.9896068685042928
9422,arXiv:2210.11416,"['Hyung Won Chung', 'Le Hou', 'Shayne Longpre', 'Barret Zoph', 'Yi Tay', 'William Fedus', 'Yunxuan Li', 'Xuezhi Wang', 'Mostafa Dehghani', 'Siddhartha Brahma', 'Albert Webson', 'Shixiang Shane Gu', 'Zhuyun Dai', 'Mirac Suzgun', 'Xinyun Chen', 'Aakanksha Chowdhery', 'Alex Castro-Ros', 'Marie Pellat', 'Kevin Robinson', 'Dasha Valter', 'Sharan Narang', 'Gaurav Mishra', 'Adams Yu', 'Vincent Zhao', 'Yanping Huang', 'Andrew Dai', 'Hongkun Yu', 'Slav Petrov', 'Ed H. Chi', 'Jeff Dean', 'Jacob Devlin', 'Adam Roberts', 'Denny Zhou', 'Quoc V. Le', 'Jason Wei']",Scaling Instruction-Finetuned Language Models,"['cs.LG', 'cs.CL']","  Finetuning language models on a collection of datasets phrased as
instructions has been shown to improve model performance and generalization to
unseen tasks. In this paper we explore instruction finetuning with a particular
focus on (1) scaling the number of tasks, (2) scaling the model size, and (3)
finetuning on chain-of-thought data. We find that instruction finetuning with
the above aspects dramatically improves performance on a variety of model
classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and
evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For
instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM
540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves
state-of-the-art performance on several benchmarks, such as 75.2% on five-shot
MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong
few-shot performance even compared to much larger models, such as PaLM 62B.
Overall, instruction finetuning is a general method for improving the
performance and usability of pretrained language models.
","[{'version': 'v1', 'created': 'Thu, 20 Oct 2022 16:58:32 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Oct 2022 17:46:04 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Nov 2022 09:44:42 GMT'}, {'version': 'v4', 'created': 'Wed, 23 Nov 2022 02:11:56 GMT'}, {'version': 'v5', 'created': 'Tue, 6 Dec 2022 21:39:48 GMT'}]",cs.LG,2022-10-20 16:58:32,"['pretrained language model', 'language model', 'PaLM']",True,Fine-Tuning & Domain Adaptation,[],False,False,False,0.18518518520000002,580.0,0.9966352624495289,0.9954682779456193,35,0.9977406235878897
9789,arXiv:2211.05100,"['BigScience Workshop', ':', 'Teven Le Scao', 'Angela Fan', 'Christopher Akiki', 'Ellie Pavlick', 'Suzana Ilić', 'Daniel Hesslow', 'Roman Castagné', 'Alexandra Sasha Luccioni', 'François Yvon', 'Matthias Gallé', 'Jonathan Tow', 'Alexander M. Rush', 'Stella Biderman', 'Albert Webson', 'Pawan Sasanka Ammanamanchi', 'Thomas Wang', 'Benoît Sagot', 'Niklas Muennighoff', 'Albert Villanova del Moral', 'Olatunji Ruwase', 'Rachel Bawden', 'Stas Bekman', 'Angelina McMillan-Major', 'Iz Beltagy', 'Huu Nguyen', 'Lucile Saulnier', 'Samson Tan', 'Pedro Ortiz Suarez', 'Victor Sanh', 'Hugo Laurençon', 'Yacine Jernite', 'Julien Launay', 'Margaret Mitchell', 'Colin Raffel', 'Aaron Gokaslan', 'Adi Simhi', 'Aitor Soroa', 'Alham Fikri Aji', 'Amit Alfassy', 'Anna Rogers', 'Ariel Kreisberg Nitzav', 'Canwen Xu', 'Chenghao Mou', 'Chris Emezue', 'Christopher Klamm', 'Colin Leong', 'Daniel van Strien', 'David Ifeoluwa Adelani', 'Dragomir Radev', 'Eduardo González Ponferrada', 'Efrat Levkovizh', 'Ethan Kim', 'Eyal Bar Natan', 'Francesco De Toni', 'Gérard Dupont', 'Germán Kruszewski', 'Giada Pistilli', 'Hady Elsahar', 'Hamza Benyamina', 'Hieu Tran', 'Ian Yu', 'Idris Abdulmumin', 'Isaac Johnson', 'Itziar Gonzalez-Dios', 'Javier de la Rosa', 'Jenny Chim', 'Jesse Dodge', 'Jian Zhu', 'Jonathan Chang', 'Jörg Frohberg', 'Joseph Tobing', 'Joydeep Bhattacharjee', 'Khalid Almubarak', 'Kimbo Chen', 'Kyle Lo', 'Leandro Von Werra', 'Leon Weber', 'Long Phan', 'Loubna Ben allal', 'Ludovic Tanguy', 'Manan Dey', 'Manuel Romero Muñoz', 'Maraim Masoud', 'María Grandury', 'Mario Šaško', 'Max Huang', 'Maximin Coavoux', 'Mayank Singh', 'Mike Tian-Jian Jiang', 'Minh Chien Vu', 'Mohammad A. Jauhar', 'Mustafa Ghaleb', 'Nishant Subramani', 'Nora Kassner', 'Nurulaqilla Khamis', 'Olivier Nguyen', 'Omar Espejel', 'Ona de Gibert', 'Paulo Villegas', 'Peter Henderson', 'Pierre Colombo', 'Priscilla Amuok', 'Quentin Lhoest', 'Rheza Harliman', 'Rishi Bommasani', 'Roberto Luis López', 'Rui Ribeiro', 'Salomey Osei', 'Sampo Pyysalo', 'Sebastian Nagel', 'Shamik Bose', 'Shamsuddeen Hassan Muhammad', 'Shanya Sharma', 'Shayne Longpre', 'Somaieh Nikpoor', 'Stanislav Silberberg', 'Suhas Pai', 'Sydney Zink', 'Tiago Timponi Torrent', 'Timo Schick', 'Tristan Thrush', 'Valentin Danchev', 'Vassilina Nikoulina', 'Veronika Laippala', 'Violette Lepercq', 'Vrinda Prabhu', 'Zaid Alyafeai', 'Zeerak Talat', 'Arun Raja', 'Benjamin Heinzerling', 'Chenglei Si', 'Davut Emre Taşar', 'Elizabeth Salesky', 'Sabrina J. Mielke', 'Wilson Y. Lee', 'Abheesht Sharma', 'Andrea Santilli', 'Antoine Chaffin', 'Arnaud Stiegler', 'Debajyoti Datta', 'Eliza Szczechla', 'Gunjan Chhablani', 'Han Wang', 'Harshit Pandey', 'Hendrik Strobelt', 'Jason Alan Fries', 'Jos Rozen', 'Leo Gao', 'Lintang Sutawika', 'M Saiful Bari', 'Maged S. Al-shaibani', 'Matteo Manica', 'Nihal Nayak', 'Ryan Teehan', 'Samuel Albanie', 'Sheng Shen', 'Srulik Ben-David', 'Stephen H. Bach', 'Taewoon Kim', 'Tali Bers', 'Thibault Fevry', 'Trishala Neeraj', 'Urmish Thakker', 'Vikas Raunak', 'Xiangru Tang', 'Zheng-Xin Yong', 'Zhiqing Sun', 'Shaked Brody', 'Yallow Uri', 'Hadar Tojarieh', 'Adam Roberts', 'Hyung Won Chung', 'Jaesung Tae', 'Jason Phang', 'Ofir Press', 'Conglong Li', 'Deepak Narayanan', 'Hatim Bourfoune', 'Jared Casper', 'Jeff Rasley', 'Max Ryabinin', 'Mayank Mishra', 'Minjia Zhang', 'Mohammad Shoeybi', 'Myriam Peyrounette', 'Nicolas Patry', 'Nouamane Tazi', 'Omar Sanseviero', 'Patrick von Platen', 'Pierre Cornette', 'Pierre François Lavallée', 'Rémi Lacroix', 'Samyam Rajbhandari', 'Sanchit Gandhi', 'Shaden Smith', 'Stéphane Requena', 'Suraj Patil', 'Tim Dettmers', 'Ahmed Baruwa', 'Amanpreet Singh', 'Anastasia Cheveleva', 'Anne-Laure Ligozat', 'Arjun Subramonian', 'Aurélie Névéol', 'Charles Lovering', 'Dan Garrette', 'Deepak Tunuguntla', 'Ehud Reiter', 'Ekaterina Taktasheva', 'Ekaterina Voloshina', 'Eli Bogdanov', 'Genta Indra Winata', 'Hailey Schoelkopf', 'Jan-Christoph Kalo', 'Jekaterina Novikova', 'Jessica Zosa Forde', 'Jordan Clive', 'Jungo Kasai', 'Ken Kawamura', 'Liam Hazan', 'Marine Carpuat', 'Miruna Clinciu', 'Najoung Kim', 'Newton Cheng', 'Oleg Serikov', 'Omer Antverg', 'Oskar van der Wal', 'Rui Zhang', 'Ruochen Zhang', 'Sebastian Gehrmann', 'Shachar Mirkin', 'Shani Pais', 'Tatiana Shavrina', 'Thomas Scialom', 'Tian Yun', 'Tomasz Limisiewicz', 'Verena Rieser', 'Vitaly Protasov', 'Vladislav Mikhailov', 'Yada Pruksachatkun', 'Yonatan Belinkov', 'Zachary Bamberger', 'Zdeněk Kasner', 'Alice Rueda', 'Amanda Pestana', 'Amir Feizpour', 'Ammar Khan', 'Amy Faranak', 'Ana Santos', 'Anthony Hevia', 'Antigona Unldreaj', 'Arash Aghagol', 'Arezoo Abdollahi', 'Aycha Tammour', 'Azadeh HajiHosseini', 'Bahareh Behroozi', 'Benjamin Ajibade', 'Bharat Saxena', 'Carlos Muñoz Ferrandis', 'Daniel McDuff', 'Danish Contractor', 'David Lansky', 'Davis David', 'Douwe Kiela', 'Duong A. Nguyen', 'Edward Tan', 'Emi Baylor', 'Ezinwanne Ozoani', 'Fatima Mirza', 'Frankline Ononiwu', 'Habib Rezanejad', 'Hessie Jones', 'Indrani Bhattacharya', 'Irene Solaiman', 'Irina Sedenko', 'Isar Nejadgholi', 'Jesse Passmore', 'Josh Seltzer', 'Julio Bonis Sanz', 'Livia Dutra', 'Mairon Samagaio', 'Maraim Elbadri', 'Margot Mieskes', 'Marissa Gerchick', 'Martha Akinlolu', 'Michael McKenna', 'Mike Qiu', 'Muhammed Ghauri', 'Mykola Burynok', 'Nafis Abrar', 'Nazneen Rajani', 'Nour Elkott', 'Nour Fahmy', 'Olanrewaju Samuel', 'Ran An', 'Rasmus Kromann', 'Ryan Hao', 'Samira Alizadeh', 'Sarmad Shubber', 'Silas Wang', 'Sourav Roy', 'Sylvain Viguier', 'Thanh Le', 'Tobi Oyebade', 'Trieu Le', 'Yoyo Yang', 'Zach Nguyen', 'Abhinav Ramesh Kashyap', 'Alfredo Palasciano', 'Alison Callahan', 'Anima Shukla', 'Antonio Miranda-Escalada', 'Ayush Singh', 'Benjamin Beilharz', 'Bo Wang', 'Caio Brito', 'Chenxi Zhou', 'Chirag Jain', 'Chuxin Xu', 'Clémentine Fourrier', 'Daniel León Periñán', 'Daniel Molano', 'Dian Yu', 'Enrique Manjavacas', 'Fabio Barth', 'Florian Fuhrimann', 'Gabriel Altay', 'Giyaseddin Bayrak', 'Gully Burns', 'Helena U. Vrabec', 'Imane Bello', 'Ishani Dash', 'Jihyun Kang', 'John Giorgi', 'Jonas Golde', 'Jose David Posada', 'Karthik Rangasai Sivaraman', 'Lokesh Bulchandani', 'Lu Liu', 'Luisa Shinzato', 'Madeleine Hahn de Bykhovetz', 'Maiko Takeuchi', 'Marc Pàmies', 'Maria A Castillo', 'Marianna Nezhurina', 'Mario Sänger', 'Matthias Samwald', 'Michael Cullan', 'Michael Weinberg', 'Michiel De Wolf', 'Mina Mihaljcic', 'Minna Liu', 'Moritz Freidank', 'Myungsun Kang', 'Natasha Seelam', 'Nathan Dahlberg', 'Nicholas Michio Broad', 'Nikolaus Muellner', 'Pascale Fung', 'Patrick Haller', 'Ramya Chandrasekhar', 'Renata Eisenberg', 'Robert Martin', 'Rodrigo Canalli', 'Rosaline Su', 'Ruisi Su', 'Samuel Cahyawijaya', 'Samuele Garda', 'Shlok S Deshmukh', 'Shubhanshu Mishra', 'Sid Kiblawi', 'Simon Ott', 'Sinee Sang-aroonsiri', 'Srishti Kumar', 'Stefan Schweter', 'Sushil Bharati', 'Tanmay Laud', 'Théo Gigant', 'Tomoya Kainuma', 'Wojciech Kusa', 'Yanis Labrak', 'Yash Shailesh Bajaj', 'Yash Venkatraman', 'Yifan Xu', 'Yingxin Xu', 'Yu Xu', 'Zhe Tan', 'Zhongli Xie', 'Zifan Ye', 'Mathilde Bras', 'Younes Belkada', 'Thomas Wolf']",BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,['cs.CL'],"  Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
","[{'version': 'v1', 'created': 'Wed, 9 Nov 2022 18:48:09 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Dec 2022 01:09:36 GMT'}, {'version': 'v3', 'created': 'Mon, 13 Mar 2023 15:55:30 GMT'}, {'version': 'v4', 'created': 'Tue, 27 Jun 2023 09:57:58 GMT'}]",cs.CL,2022-11-09 18:48:09,"['language model', 'large language model']",True,Translation & Low-Resource Languages,['googlegroups.com'],False,False,False,0.2826086957,576.0,0.9959623149394348,0.9952165156092648,394,0.9972887483054677
10450,arXiv:2212.10560,"['Yizhong Wang', 'Yeganeh Kordi', 'Swaroop Mishra', 'Alisa Liu', 'Noah A. Smith', 'Daniel Khashabi', 'Hannaneh Hajishirzi']",Self-Instruct: Aligning Language Models with Self-Generated Instructions,"['cs.CL', 'cs.AI']","  Large ""instruction-tuned"" language models (i.e., finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is often limited in quantity, diversity, and creativity, therefore
hindering the generality of the tuned model. We introduce Self-Instruct, a
framework for improving the instruction-following capabilities of pretrained
language models by bootstrapping off their own generations. Our pipeline
generates instructions, input, and output samples from a language model, then
filters invalid or similar ones before using them to finetune the original
model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute
improvement over the original model on Super-NaturalInstructions, on par with
the performance of InstructGPT-001, which was trained with private user data
and human annotations. For further evaluation, we curate a set of
expert-written instructions for novel tasks, and show through human evaluation
that tuning GPT3 with Self-Instruct outperforms using existing public
instruction datasets by a large margin, leaving only a 5% absolute gap behind
InstructGPT-001. Self-Instruct provides an almost annotation-free method for
aligning pre-trained language models with instructions, and we release our
large synthetic dataset to facilitate future studies on instruction tuning. Our
code and data are available at https://github.com/yizhongw/self-instruct.
","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 18:59:19 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 23:50:07 GMT'}]",cs.CL,2022-12-20 18:59:19,['language model'],True,Fine-Tuning & Domain Adaptation,['washington.edu'],False,True,True,0.6000000000000001,316.0,0.9952893674293405,0.9916918429003021,7,0.9968368730230457
9898,arXiv:2211.09110,"['Percy Liang', 'Rishi Bommasani', 'Tony Lee', 'Dimitris Tsipras', 'Dilara Soylu', 'Michihiro Yasunaga', 'Yian Zhang', 'Deepak Narayanan', 'Yuhuai Wu', 'Ananya Kumar', 'Benjamin Newman', 'Binhang Yuan', 'Bobby Yan', 'Ce Zhang', 'Christian Cosgrove', 'Christopher D. Manning', 'Christopher Ré', 'Diana Acosta-Navas', 'Drew A. Hudson', 'Eric Zelikman', 'Esin Durmus', 'Faisal Ladhak', 'Frieda Rong', 'Hongyu Ren', 'Huaxiu Yao', 'Jue Wang', 'Keshav Santhanam', 'Laurel Orr', 'Lucia Zheng', 'Mert Yuksekgonul', 'Mirac Suzgun', 'Nathan Kim', 'Neel Guha', 'Niladri Chatterji', 'Omar Khattab', 'Peter Henderson', 'Qian Huang', 'Ryan Chi', 'Sang Michael Xie', 'Shibani Santurkar', 'Surya Ganguli', 'Tatsunori Hashimoto', 'Thomas Icard', 'Tianyi Zhang', 'Vishrav Chaudhary', 'William Wang', 'Xuechen Li', 'Yifan Mai', 'Yuhui Zhang', 'Yuta Koreeda']",Holistic Evaluation of Language Models,"['cs.CL', 'cs.AI', 'cs.LG']","  Language models (LMs) are becoming the foundation for almost all major
language technologies, but their capabilities, limitations, and risks are not
well understood. We present Holistic Evaluation of Language Models (HELM) to
improve the transparency of language models. First, we taxonomize the vast
space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)
that are of interest for LMs. Then we select a broad subset based on coverage
and feasibility, noting what's missing or underrepresented (e.g. question
answering for neglected English dialects, metrics for trustworthiness). Second,
we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,
robustness, fairness, bias, toxicity, and efficiency) for each of 16 core
scenarios when possible (87.5% of the time). This ensures metrics beyond
accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We
also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze
specific aspects (e.g. reasoning, disinformation). Third, we conduct a
large-scale evaluation of 30 prominent language models (spanning open,
limited-access, and closed models) on all 42 scenarios, 21 of which were not
previously used in mainstream LM evaluation. Prior to HELM, models on average
were evaluated on just 17.9% of the core HELM scenarios, with some prominent
models not sharing a single scenario in common. We improve this to 96.0%: now
all 30 models have been densely benchmarked on the same core scenarios and
metrics under standardized conditions. Our evaluation surfaces 25 top-level
findings. For full transparency, we release all raw model prompts and
completions publicly for further analysis, as well as a general modular
toolkit. We intend for HELM to be a living benchmark for the community,
continuously updated with new scenarios, metrics, and models.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 18:51:34 GMT'}]",cs.CL,2022-11-16 18:51:34,['language model'],True,Applications and Benchmark Evaluations,['stanford.edu'],False,True,False,0.2258064516,238.0,0.9946164199192463,0.990055387713998,50,0.9963849977406236
9096,arXiv:2210.02414,"['Aohan Zeng', 'Xiao Liu', 'Zhengxiao Du', 'Zihan Wang', 'Hanyu Lai', 'Ming Ding', 'Zhuoyi Yang', 'Yifan Xu', 'Wendi Zheng', 'Xiao Xia', 'Weng Lam Tam', 'Zixuan Ma', 'Yufei Xue', 'Jidong Zhai', 'Wenguang Chen', 'Peng Zhang', 'Yuxiao Dong', 'Jie Tang']",GLM-130B: An Open Bilingual Pre-trained Model,"['cs.CL', 'cs.AI', 'cs.LG']","  We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language
model with 130 billion parameters. It is an attempt to open-source a 100B-scale
model at least as good as GPT-3 and unveil how models of such a scale can be
successfully pre-trained. Over the course of this effort, we face numerous
unexpected technical and engineering challenges, particularly on loss spikes
and disconvergence. In this paper, we introduce the training process of
GLM-130B including its design choices, training strategies for both efficiency
and stability, and engineering efforts. The resultant GLM-130B model offers
significant outperformance over GPT-3 175B on a wide range of popular English
benchmarks while the performance advantage is not observed in OPT-175B and
BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0
260B -- the largest Chinese language model -- across related benchmarks.
Finally, we leverage a unique scaling property of GLM-130B to reach INT4
quantization, without quantization aware training and with almost no
performance loss, making it the first among 100B-scale models. More
importantly, the property allows its effective inference on 4$\times$RTX 3090
(24G) or 8$\times$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs
required for using 100B-scale models. The GLM-130B model weights are publicly
accessible and its code, training logs, related toolkit, and lessons learned
are open-sourced at https://github.com/THUDM/GLM-130B .
","[{'version': 'v1', 'created': 'Wed, 5 Oct 2022 17:34:44 GMT'}]",cs.CL,2022-10-05 17:34:44,"['language model', 'GPT-3']",True,Efficiency & Performance,['tsinghua.edu.cn'],False,True,False,0.2,224.0,0.993943472409152,0.9894259818731118,18,0.9959331224582015
11254,arXiv:2302.13971,"['Hugo Touvron', 'Thibaut Lavril', 'Gautier Izacard', 'Xavier Martinet', 'Marie-Anne Lachaux', 'Timothée Lacroix', 'Baptiste Rozière', 'Naman Goyal', 'Eric Hambro', 'Faisal Azhar', 'Aurelien Rodriguez', 'Armand Joulin', 'Edouard Grave', 'Guillaume Lample']",LLaMA: Open and Efficient Foundation Language Models,['cs.CL'],"  We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.
","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 17:11:15 GMT'}]",cs.CL,2023-02-27 17:11:15,"['language model', 'GPT-3', 'PaLM', 'LLaMA']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.0714285714,1378.0,0.9985272459499264,0.9965769410300296,14,0.9978237214363439
11524,arXiv:2303.08774,['OpenAI'],GPT-4 Technical Report,"['cs.CL', 'cs.AI']","  We report the development of GPT-4, a large-scale, multimodal model which can
accept image and text inputs and produce text outputs. While less capable than
humans in many real-world scenarios, GPT-4 exhibits human-level performance on
various professional and academic benchmarks, including passing a simulated bar
exam with a score around the top 10% of test takers. GPT-4 is a
Transformer-based model pre-trained to predict the next token in a document.
The post-training alignment process results in improved performance on measures
of factuality and adherence to desired behavior. A core component of this
project was developing infrastructure and optimization methods that behave
predictably across a wide range of scales. This allowed us to accurately
predict some aspects of GPT-4's performance based on models trained with no
more than 1/1,000th the compute of GPT-4.
","[{'version': 'v1', 'created': 'Wed, 15 Mar 2023 17:15:04 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Mar 2023 04:59:24 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Mar 2023 17:46:54 GMT'}]",cs.CL,2023-03-15 17:15:04,['GPT-4'],True,Applications of LLMs/ChatGPT,[],False,False,,,1006.0,0.9977908689248896,0.9964213474404855,1,0.9976060935799782
11652,arXiv:2303.12712,"['Sébastien Bubeck', 'Varun Chandrasekaran', 'Ronen Eldan', 'Johannes Gehrke', 'Eric Horvitz', 'Ece Kamar', 'Peter Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Scott Lundberg', 'Harsha Nori', 'Hamid Palangi', 'Marco Tulio Ribeiro', 'Yi Zhang']",Sparks of Artificial General Intelligence: Early experiments with GPT-4,"['cs.CL', 'cs.AI']","  Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
","[{'version': 'v1', 'created': 'Wed, 22 Mar 2023 16:51:28 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Mar 2023 17:07:43 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Mar 2023 22:36:40 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Apr 2023 17:00:10 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Apr 2023 20:41:31 GMT'}]",cs.CL,2023-03-22 16:51:28,"['language model', 'PaLM', 'GPT-4', 'large language model', 'ChatGPT']",True,Applications of LLMs/ChatGPT,[],False,False,False,0.0909090909,546.0,0.9970544918998527,0.9962657538509413,14,0.9973884657236126
10807,arXiv:2301.12597,"['Junnan Li', 'Dongxu Li', 'Silvio Savarese', 'Steven Hoi']","BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image
  Encoders and Large Language Models",['cs.CV'],"  The cost of vision-and-language pre-training has become increasingly
prohibitive due to end-to-end training of large-scale models. This paper
proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps
vision-language pre-training from off-the-shelf frozen pre-trained image
encoders and frozen large language models. BLIP-2 bridges the modality gap with
a lightweight Querying Transformer, which is pre-trained in two stages. The
first stage bootstraps vision-language representation learning from a frozen
image encoder. The second stage bootstraps vision-to-language generative
learning from a frozen language model. BLIP-2 achieves state-of-the-art
performance on various vision-language tasks, despite having significantly
fewer trainable parameters than existing methods. For example, our model
outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable
parameters. We also demonstrate the model's emerging capabilities of zero-shot
image-to-text generation that can follow natural language instructions.
","[{'version': 'v1', 'created': 'Mon, 30 Jan 2023 00:56:51 GMT'}, {'version': 'v2', 'created': 'Mon, 1 May 2023 07:30:11 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Jun 2023 07:57:29 GMT'}]",cs.CV,2023-01-30 00:56:51,"['language model', 'large language model']",True,Vision-Language Models,[],False,False,False,0.0,449.0,0.9963181148748159,0.9961101602613972,4,0.997170837867247
11875,arXiv:2303.18223,"['Wayne Xin Zhao', 'Kun Zhou', 'Junyi Li', 'Tianyi Tang', 'Xiaolei Wang', 'Yupeng Hou', 'Yingqian Min', 'Beichen Zhang', 'Junjie Zhang', 'Zican Dong', 'Yifan Du', 'Chen Yang', 'Yushuo Chen', 'Zhipeng Chen', 'Jinhao Jiang', 'Ruiyang Ren', 'Yifan Li', 'Xinyu Tang', 'Zikang Liu', 'Peiyu Liu', 'Jian-Yun Nie', 'Ji-Rong Wen']",A Survey of Large Language Models,"['cs.CL', 'cs.AI']","  Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.
","[{'version': 'v1', 'created': 'Fri, 31 Mar 2023 17:28:46 GMT'}, {'version': 'v10', 'created': 'Sun, 7 May 2023 17:59:15 GMT'}, {'version': 'v11', 'created': 'Thu, 29 Jun 2023 16:09:05 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Apr 2023 15:49:09 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Apr 2023 16:20:17 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Apr 2023 16:13:54 GMT'}, {'version': 'v5', 'created': 'Sun, 16 Apr 2023 16:42:37 GMT'}, {'version': 'v6', 'created': 'Mon, 24 Apr 2023 16:53:57 GMT'}, {'version': 'v7', 'created': 'Tue, 25 Apr 2023 14:42:36 GMT'}, {'version': 'v8', 'created': 'Thu, 27 Apr 2023 15:54:48 GMT'}, {'version': 'v9', 'created': 'Fri, 28 Apr 2023 15:39:09 GMT'}]",cs.CL,2023-03-31 17:28:46,"['language model', 'large language model', 'ChatGPT']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.0,326.0,0.9955817378497791,0.995798973082309,22,0.9969532100108814
12302,arXiv:2304.10592,"['Deyao Zhu', 'Jun Chen', 'Xiaoqian Shen', 'Xiang Li', 'Mohamed Elhoseiny']","MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large
  Language Models",['cs.CV'],"  The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such
as directly generating websites from handwritten text and identifying humorous
elements within images. These features are rarely observed in previous
vision-language models. We believe the primary reason for GPT-4's advanced
multi-modal generation capabilities lies in the utilization of a more advanced
large language model (LLM). To examine this phenomenon, we present MiniGPT-4,
which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one
projection layer. Our findings reveal that MiniGPT-4 possesses many
capabilities similar to those exhibited by GPT-4 like detailed image
description generation and website creation from hand-written drafts.
Furthermore, we also observe other emerging capabilities in MiniGPT-4,
including writing stories and poems inspired by given images, providing
solutions to problems shown in images, teaching users how to cook based on food
photos, etc. In our experiment, we found that only performing the pretraining
on raw image-text pairs could produce unnatural language outputs that lack
coherency including repetition and fragmented sentences. To address this
problem, we curate a high-quality, well-aligned dataset in the second stage to
finetune our model using a conversational template. This step proved crucial
for augmenting the model's generation reliability and overall usability.
Notably, our model is highly computationally efficient, as we only train a
projection layer utilizing approximately 5 million aligned image-text pairs.
Our code, pre-trained model, and collected dataset are available at
https://minigpt-4.github.io/.
","[{'version': 'v1', 'created': 'Thu, 20 Apr 2023 18:25:35 GMT'}]",cs.CV,2023-04-20 18:25:35,"['language model', 'large language model', 'GPT-4']",True,Video & Multimodal Models,['kaust.edu.sa'],False,True,False,0.0,180.0,0.9975285758418289,0.9948654115450444,5,0.9958650707290533
12214,arXiv:2304.08485,"['Haotian Liu', 'Chunyuan Li', 'Qingyang Wu', 'Yong Jae Lee']",Visual Instruction Tuning,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","  Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.
","[{'version': 'v1', 'created': 'Mon, 17 Apr 2023 17:59:25 GMT'}]",cs.CV,2023-04-17 17:59:25,"['language model', 'large language model', 'GPT-4']",True,Vision-Language Models,[],False,False,False,0.0,172.0,0.9972196478220574,0.9945542243659561,4,0.9954298150163221
13076,arXiv:2305.10403,"['Rohan Anil', 'Andrew M. Dai', 'Orhan Firat', 'Melvin Johnson', 'Dmitry Lepikhin', 'Alexandre Passos', 'Siamak Shakeri', 'Emanuel Taropa', 'Paige Bailey', 'Zhifeng Chen', 'Eric Chu', 'Jonathan H. Clark', 'Laurent El Shafey', 'Yanping Huang', 'Kathy Meier-Hellstern', 'Gaurav Mishra', 'Erica Moreira', 'Mark Omernick', 'Kevin Robinson', 'Sebastian Ruder', 'Yi Tay', 'Kefan Xiao', 'Yuanzhong Xu', 'Yujing Zhang', 'Gustavo Hernandez Abrego', 'Junwhan Ahn', 'Jacob Austin', 'Paul Barham', 'Jan Botha', 'James Bradbury', 'Siddhartha Brahma', 'Kevin Brooks', 'Michele Catasta', 'Yong Cheng', 'Colin Cherry', 'Christopher A. Choquette-Choo', 'Aakanksha Chowdhery', 'Clément Crepy', 'Shachi Dave', 'Mostafa Dehghani', 'Sunipa Dev', 'Jacob Devlin', 'Mark Díaz', 'Nan Du', 'Ethan Dyer', 'Vlad Feinberg', 'Fangxiaoyu Feng', 'Vlad Fienber', 'Markus Freitag', 'Xavier Garcia', 'Sebastian Gehrmann', 'Lucas Gonzalez', 'Guy Gur-Ari', 'Steven Hand', 'Hadi Hashemi', 'Le Hou', 'Joshua Howland', 'Andrea Hu', 'Jeffrey Hui', 'Jeremy Hurwitz', 'Michael Isard', 'Abe Ittycheriah', 'Matthew Jagielski', 'Wenhao Jia', 'Kathleen Kenealy', 'Maxim Krikun', 'Sneha Kudugunta', 'Chang Lan', 'Katherine Lee', 'Benjamin Lee', 'Eric Li', 'Music Li', 'Wei Li', 'YaGuang Li', 'Jian Li', 'Hyeontaek Lim', 'Hanzhao Lin', 'Zhongtao Liu', 'Frederick Liu', 'Marcello Maggioni', 'Aroma Mahendru', 'Joshua Maynez', 'Vedant Misra', 'Maysam Moussalem', 'Zachary Nado', 'John Nham', 'Eric Ni', 'Andrew Nystrom', 'Alicia Parrish', 'Marie Pellat', 'Martin Polacek', 'Alex Polozov', 'Reiner Pope', 'Siyuan Qiao', 'Emily Reif', 'Bryan Richter', 'Parker Riley', 'Alex Castro Ros', 'Aurko Roy', 'Brennan Saeta', 'Rajkumar Samuel', 'Renee Shelby', 'Ambrose Slone', 'Daniel Smilkov', 'David R. So', 'Daniel Sohn', 'Simon Tokumine', 'Dasha Valter', 'Vijay Vasudevan', 'Kiran Vodrahalli', 'Xuezhi Wang', 'Pidong Wang', 'Zirui Wang', 'Tao Wang', 'John Wieting', 'Yuhuai Wu', 'Kelvin Xu', 'Yunhan Xu', 'Linting Xue', 'Pengcheng Yin', 'Jiahui Yu', 'Qiao Zhang', 'Steven Zheng', 'Ce Zheng', 'Weikang Zhou', 'Denny Zhou', 'Slav Petrov', 'Yonghui Wu']",PaLM 2 Technical Report,"['cs.CL', 'cs.AI']","  We introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
  When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report.
","[{'version': 'v1', 'created': 'Wed, 17 May 2023 17:46:53 GMT'}]",cs.CL,2023-05-17 17:46:53,"['language model', 'PaLM']",True,Pretrained LMs & Text Classification,['google.com'],True,False,False,0.1489361702,168.0,0.9969107198022861,0.994398630776412,128,0.9952121871599565
12013,arXiv:2304.03277,"['Baolin Peng', 'Chunyuan Li', 'Pengcheng He', 'Michel Galley', 'Jianfeng Gao']",Instruction Tuning with GPT-4,"['cs.CL', 'cs.AI']","  Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.
","[{'version': 'v1', 'created': 'Thu, 6 Apr 2023 17:58:09 GMT'}]",cs.CL,2023-04-06 17:58:09,"['language model', 'large language model', 'LLaMA', 'GPT-4']",True,Prompts & In-Context Learning,['microsoft.com'],True,False,False,0.0,138.0,0.9966017917825146,0.9939318500077797,5,0.9945593035908596
12021,arXiv:2304.03442,"['Joon Sung Park', ""Joseph C. O'Brien"", 'Carrie J. Cai', 'Meredith Ringel Morris', 'Percy Liang', 'Michael S. Bernstein']",Generative Agents: Interactive Simulacra of Human Behavior,"['cs.HC', 'cs.AI', 'cs.LG']","  Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 01:55:19 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Aug 2023 00:21:19 GMT'}]",cs.HC,2023-04-07 01:55:19,"['language model', 'large language model']",True,Human Feedback & Interaction,"['google.com', 'stanford.edu']",True,True,False,0.4,123.0,0.9962928637627433,0.9932316788548312,6,0.9935799782372143
16002,arXiv:2308.02976,"['José Cañete', 'Gabriel Chaperon', 'Rodrigo Fuentes', 'Jou-Hui Ho', 'Hojin Kang', 'Jorge Pérez']",Spanish Pre-trained BERT Model and Evaluation Data,"['cs.CL', 'cs.AI', 'cs.LG']","  The Spanish language is one of the top 5 spoken languages in the world.
Nevertheless, finding resources to train or evaluate Spanish language models is
not an easy task. In this paper we help bridge this gap by presenting a
BERT-based language model pre-trained exclusively on Spanish data. As a second
contribution, we also compiled several tasks specifically for the Spanish
language in a single repository much in the spirit of the GLUE benchmark. By
fine-tuning our pre-trained Spanish model, we obtain better results compared to
other BERT-based models pre-trained on multilingual corpora for most of the
tasks, even achieving a new state-of-the-art on some of them. We have publicly
released our model, the pre-training data, and the compilation of the Spanish
benchmarks.
","[{'version': 'v1', 'created': 'Sun, 6 Aug 2023 00:16:04 GMT'}]",cs.CL,2023-08-06 00:16:04,"['BERT', 'language model']",True,Multilingual Transfer Learning,"['dcc.uchile.cl', 'ug.uchile.cl']",False,False,False,0.0,353.0,0.9934497816593887,0.9959545666718531,6,0.9934497816593887
15547,arXiv:2307.09288,"['Hugo Touvron', 'Louis Martin', 'Kevin Stone', 'Peter Albert', 'Amjad Almahairi', 'Yasmine Babaei', 'Nikolay Bashlykov', 'Soumya Batra', 'Prajjwal Bhargava', 'Shruti Bhosale', 'Dan Bikel', 'Lukas Blecher', 'Cristian Canton Ferrer', 'Moya Chen', 'Guillem Cucurull', 'David Esiobu', 'Jude Fernandes', 'Jeremy Fu', 'Wenyin Fu', 'Brian Fuller', 'Cynthia Gao', 'Vedanuj Goswami', 'Naman Goyal', 'Anthony Hartshorn', 'Saghar Hosseini', 'Rui Hou', 'Hakan Inan', 'Marcin Kardas', 'Viktor Kerkez', 'Madian Khabsa', 'Isabel Kloumann', 'Artem Korenev', 'Punit Singh Koura', 'Marie-Anne Lachaux', 'Thibaut Lavril', 'Jenya Lee', 'Diana Liskovich', 'Yinghai Lu', 'Yuning Mao', 'Xavier Martinet', 'Todor Mihaylov', 'Pushkar Mishra', 'Igor Molybog', 'Yixin Nie', 'Andrew Poulton', 'Jeremy Reizenstein', 'Rashi Rungta', 'Kalyan Saladi', 'Alan Schelten', 'Ruan Silva', 'Eric Michael Smith', 'Ranjan Subramanian', 'Xiaoqing Ellen Tan', 'Binh Tang', 'Ross Taylor', 'Adina Williams', 'Jian Xiang Kuan', 'Puxin Xu', 'Zheng Yan', 'Iliyan Zarov', 'Yuchen Zhang', 'Angela Fan', 'Melanie Kambadur', 'Sharan Narang', 'Aurelien Rodriguez', 'Robert Stojnic', 'Sergey Edunov', 'Thomas Scialom']",Llama 2: Open Foundation and Fine-Tuned Chat Models,"['cs.CL', 'cs.AI']","  In this work, we develop and release Llama 2, a collection of pretrained and
fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
dialogue use cases. Our models outperform open-source chat models on most
benchmarks we tested, and based on our human evaluations for helpfulness and
safety, may be a suitable substitute for closed-source models. We provide a
detailed description of our approach to fine-tuning and safety improvements of
Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.
","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 14:31:57 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jul 2023 17:08:59 GMT'}]",cs.CL,2023-07-18 14:31:57,"['language model', 'large language model']",True,Dialogue & Conversational AI,['fb.com'],True,False,False,0.22916666670000002,216.0,0.9929039301310044,0.9951765987241326,68,0.9929039301310044
15532,arXiv:2307.09009,"['Lingjiao Chen', 'Matei Zaharia', 'James Zou']",How is ChatGPT's behavior changing over time?,"['cs.CL', 'cs.AI', 'cs.LG']","  GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)
services. However, when and how these models are updated over time is opaque.
Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on
several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)
opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating
code, 6) US Medical License tests, and 7) visual reasoning. We find that the
performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was reasonable at identifying prime vs.
composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same
questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity
to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in
June than in March in this task. GPT-4 became less willing to answer sensitive
questions and opinion survey questions in June than in March. GPT-4 performed
better at multi-hop questions in June than in March, while GPT-3.5's
performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting
mistakes in code generation in June than in March. Overall, our findings show
that the behavior of the ""same"" LLM service can change substantially in a
relatively short amount of time, highlighting the need for continuous
monitoring of LLMs.
","[{'version': 'v1', 'created': 'Tue, 18 Jul 2023 06:56:08 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Aug 2023 14:23:58 GMT'}]",cs.CL,2023-07-18 06:56:08,"['language model', 'GPT-3', 'GPT-4', 'large language model', 'ChatGPT']",True,"LLMs, Reasoning, Chain-of-Thought",[],False,False,False,0.0,40.0,0.99235807860262,0.9797728333592656,3,0.99235807860262
15261,arXiv:2307.03172,"['Nelson F. Liu', 'Kevin Lin', 'John Hewitt', 'Ashwin Paranjape', 'Michele Bevilacqua', 'Fabio Petroni', 'Percy Liang']",Lost in the Middle: How Language Models Use Long Contexts,['cs.CL'],"  While recent language models have the ability to take long contexts as input,
relatively little is known about how well they use longer context. We analyze
language model performance on two tasks that require identifying relevant
information within their input contexts: multi-document question answering and
key-value retrieval. We find that performance is often highest when relevant
information occurs at the beginning or end of the input context, and
significantly degrades when models must access relevant information in the
middle of long contexts. Furthermore, performance substantially decreases as
the input context grows longer, even for explicitly long-context models. Our
analysis provides a better understanding of how language models use their input
context and provides new evaluation protocols for future long-context models.
","[{'version': 'v1', 'created': 'Thu, 6 Jul 2023 17:54:11 GMT'}, {'version': 'v2', 'created': 'Mon, 31 Jul 2023 17:48:48 GMT'}]",cs.CL,2023-07-06 17:54:11,['language model'],True,Interpretability & Reasoning,['stanford.edu'],False,True,False,0.0,36.0,0.9915393013100436,0.9768943519526996,7,0.9915393013100436
15255,arXiv:2307.03109,"['Yupeng Chang', 'Xu Wang', 'Jindong Wang', 'Yuan Wu', 'Linyi Yang', 'Kaijie Zhu', 'Hao Chen', 'Xiaoyuan Yi', 'Cunxiang Wang', 'Yidong Wang', 'Wei Ye', 'Yue Zhang', 'Yi Chang', 'Philip S. Yu', 'Qiang Yang', 'Xing Xie']",A Survey on Evaluation of Large Language Models,"['cs.CL', 'cs.AI']","  Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
","[{'version': 'v1', 'created': 'Thu, 6 Jul 2023 16:28:35 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Jul 2023 12:31:50 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Jul 2023 15:43:03 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Jul 2023 12:33:20 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Jul 2023 08:11:21 GMT'}, {'version': 'v6', 'created': 'Wed, 2 Aug 2023 07:39:17 GMT'}, {'version': 'v7', 'created': 'Mon, 28 Aug 2023 05:50:53 GMT'}]",cs.CL,2023-07-06 16:28:35,"['language model', 'large language model']",True,"LLMs, Reasoning, Chain-of-Thought","['microsoft.com', 'jlu.edu.cn']",True,True,False,0.0,36.0,0.9915393013100436,0.9768943519526996,16,0.9915393013100436
